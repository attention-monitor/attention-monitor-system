import sys
import os
import queue
import threading
import time
import json
import random
import math
import traceback
import warnings
from datetime import datetime, timedelta
from collections import deque
import statistics
from typing import Optional, Tuple, List
import cv2
import numpy as np
from PyQt5.QtWidgets import *
from PyQt5.QtCore import *
from PyQt5.QtGui import *
import mediapipe as mp
import dlib
from scipy.ndimage import zoom
from tensorflow.keras.models import load_model
from imutils import face_utils

warnings.filterwarnings('ignore')

try:
    import pyttsx3

    TTS_AVAILABLE = True
except ImportError:
    TTS_AVAILABLE = False
    print("æ³¨æ„ï¼špyttsx3 æœªå®‰è£…ï¼Œè¯­éŸ³åŠŸèƒ½å°†ä¸å¯ç”¨")

# ============================================================================
# åˆå§‹åŒ–MediaPipe
# ============================================================================
mp_pose = mp.solutions.pose
mp_face_mesh = mp.solutions.face_mesh
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles


# ============================================================================
# æ³¨æ„åŠ›è¯†åˆ«é…ç½®å’Œç±»
# ============================================================================

class AttentionConfig:
    """æ³¨æ„åŠ›è¯†åˆ«é…ç½®"""

    def __init__(self):
        self.ear_thresh: float = 0.21  # < EAR => çœ¼ç›é—­åˆ
        self.ear_consec_frames: int = 1  # è¿ç»­å¸§æ•°åˆ¤æ–­çœ¨çœ¼/é—­åˆ
        self.yaw_thresh_deg: float = 20.0  # |yaw| > => è½¬å¤´
        self.pitch_thresh_deg: float = 20.0  # |pitch| > => æŠ¬å¤´/ä½å¤´
        self.roll_thresh_deg: float = 25.0  # ä»…ç”¨äºæ˜¾ç¤º/è¯Šæ–­
        self.gaze_off_center: float = 0.35  # |gaze_x| æˆ– |gaze_y| > => è§†çº¿åç¦»
        self.min_face_conf: float = 0.5


# MediaPipe FaceMesh åœ°æ ‡ç´¢å¼•
R_EYE = [33, 160, 158, 133, 153, 144]
L_EYE = [362, 385, 387, 263, 373, 380]

# å¤´éƒ¨å§¿æ€2Dç‚¹
POSE_LANDMARKS = {
    'nose_tip': 1,
    'chin': 152,
    'left_eye_outer': 263,
    'right_eye_outer': 33,
    'left_mouth': 291,
    'right_mouth': 61
}

# 3Dæ¨¡å‹å‚è€ƒç‚¹ï¼ˆæ¯«ç±³ï¼‰
MODEL_POINTS_3D = np.array([
    [0.0, 0.0, 0.0],  # é¼»å°–
    [0.0, -63.6, -12.5],  # ä¸‹å·´
    [-43.3, 32.7, -26.0],  # å·¦çœ¼è§’
    [43.3, 32.7, -26.0],  # å³çœ¼è§’
    [-28.9, -28.9, -24.1],  # å·¦å˜´è§’
    [28.9, -28.9, -24.1]  # å³å˜´è§’
], dtype=np.float64)

# è™¹è†œåœ°æ ‡ç´¢å¼•èŒƒå›´
RIGHT_IRIS = list(range(468, 473))
LEFT_IRIS = list(range(473, 478))


class AttentionAnalyzer:
    """æ³¨æ„åŠ›åˆ†æå™¨"""

    def __init__(self):
        self.config = AttentionConfig()
        self.face_mesh = mp_face_mesh.FaceMesh(
            static_image_mode=False,
            refine_landmarks=True,
            max_num_faces=1,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )

        # çŠ¶æ€å˜é‡
        self.closed_counter = 0
        self.blinks = 0
        self.attention_history = []
        self.ear_history = deque(maxlen=30)
        self.gaze_history = deque(maxlen=30)
        self.pose_history = deque(maxlen=30)

        # å½“å‰çŠ¶æ€
        self.current_state = {
            "attention_label": "åˆå§‹åŒ–ä¸­",
            "ear_left": 0.0,
            "ear_right": 0.0,
            "yaw": 0.0,
            "pitch": 0.0,
            "roll": 0.0,
            "gaze_x": 0.0,
            "gaze_y": 0.0,
            "blink_count": 0
        }

    def landmarks_to_np(self, landmarks, w, h) -> np.ndarray:
        """å°†åœ°æ ‡è½¬æ¢ä¸ºnumpyæ•°ç»„"""
        pts = []
        for lm in landmarks:
            x, y = int(lm.x * w), int(lm.y * h)
            pts.append((x, y))
        return np.array(pts, dtype=np.int32)

    def eye_aspect_ratio(self, eye_pts: np.ndarray) -> float:
        """è®¡ç®—çœ¼ç›çºµæ¨ªæ¯”"""
        if len(eye_pts) < 6:
            return 0.3

        try:
            p1, p2, p3, p4, p5, p6 = eye_pts[:6]
            A = np.linalg.norm(p2 - p6)
            B = np.linalg.norm(p3 - p5)
            C = np.linalg.norm(p1 - p4)
            ear = (A + B) / (2.0 * C + 1e-6)
            return float(ear)
        except:
            return 0.3

    def head_pose(self, w: int, h: int, pts: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """è®¡ç®—å¤´éƒ¨å§¿æ€"""
        try:
            idx = POSE_LANDMARKS
            image_points = np.array([
                pts[idx['nose_tip']],
                pts[idx['chin']],
                pts[idx['left_eye_outer']],
                pts[idx['right_eye_outer']],
                pts[idx['left_mouth']],
                pts[idx['right_mouth']]
            ], dtype=np.float64)

            focal_length = w
            center = (w / 2, h / 2)
            camera_matrix = np.array(
                [[focal_length, 0, center[0]],
                 [0, focal_length, center[1]],
                 [0, 0, 1]], dtype=np.float64)

            dist_coeffs = np.zeros((4, 1))

            success, rotation_vec, translation_vec = cv2.solvePnP(
                MODEL_POINTS_3D,
                image_points,
                camera_matrix,
                dist_coeffs,
                flags=cv2.SOLVEPNP_ITERATIVE
            )

            if not success:
                return None, None, None

            rotation_mat, _ = cv2.Rodrigues(rotation_vec)
            pose_mat = np.hstack((rotation_mat, translation_vec))
            _, _, _, _, _, _, euler_angles = cv2.decomposeProjectionMatrix(pose_mat)
            pitch, yaw, roll = euler_angles.flatten()
            return np.array([pitch, yaw, roll]), rotation_vec, translation_vec
        except:
            return None, None, None

    def iris_center(self, all_pts: np.ndarray, iris_idx: List[int]) -> Optional[np.ndarray]:
        """è®¡ç®—è™¹è†œä¸­å¿ƒ"""
        if len(iris_idx) == 0:
            return None
        try:
            iris_pts = all_pts[iris_idx]
            c = iris_pts.mean(axis=0)
            return c
        except:
            return None

    def gaze_vector(self, eye_pts: np.ndarray, iris_c: np.ndarray) -> Tuple[float, float]:
        """è®¡ç®—è§†çº¿å‘é‡"""
        try:
            x_min, y_min = eye_pts.min(axis=0)
            x_max, y_max = eye_pts.max(axis=0)
            cx = (x_min + x_max) / 2.0
            cy = (y_min + y_max) / 2.0

            nx = 0.0 if x_max == x_min else (iris_c[0] - cx) / ((x_max - x_min) / 2.0)
            ny = 0.0 if y_max == y_min else (iris_c[1] - cy) / ((y_max - y_min) / 2.0)

            nx = float(np.clip(nx, -1.5, 1.5))
            ny = float(np.clip(ny, -1.5, 1.5))

            return nx, ny
        except:
            return 0.0, 0.0

    def attention_label(self, ear_l: float, ear_r: float, yaw: float,
                        pitch: float, gaze: Tuple[float, float]) -> str:
        """ç¡®å®šæ³¨æ„åŠ›æ ‡ç­¾"""
        eyes_open = (ear_l > self.config.ear_thresh) and (ear_r > self.config.ear_thresh)
        looking_forward = (abs(yaw) < self.config.yaw_thresh_deg) and (abs(pitch) < self.config.pitch_thresh_deg)
        gaze_centered = (abs(gaze[0]) < self.config.gaze_off_center) and (abs(gaze[1]) < self.config.gaze_off_center)

        if not eyes_open:
            return "çœ¼ç›é—­åˆ"
        if not looking_forward:
            return "è§†çº¿åç¦»"
        if not gaze_centered:
            return "è§†çº¿åç§»"
        return "ä¸“æ³¨"

    def analyze_frame(self, frame):
        """åˆ†æå•å¸§æ³¨æ„åŠ›"""
        try:
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            h, w = frame.shape[:2]

            result = self.face_mesh.process(rgb_frame)

            # é»˜è®¤çŠ¶æ€
            label = "æœªæ£€æµ‹åˆ°é¢éƒ¨"
            ear_l = ear_r = 0.0
            euler = np.array([0.0, 0.0, 0.0])
            gaze_xy = (0.0, 0.0)
            face_detected = False

            if result.multi_face_landmarks:
                face_detected = True
                lms = result.multi_face_landmarks[0].landmark
                pts = self.landmarks_to_np(lms, w, h)

                # è®¡ç®—EAR
                eye_r = pts[R_EYE]
                eye_l = pts[L_EYE]
                ear_r = self.eye_aspect_ratio(eye_r)
                ear_l = self.eye_aspect_ratio(eye_l)

                # çœ¨çœ¼/é—­çœ¼è®¡æ•°å™¨
                if ear_l < self.config.ear_thresh and ear_r < self.config.ear_thresh:
                    self.closed_counter += 1
                else:
                    if self.closed_counter >= self.config.ear_consec_frames:
                        self.blinks += 1
                    self.closed_counter = 0

                # å¤´éƒ¨å§¿æ€
                euler_result, _, _ = self.head_pose(w, h, pts)
                if euler_result is not None:
                    euler = euler_result

                # è§†çº¿æ–¹å‘
                ir_c_r = self.iris_center(pts, RIGHT_IRIS)
                ir_c_l = self.iris_center(pts, LEFT_IRIS)

                if ir_c_r is not None and ir_c_l is not None:
                    gx_r, gy_r = self.gaze_vector(eye_r, ir_c_r)
                    gx_l, gy_l = self.gaze_vector(eye_l, ir_c_l)
                    gaze_xy = ((gx_r + gx_l) / 2.0, (gy_r + gy_l) / 2.0)

                # æ³¨æ„åŠ›æ ‡ç­¾
                pitch, yaw, roll = [float(x) for x in euler]
                label = self.attention_label(ear_l, ear_r, yaw, pitch, gaze_xy)

            # æ›´æ–°å½“å‰çŠ¶æ€
            self.current_state = {
                "attention_label": label,
                "ear_left": ear_l,
                "ear_right": ear_r,
                "yaw": float(euler[1]) if euler is not None else 0.0,
                "pitch": float(euler[0]) if euler is not None else 0.0,
                "roll": float(euler[2]) if euler is not None else 0.0,
                "gaze_x": gaze_xy[0],
                "gaze_y": gaze_xy[1],
                "blink_count": self.blinks,
                "face_detected": face_detected
            }

            # æ›´æ–°å†å²è®°å½•
            attention_score = self.calculate_attention_score()
            self.attention_history.append(attention_score)
            self.ear_history.append((ear_l + ear_r) / 2)
            self.gaze_history.append(math.sqrt(gaze_xy[0] ** 2 + gaze_xy[1] ** 2))
            self.pose_history.append(abs(euler[1]) + abs(euler[0]))

            return self.current_state

        except Exception as e:
            print(f"æ³¨æ„åŠ›åˆ†æé”™è¯¯: {e}")
            return self.current_state

    def calculate_attention_score(self):
        """è®¡ç®—ç»¼åˆæ³¨æ„åŠ›åˆ†æ•°"""
        state = self.current_state

        if state["attention_label"] == "æœªæ£€æµ‹åˆ°é¢éƒ¨":
            return 0

        score = 100

        # çœ¼ç›çŠ¶æ€æ‰£åˆ†
        if state["attention_label"] == "çœ¼ç›é—­åˆ":
            score -= 40

        # å¤´éƒ¨å§¿æ€æ‰£åˆ†
        if abs(state["yaw"]) > self.config.yaw_thresh_deg:
            yaw_penalty = min(30, abs(state["yaw"]) / self.config.yaw_thresh_deg * 15)
            score -= yaw_penalty

        if abs(state["pitch"]) > self.config.pitch_thresh_deg:
            pitch_penalty = min(25, abs(state["pitch"]) / self.config.pitch_thresh_deg * 12)
            score -= pitch_penalty

        # è§†çº¿æ–¹å‘æ‰£åˆ†
        gaze_magnitude = math.sqrt(state["gaze_x"] ** 2 + state["gaze_y"] ** 2)
        if gaze_magnitude > self.config.gaze_off_center:
            gaze_penalty = min(35, gaze_magnitude / self.config.gaze_off_center * 20)
            score -= gaze_penalty

        # çœ¨çœ¼é¢‘ç‡ï¼ˆé€‚åº¦çœ¨çœ¼æ˜¯å¥½çš„ï¼Œä½†è¿‡å¤šå¯èƒ½è¡¨ç¤ºç–²åŠ³ï¼‰
        if len(self.attention_history) > 100:
            recent_blinks = min(20, self.blinks / (len(self.attention_history) / 100) * 2)
            if recent_blinks > 15:  # çœ¨çœ¼è¿‡å¤š
                score -= 10

        return max(0, min(100, score))

    def get_attention_stats(self):
        """è·å–æ³¨æ„åŠ›ç»Ÿè®¡"""
        if not self.attention_history:
            return {
                "avg_score": 0,
                "max_score": 0,
                "min_score": 0,
                "trend": "ç¨³å®š",
                "focus_percentage": 0,
                "blink_rate": 0
            }

        try:
            scores = list(self.attention_history)
            avg_score = statistics.mean(scores) if scores else 0
            max_score = max(scores) if scores else 0
            min_score = min(scores) if scores else 0

            # è®¡ç®—è¶‹åŠ¿
            if len(scores) >= 30:
                recent = scores[-15:]
                earlier = scores[-30:-15] if len(scores) >= 30 else recent
                recent_avg = statistics.mean(recent) if recent else 0
                earlier_avg = statistics.mean(earlier) if earlier else 0

                if recent_avg > earlier_avg + 5:
                    trend = "ä¸Šå‡"
                elif recent_avg < earlier_avg - 5:
                    trend = "ä¸‹é™"
                else:
                    trend = "ç¨³å®š"
            else:
                trend = "åˆ†æä¸­"

            # è®¡ç®—ä¸“æ³¨ç™¾åˆ†æ¯”
            focused_frames = sum(1 for s in scores if s >= 70)
            focus_percentage = (focused_frames / len(scores) * 100) if scores else 0

            # è®¡ç®—çœ¨çœ¼ç‡ï¼ˆæ¯åˆ†é’Ÿï¼‰
            blink_rate = (self.blinks / (len(scores) / 30)) * 60 if scores else 0

            return {
                "avg_score": round(avg_score, 1),
                "max_score": round(max_score, 1),
                "min_score": round(min_score, 1),
                "trend": trend,
                "focus_percentage": round(focus_percentage, 1),
                "blink_rate": round(blink_rate, 1)
            }
        except:
            return {
                "avg_score": 0,
                "max_score": 0,
                "min_score": 0,
                "trend": "æœªçŸ¥",
                "focus_percentage": 0,
                "blink_rate": 0
            }

    def reset(self):
        """é‡ç½®åˆ†æå™¨"""
        self.closed_counter = 0
        self.blinks = 0
        self.attention_history.clear()
        self.ear_history.clear()
        self.gaze_history.clear()
        self.pose_history.clear()


# ============================================================================
# æƒ…ç»ªè¯†åˆ«ç±»
# ============================================================================

class EmotionAnalyzer:
    """æƒ…ç»ªåˆ†æå™¨"""

    def __init__(self):
        # æƒ…ç»ªæ¨¡å‹å‚æ•°
        self.shape_x = 48
        self.shape_y = 48
        self.input_shape = (self.shape_x, self.shape_y, 1)
        self.nClasses = 7

        # æƒ…ç»ªæ ‡ç­¾
        self.emotion_labels = [
            "ç”Ÿæ°”", "åŒæ¶", "ææƒ§", "å¿«ä¹",
            "æ‚²ä¼¤", "æƒŠè®¶", "ä¸­æ€§"
        ]

        # æƒ…ç»ªé¢œè‰²æ˜ å°„
        self.emotion_colors = {
            "ç”Ÿæ°”": (0, 0, 255),  # çº¢è‰²
            "åŒæ¶": (0, 128, 0),  # ç»¿è‰²
            "ææƒ§": (128, 0, 128),  # ç´«è‰²
            "å¿«ä¹": (0, 255, 255),  # é»„è‰²
            "æ‚²ä¼¤": (255, 0, 0),  # è“è‰²
            "æƒŠè®¶": (0, 165, 255),  # æ©™è‰²
            "ä¸­æ€§": (200, 200, 200)  # ç°è‰²
        }

        # åŠ è½½æ¨¡å‹
        self.model = None
        self.face_detector = None
        self.predictor = None

        try:
            # å°è¯•åŠ è½½æƒ…ç»ªè¯†åˆ«æ¨¡å‹
            self.model = load_model('Models/EmotionXCeption/video.h5')
            print("æƒ…ç»ªæ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"åŠ è½½æƒ…ç»ªæ¨¡å‹å¤±è´¥: {e}")
            print("ä½¿ç”¨å¤‡ç”¨æƒ…ç»ªæ£€æµ‹")

        try:
            # åŠ è½½dlibé¢éƒ¨æ£€æµ‹å™¨å’Œç‰¹å¾ç‚¹é¢„æµ‹å™¨
            self.face_detector = dlib.get_frontal_face_detector()
            self.predictor = dlib.shape_predictor("Models/Landmarks/face_landmarks.dat")
            print("Dlibæ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"åŠ è½½dlibæ¨¡å‹å¤±è´¥: {e}")

        # çŠ¶æ€å˜é‡
        self.emotion_history = deque(maxlen=100)
        self.current_emotion = "ä¸­æ€§"
        self.emotion_probabilities = [0.0] * 7
        self.emotion_confidence = 0.0

        # é¢éƒ¨ç‰¹å¾ç‚¹ç´¢å¼•
        (self.lStart, self.lEnd) = face_utils.FACIAL_LANDMARKS_IDXS["left_eye"]
        (self.rStart, self.rEnd) = face_utils.FACIAL_LANDMARKS_IDXS["right_eye"]
        (self.nStart, self.nEnd) = face_utils.FACIAL_LANDMARKS_IDXS["nose"]
        (self.mStart, self.mEnd) = face_utils.FACIAL_LANDMARKS_IDXS["mouth"]
        (self.jStart, self.jEnd) = face_utils.FACIAL_LANDMARKS_IDXS["jaw"]

    def detect_face_dlib(self, frame):
        """ä½¿ç”¨dlibæ£€æµ‹é¢éƒ¨"""
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        rects = self.face_detector(gray, 1)
        return gray, rects

    def extract_face_features(self, gray, rect):
        """æå–é¢éƒ¨ç‰¹å¾"""
        try:
            shape = self.predictor(gray, rect)
            shape = face_utils.shape_to_np(shape)

            # è·å–é¢éƒ¨åæ ‡
            (x, y, w, h) = face_utils.rect_to_bb(rect)
            face = gray[y:y + h, x:x + w]

            # ç¼©æ”¾é¢éƒ¨å›¾åƒ
            if face.size == 0:
                return None

            face_resized = zoom(face, (self.shape_x / face.shape[0], self.shape_y / face.shape[1]))

            # è½¬æ¢ä¸ºæµ®ç‚¹æ•°å¹¶å½’ä¸€åŒ–
            face_resized = face_resized.astype(np.float32)
            if face_resized.max() > 0:
                face_resized /= float(face_resized.max())

            # é‡å¡‘ä¸ºæ¨¡å‹è¾“å…¥å½¢çŠ¶
            face_resized = np.reshape(face_resized, (1, self.shape_x, self.shape_y, 1))

            return face_resized, shape, (x, y, w, h)
        except Exception as e:
            print(f"é¢éƒ¨ç‰¹å¾æå–é”™è¯¯: {e}")
            return None

    def predict_emotion(self, face_image):
        """é¢„æµ‹æƒ…ç»ª"""
        if self.model is None or face_image is None:
            return "ä¸­æ€§", [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.4], 0.5

        try:
            prediction = self.model.predict(face_image, verbose=0)
            emotion_idx = np.argmax(prediction[0])
            emotion = self.emotion_labels[emotion_idx]
            confidence = float(prediction[0][emotion_idx])

            return emotion, prediction[0].tolist(), confidence
        except Exception as e:
            print(f"æƒ…ç»ªé¢„æµ‹é”™è¯¯: {e}")
            return "ä¸­æ€§", [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.4], 0.5

    def analyze_frame(self, frame):
        """åˆ†æå•å¸§æƒ…ç»ª"""
        try:
            # ä½¿ç”¨dlibæ£€æµ‹é¢éƒ¨
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            rects = self.face_detector(gray, 1)

            emotions = []
            face_shapes = []
            face_boxes = []

            for rect in rects:
                # æå–é¢éƒ¨ç‰¹å¾
                result = self.extract_face_features(gray, rect)
                if result is None:
                    continue

                face_image, shape, (x, y, w, h) = result

                # é¢„æµ‹æƒ…ç»ª
                emotion, probabilities, confidence = self.predict_emotion(face_image)

                emotions.append({
                    "emotion": emotion,
                    "probabilities": probabilities,
                    "confidence": confidence,
                    "box": (x, y, w, h)
                })

                face_shapes.append(shape)
                face_boxes.append((x, y, w, h))

            # æ›´æ–°å½“å‰æƒ…ç»ªï¼ˆä½¿ç”¨æœ€å¤§é¢éƒ¨æˆ–å¹³å‡ï¼‰
            if emotions:
                # é€‰æ‹©æœ€å¤§é¢éƒ¨çš„æƒ…ç»ª
                max_face_idx = max(range(len(face_boxes)),
                                   key=lambda i: face_boxes[i][2] * face_boxes[i][3])

                self.current_emotion = emotions[max_face_idx]["emotion"]
                self.emotion_probabilities = emotions[max_face_idx]["probabilities"]
                self.emotion_confidence = emotions[max_face_idx]["confidence"]

                # æ›´æ–°å†å²è®°å½•
                self.emotion_history.append(self.current_emotion)
            else:
                # æ²¡æœ‰æ£€æµ‹åˆ°é¢éƒ¨
                self.current_emotion = "æœªæ£€æµ‹åˆ°é¢éƒ¨"
                self.emotion_probabilities = [0.0] * 7
                self.emotion_confidence = 0.0

            # ç¡®ä¿face_countæ­£ç¡®è¿”å›
            face_count = len(rects)

            return {
                "emotion": self.current_emotion,
                "probabilities": self.emotion_probabilities,
                "confidence": self.emotion_confidence,
                "face_count": face_count,  # ç¡®ä¿è¿™é‡Œè¿”å›æ­£ç¡®çš„é¢éƒ¨æ•°é‡
                "face_shapes": face_shapes,
                "face_boxes": face_boxes
            }

        except Exception as e:
            print(f"æƒ…ç»ªåˆ†æé”™è¯¯: {e}")
            return {
                "emotion": "é”™è¯¯",
                "probabilities": [0.0] * 7,
                "confidence": 0.0,
                "face_count": 0,  # é”™è¯¯æ—¶ä¹Ÿè¿”å›0
                "face_shapes": [],
                "face_boxes": []
            }

    def get_emotion_stats(self):
        """è·å–æƒ…ç»ªç»Ÿè®¡"""
        if not self.emotion_history:
            return {
                "dominant_emotion": "æœªçŸ¥",
                "emotion_stability": 0,
                "positive_ratio": 0,
                "negative_ratio": 0,
                "emotion_changes": 0
            }

        try:
            history = list(self.emotion_history)

            # ä¸»å¯¼æƒ…ç»ª
            from collections import Counter
            emotion_counts = Counter(history)
            dominant_emotion = emotion_counts.most_common(1)[0][0] if emotion_counts else "æœªçŸ¥"

            # æƒ…ç»ªç¨³å®šæ€§ï¼ˆç›¸åŒæƒ…ç»ªè¿ç»­å¸§çš„æ¯”ä¾‹ï¼‰
            if len(history) > 1:
                changes = sum(1 for i in range(1, len(history)) if history[i] != history[i - 1])
                stability = 1 - (changes / (len(history) - 1))
            else:
                stability = 1.0

            # ç§¯æ/æ¶ˆææƒ…ç»ªæ¯”ä¾‹
            positive_emotions = ["å¿«ä¹", "æƒŠè®¶", "ä¸­æ€§"]
            negative_emotions = ["ç”Ÿæ°”", "åŒæ¶", "ææƒ§", "æ‚²ä¼¤"]

            positive_count = sum(1 for e in history if e in positive_emotions)
            negative_count = sum(1 for e in history if e in negative_emotions)

            positive_ratio = positive_count / len(history) if history else 0
            negative_ratio = negative_count / len(history) if history else 0

            # æƒ…ç»ªå˜åŒ–æ¬¡æ•°
            emotion_changes = sum(1 for i in range(1, len(history)) if history[i] != history[i - 1])

            return {
                "dominant_emotion": dominant_emotion,
                "emotion_stability": round(stability * 100, 1),
                "positive_ratio": round(positive_ratio * 100, 1),
                "negative_ratio": round(negative_ratio * 100, 1),
                "emotion_changes": emotion_changes
            }
        except:
            return {
                "dominant_emotion": "æœªçŸ¥",
                "emotion_stability": 0,
                "positive_ratio": 0,
                "negative_ratio": 0,
                "emotion_changes": 0
            }

    def reset(self):
        """é‡ç½®åˆ†æå™¨"""
        self.emotion_history.clear()
        self.current_emotion = "ä¸­æ€§"
        self.emotion_probabilities = [0.0] * 7
        self.emotion_confidence = 0.0


# ============================================================================
# è¯­éŸ³æé†’ç³»ç»Ÿ
# ============================================================================

class VoiceReminderSystem:
    """å¢å¼ºç‰ˆè¯­éŸ³æé†’ç³»ç»Ÿ"""

    def __init__(self):
        self.engine = None
        self.is_speaking = False
        self.last_reminder_time = 0
        self.reminder_cooldown = 15

        # è¯­éŸ³é˜Ÿåˆ—å’Œçº¿ç¨‹
        self.voice_queue = queue.Queue()
        self.voice_thread = None
        self.voice_thread_running = False

        # è¯­éŸ³è®¾ç½®
        self.speech_rate = 150
        self.volume = 0.8
        self.pitch = 110

        if TTS_AVAILABLE:
            try:
                self.engine = pyttsx3.init()
                self.setup_voice_engine()
                self.start_voice_thread()
                print("è¯­éŸ³ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"åˆå§‹åŒ–è¯­éŸ³å¼•æ“å¤±è´¥: {e}")
                self.engine = None
        else:
            print("è¯­éŸ³åŠŸèƒ½ä¸å¯ç”¨")

    def setup_voice_engine(self):
        """è®¾ç½®è¯­éŸ³å¼•æ“å‚æ•°"""
        if not self.engine:
            return

        try:
            voices = self.engine.getProperty('voices')
            chinese_voices = []
            female_voices = []

            for voice in voices:
                voice_info = voice.name.lower()
                if 'chinese' in voice_info or 'zh' in voice_info:
                    chinese_voices.append(voice)
                elif 'female' in voice_info or 'f' in voice_info:
                    female_voices.append(voice)

            if chinese_voices:
                self.engine.setProperty('voice', chinese_voices[0].id)
            elif female_voices:
                self.engine.setProperty('voice', female_voices[0].id)

            self.engine.setProperty('rate', self.speech_rate)
            self.engine.setProperty('volume', self.volume)
            self.engine.setProperty('pitch', self.pitch)

        except Exception as e:
            print(f"è¯­éŸ³è®¾ç½®é”™è¯¯: {e}")

    def start_voice_thread(self):
        """å¯åŠ¨è¯­éŸ³çº¿ç¨‹"""
        if self.engine and not self.voice_thread_running:
            self.voice_thread_running = True
            self.voice_thread = threading.Thread(
                target=self._voice_worker,
                daemon=True,
                name="è¯­éŸ³å·¥ä½œçº¿ç¨‹"
            )
            self.voice_thread.start()

    def _voice_worker(self):
        """è¯­éŸ³å·¥ä½œçº¿ç¨‹"""
        while self.voice_thread_running:
            try:
                text = self.voice_queue.get(timeout=2)

                if self.engine:
                    try:
                        self.engine.say(text)
                        self.engine.runAndWait()
                        self.is_speaking = False
                    except Exception as e:
                        print(f"è¯­éŸ³æ’­æ”¾é”™è¯¯: {e}")
                        self.is_speaking = False

                self.voice_queue.task_done()

            except queue.Empty:
                continue
            except Exception as e:
                print(f"è¯­éŸ³å·¥ä½œçº¿ç¨‹é”™è¯¯: {e}")
                continue

    def speak(self, text):
        """è¯­éŸ³æ’­æŠ¥"""
        if self.engine is None:
            return False

        current_time = time.time()

        if current_time - self.last_reminder_time < self.reminder_cooldown:
            return False

        try:
            self.voice_queue.put(text)
            self.last_reminder_time = current_time
            self.is_speaking = True
            return True
        except Exception as e:
            print(f"æ·»åŠ è¯­éŸ³åˆ°é˜Ÿåˆ—å¤±è´¥: {e}")
            return False

    def stop(self):
        """åœæ­¢è¯­éŸ³ç³»ç»Ÿ"""
        self.voice_thread_running = False
        if self.voice_thread:
            self.voice_thread.join(timeout=2)
        if self.engine:
            try:
                self.engine.stop()
            except:
                pass


# ============================================================================
# ä¸»UIç•Œé¢
# ============================================================================

class ADHDDetectionSystem(QMainWindow):
    """å¤šåŠ¨ç—‡å„¿ç«¥æ³¨æ„åŠ›ä¸æƒ…ç»ªæ£€æµ‹ç³»ç»Ÿ"""

    # æ·»åŠ ä¿¡å·
    modeling_progress_updated = pyqtSignal(int)
    modeling_finished = pyqtSignal(bool)

    def __init__(self):
        super().__init__()

        # è®¾ç½®ä¸­æ–‡å­—ä½“
        QFontDatabase.addApplicationFont("msyh.ttc")  # å¾®è½¯é›…é»‘
        font = QFont("Microsoft YaHei", 9)
        QApplication.setFont(font)

        # åˆå§‹åŒ–åˆ†æå™¨
        self.attention_analyzer = AttentionAnalyzer()
        self.emotion_analyzer = EmotionAnalyzer()
        self.voice_system = VoiceReminderSystem()

        # åˆå§‹åŒ–æ–°åŠŸèƒ½
        self.facial_modeling = FacialModeling()
        self.attention_scoring = OptimizedAttentionScoringSystem()
        self.calibration_system = CalibrationSystem()
        self.realtime_charts = RealTimeCharts()

        # æ‘„åƒå¤´å’Œè§†é¢‘
        self.camera = None
        self.video_path = None
        self.video_capture = None
        self.is_live = False
        self.is_playing = False

        # è®°å½•çŠ¶æ€
        self.is_recording = False
        self.video_writer = None
        self.record_data = []
        self.session_start_time = None
        self.frame_count = 0

        # åˆå§‹åŒ–UIç›¸å…³çš„å±æ€§
        self.neutral_duration_label = None
        self.attention_stability_label = None
        self.gaze_deviation_label = None
        self.head_stability_label = None
        self.focus_duration_label = None
        self.distraction_count_label = None
        self.refocus_rate_label = None
        self.emotion_change_freq_label = None
        self.extreme_emotion_label = None
        self.face_count_label = None

        # æ˜¾ç¤ºè®¾ç½®
        self.show_attention_overlay = True
        self.show_emotion_overlay = True
        self.show_landmarks = True
        self.show_calibration = False

        # åˆå§‹åŒ–æ‰€æœ‰UIç›¸å…³çš„å±æ€§
        self.min_attention_label = None
        self.focus_percent_label = None
        self.trend_label = None
        self.blink_rate_label = None
        self.dominant_emotion_label = None
        self.emotion_stability_label = None
        self.positive_ratio_label = None
        self.negative_ratio_label = None

        # è¯¦ç»†ç»Ÿè®¡æ ‡ç­¾
        self.attention_stability_label = None
        self.gaze_deviation_label = None
        self.head_stability_label = None
        self.focus_duration_label = None
        self.distraction_count_label = None
        self.refocus_rate_label = None

        # æƒ…ç»ªç»Ÿè®¡æ ‡ç­¾
        self.face_count_label = None
        self.emotion_change_freq_label = None
        self.neutral_duration_label = None
        self.extreme_emotion_label = None

        # å¤šåŠ¨ç—‡ç‰¹å¾æ ‡ç­¾
        self.inattention_ratio_label = None
        self.hyperactivity_label = None
        self.emotion_volatility_label = None
        self.risk_level_label = None
        self.focus_pattern_label = None
        self.adhd_features_label = None

        # çŠ¶æ€å˜é‡
        self.attention_stats_history = deque(maxlen=100)
        self.emotion_stats_history = deque(maxlen=100)
        self.alerts = []
        self.is_calibrating = False
        self.calibration_step = 0

        # åˆå§‹åŒ–UI
        self.init_ui()

        # å®šæ—¶å™¨
        self.timer = QTimer()
        self.timer.timeout.connect(self.update_frame)

        # çŠ¶æ€æ›´æ–°å®šæ—¶å™¨
        self.status_timer = QTimer()
        self.status_timer.timeout.connect(self.update_status)
        self.status_timer.start(1000)

        # å›¾è¡¨æ›´æ–°å®šæ—¶å™¨
        self.chart_timer = QTimer()
        self.chart_timer.timeout.connect(self.update_charts_widgets)
        self.chart_timer.start(200)  # 5 FPSæ›´æ–°å›¾è¡¨

        # è¯­éŸ³æ§åˆ¶
        self.voice_enabled = True

        # è¿æ¥ä¿¡å·å’Œæ§½
        self.modeling_progress_updated.connect(self.update_modeling_progress)
        self.modeling_finished.connect(self.finish_modeling)

        print("å¤šåŠ¨ç—‡æ£€æµ‹ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

    def init_ui(self):
        """åˆå§‹åŒ–UIç•Œé¢"""
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        self.setFont(QFont("Microsoft YaHei", 9))

        self.setWindowTitle("å¤šåŠ¨ç—‡å„¿ç«¥æ³¨æ„åŠ›ä¸æƒ…ç»ªæ£€æµ‹ç³»ç»Ÿ v5.0")
        self.setGeometry(100, 100, 1200, 800)

        # è®¾ç½®æ ·å¼
        self.setStyleSheet("""
            QMainWindow {
                background-color: #f5f7fa;
            }
            QGroupBox {
                font-size: 14px;
                font-weight: bold;
                border: 2px solid #4a6fa5;
                border-radius: 8px;
                margin-top: 10px;
                padding-top: 10px;
                background-color: white;
            }
            QGroupBox::title {
                subcontrol-origin: margin;
                left: 10px;
                padding: 0 8px 0 8px;
                color: #2c3e50;
            }
            QPushButton {
                font-size: 13px;
                font-weight: bold;
                padding: 8px 15px;
                border-radius: 6px;
                background-color: #4a6fa5;
                color: white;
                border: 1px solid #385d8a;
            }
            QPushButton:hover {
                background-color: #385d8a;
            }
            QPushButton:pressed {
                background-color: #2c4a6e;
            }
            QLabel {
                font-size: 13px;
                color: #34495e;
            }
            QTextEdit {
                font-size: 12px;
                border: 1px solid #d1d9e6;
                border-radius: 4px;
                background-color: white;
                padding: 5px;
            }
            QProgressBar {
                border: 1px solid #d1d9e6;
                border-radius: 4px;
                text-align: center;
                background-color: white;
            }
            QProgressBar::chunk {
                border-radius: 4px;
                background-color: #3498db;
            }
            QCheckBox {
                font-size: 13px;
                color: #34495e;
            }
            QComboBox {
                padding: 5px;
                border: 1px solid #d1d9e6;
                border-radius: 4px;
                background-color: white;
            }
            QSlider::groove:horizontal {
                height: 6px;
                background: #d1d9e6;
                border-radius: 3px;
            }
            QSlider::handle:horizontal {
                background: #3498db;
                border: 1px solid #2980b9;
                width: 18px;
                height: 18px;
                margin: -6px 0;
                border-radius: 9px;
            }
        """)

        # åˆ›å»ºä¸­å¤®éƒ¨ä»¶
        central_widget = QWidget()
        self.setCentralWidget(central_widget)

        # ä¸»å¸ƒå±€
        main_layout = QHBoxLayout()
        main_layout.setSpacing(10)
        main_layout.setContentsMargins(10, 10, 10, 10)
        central_widget.setLayout(main_layout)

        # ====================================================================
        # å·¦ä¾§é¢æ¿ï¼šè§†é¢‘å’Œå›¾è¡¨
        # ====================================================================
        left_panel = QVBoxLayout()
        left_panel.setSpacing(10)

        # è§†é¢‘æ˜¾ç¤ºåŒºåŸŸ
        video_group = QGroupBox("ğŸ“¹ å®æ—¶è§†é¢‘ç”»é¢")
        video_layout = QVBoxLayout()

        self.video_label = QLabel()
        self.video_label.setAlignment(Qt.AlignCenter)
        self.video_label.setMinimumSize(1000, 450)  # å‡å°å°ºå¯¸
        self.video_label.setMaximumSize(1000, 450)  # å‡å°å°ºå¯¸
        self.video_label.setStyleSheet("""
            background-color: #2c3e50;
            border: 3px solid #4a6fa5;
            border-radius: 8px;
            color: white;
            font-size: 16px;
        """)
        self.video_label.setText("ç­‰å¾…è§†é¢‘æº...")

        # è§†é¢‘ä¿¡æ¯æ ‡ç­¾
        self.video_info_label = QLabel("å°±ç»ª")
        self.video_info_label.setStyleSheet("color: #7f8c8d; font-size: 12px;")
        self.video_info_label.setAlignment(Qt.AlignCenter)

        video_layout.addWidget(self.video_label)
        video_layout.addWidget(self.video_info_label)

        # è§†é¢‘æ§åˆ¶æŒ‰é’®
        control_layout = QHBoxLayout()

        self.camera_btn = QPushButton("ğŸ“· å¯åŠ¨æ‘„åƒå¤´")
        self.camera_btn.setStyleSheet("background-color: #27ae60;")
        self.camera_btn.clicked.connect(self.start_camera)

        self.video_btn = QPushButton("ğŸ“ ä¸Šä¼ è§†é¢‘")
        self.video_btn.setStyleSheet("background-color: #e67e22;")
        self.video_btn.clicked.connect(self.upload_video)

        self.record_btn = QPushButton("â— å¼€å§‹å½•åˆ¶")
        self.record_btn.setStyleSheet("background-color: #e74c3c;")
        self.record_btn.clicked.connect(self.toggle_recording)
        self.record_btn.setEnabled(False)

        self.pause_btn = QPushButton("â¸ï¸ æš‚åœ")
        self.pause_btn.setStyleSheet("background-color: #f39c12;")
        self.pause_btn.clicked.connect(self.toggle_pause)
        self.pause_btn.setEnabled(False)

        self.calibrate_btn = QPushButton("ğŸ¯ å¼€å§‹æ ¡å‡†")
        self.calibrate_btn.setStyleSheet("background-color: #9b59b6;")
        self.calibrate_btn.clicked.connect(self.start_calibration)
        self.calibrate_btn.setEnabled(False)

        control_layout.addWidget(self.camera_btn)
        control_layout.addWidget(self.video_btn)
        control_layout.addWidget(self.record_btn)
        control_layout.addWidget(self.pause_btn)
        control_layout.addWidget(self.calibrate_btn)
        control_layout.addStretch()

        video_layout.addLayout(control_layout)
        video_group.setLayout(video_layout)
        left_panel.addWidget(video_group)

        # å®æ—¶å›¾è¡¨åŒºåŸŸ
        charts_group = QGroupBox("ğŸ“Š å®æ—¶å›¾è¡¨")
        charts_layout = QGridLayout()
        charts_layout.setHorizontalSpacing(5)  # å‡å°‘æ°´å¹³é—´è·
        charts_layout.setVerticalSpacing(5)  # å‡å°‘å‚ç›´é—´è·

        # åˆ›å»ºè‡ªå®šä¹‰å›¾è¡¨æ ‡ç­¾
        self.attention_chart_widget = QLabel()
        self.attention_chart_widget.setAlignment(Qt.AlignCenter)
        self.attention_chart_widget.setMinimumSize(320, 160)  # å‡å°å°ºå¯¸
        self.attention_chart_widget.setMaximumSize(320, 160)  # å‡å°å°ºå¯¸
        self.attention_chart_widget.setStyleSheet("""
            background-color: white;
            border: 1px solid #d1d9e6;
            border-radius: 4px;
        """)
        self.attention_chart_widget.setText("æ­£åœ¨åˆå§‹åŒ–...")

        self.gaze_chart_widget = QLabel()
        self.gaze_chart_widget.setAlignment(Qt.AlignCenter)
        self.gaze_chart_widget.setMinimumSize(320, 160)  # å‡å°å°ºå¯¸
        self.gaze_chart_widget.setMaximumSize(320, 160)  # å‡å°å°ºå¯¸
        self.gaze_chart_widget.setStyleSheet("""
            background-color: white;
            border: 1px solid #d1d9e6;
            border-radius: 4px;
        """)
        self.gaze_chart_widget.setText("æ­£åœ¨åˆå§‹åŒ–...")

        self.eye_chart_widget = QLabel()
        self.eye_chart_widget.setAlignment(Qt.AlignCenter)
        self.eye_chart_widget.setMinimumSize(320, 160)  # å‡å°å°ºå¯¸
        self.eye_chart_widget.setMaximumSize(320, 160)  # å‡å°å°ºå¯¸
        self.eye_chart_widget.setStyleSheet("""
            background-color: white;
            border: 1px solid #d1d9e6;
            border-radius: 4px;
        """)
        self.eye_chart_widget.setText("æ­£åœ¨åˆå§‹åŒ–...")

        self.attention_chart_title = QLabel("æ³¨æ„åŠ›åˆ†æ•°è¶‹åŠ¿")
        self.attention_chart_title.setAlignment(Qt.AlignCenter)
        self.attention_chart_title.setStyleSheet("font-weight: bold; font-size: 12px;")

        self.gaze_chart_title = QLabel("è§†çº¿è¿½è¸ª")
        self.gaze_chart_title.setAlignment(Qt.AlignCenter)
        self.gaze_chart_title.setStyleSheet("font-weight: bold; font-size: 12px;")

        self.eye_chart_title = QLabel("çœ¼éƒ¨ä¸å¤´éƒ¨ç‰¹å¾")
        self.eye_chart_title.setAlignment(Qt.AlignCenter)
        self.eye_chart_title.setStyleSheet("font-weight: bold; font-size: 12px;")

        charts_layout.addWidget(self.attention_chart_title, 0, 0)
        charts_layout.addWidget(self.gaze_chart_title, 0, 1)
        charts_layout.addWidget(self.eye_chart_title, 0, 2)

        charts_layout.addWidget(self.attention_chart_widget, 1, 0)
        charts_layout.addWidget(self.gaze_chart_widget, 1, 1)
        charts_layout.addWidget(self.eye_chart_widget, 1, 2)

        charts_group.setLayout(charts_layout)
        left_panel.addWidget(charts_group)

        # ====================================================================
        # å³ä¾§é¢æ¿ï¼šåˆ†æå’Œæ§åˆ¶
        # ====================================================================
        right_panel = QVBoxLayout()
        right_panel.setSpacing(10)

        # åˆ›å»ºé€‰é¡¹å¡
        self.right_tab_widget = QTabWidget()

        # åˆ›å»ºå„ä¸ªé€‰é¡¹å¡é¡µé¢
        self.attention_emotion_tab = QWidget()
        self.calibration_tab = QWidget()
        self.control_tab = QWidget()
        self.stats_tab = QWidget()
        self.alert_tab = QWidget()

        # è®¾ç½®å„ä¸ªé€‰é¡¹å¡çš„å¸ƒå±€
        self.setup_attention_emotion_tab()
        self.setup_calibration_tab()
        self.setup_control_tab()
        self.setup_stats_tab()
        self.setup_alert_tab()

        # æ·»åŠ é€‰é¡¹å¡
        self.right_tab_widget.addTab(self.attention_emotion_tab, "ğŸ¯ æ³¨æ„åŠ›ä¸æƒ…ç»ª")
        self.right_tab_widget.addTab(self.calibration_tab, "ğŸ¯ æ ¡å‡†")
        self.right_tab_widget.addTab(self.control_tab, "âš™ï¸ æ§åˆ¶")
        self.right_tab_widget.addTab(self.stats_tab, "ğŸ“ˆ ç»Ÿè®¡")
        self.right_tab_widget.addTab(self.alert_tab, "âš ï¸ è­¦æŠ¥")

        right_panel.addWidget(self.right_tab_widget)

        # æ“ä½œæŒ‰é’®ç»„ï¼ˆæ”¾åœ¨é€‰é¡¹å¡ä¸‹æ–¹ï¼‰
        action_group = QGroupBox("ğŸ› ï¸ æ“ä½œ")
        action_layout = QHBoxLayout()

        self.export_btn = QPushButton("ğŸ“Š å¯¼å‡ºæŠ¥å‘Š")
        self.export_btn.clicked.connect(self.export_report)
        self.export_btn.setStyleSheet("background-color: #9b59b6;")

        self.reset_btn = QPushButton("ğŸ”„ é‡ç½®åˆ†æ")
        self.reset_btn.clicked.connect(self.reset_analysis)
        self.reset_btn.setStyleSheet("background-color: #95a5a6;")

        self.quit_btn = QPushButton("ğŸšª é€€å‡º")
        self.quit_btn.clicked.connect(self.close)
        self.quit_btn.setStyleSheet("background-color: #e74c3c;")

        action_layout.addWidget(self.export_btn)
        action_layout.addWidget(self.reset_btn)
        action_layout.addWidget(self.quit_btn)
        action_group.setLayout(action_layout)
        right_panel.addWidget(action_group)

        # å°†å·¦å³é¢æ¿æ·»åŠ åˆ°ä¸»å¸ƒå±€
        main_layout.addLayout(left_panel, 3)
        main_layout.addLayout(right_panel, 2)

    def setup_attention_emotion_tab(self):
        """è®¾ç½®æ³¨æ„åŠ›ä¸æƒ…ç»ªé€‰é¡¹å¡"""
        layout = QVBoxLayout()
        layout.setSpacing(10)

        # æ³¨æ„åŠ›åˆ†æç»„
        attention_group = QGroupBox("ğŸ¯ æ³¨æ„åŠ›åˆ†æ")
        attention_layout = QVBoxLayout()

        # æ³¨æ„åŠ›åˆ†æ•°
        score_layout = QHBoxLayout()
        score_layout.addWidget(QLabel("æ³¨æ„åŠ›åˆ†æ•°:"))
        self.attention_score_label = QLabel("0")
        self.attention_score_label.setStyleSheet("font-size: 24px; font-weight: bold; color: #2c3e50;")
        score_layout.addWidget(self.attention_score_label)
        score_layout.addStretch()

        # æ³¨æ„åŠ›çŠ¶æ€
        state_layout = QHBoxLayout()
        state_layout.addWidget(QLabel("çŠ¶æ€:"))
        self.attention_state_label = QLabel("åˆå§‹åŒ–ä¸­")
        self.attention_state_label.setStyleSheet("font-size: 16px; font-weight: bold; color: #7f8c8d;")
        state_layout.addWidget(self.attention_state_label)
        state_layout.addStretch()

        # æ³¨æ„åŠ›è¿›åº¦æ¡
        self.attention_progress = QProgressBar()
        self.attention_progress.setRange(0, 100)
        self.attention_progress.setValue(0)
        self.attention_progress.setTextVisible(True)
        self.attention_progress.setFormat("%v/100")

        # è¯¦ç»†æŒ‡æ ‡
        metrics_layout = QGridLayout()
        metrics_layout.addWidget(QLabel("çœ¼ç›çºµæ¨ªæ¯”:"), 0, 0)
        self.ear_label = QLabel("0.00")
        metrics_layout.addWidget(self.ear_label, 0, 1)

        metrics_layout.addWidget(QLabel("å¤´éƒ¨åè½¬:"), 1, 0)
        self.yaw_label = QLabel("0.0Â°")
        metrics_layout.addWidget(self.yaw_label, 1, 1)

        metrics_layout.addWidget(QLabel("å¤´éƒ¨ä¿¯ä»°:"), 2, 0)
        self.pitch_label = QLabel("0.0Â°")
        metrics_layout.addWidget(self.pitch_label, 2, 1)

        metrics_layout.addWidget(QLabel("è§†çº¿X:"), 0, 2)
        self.gaze_x_label = QLabel("0.00")
        metrics_layout.addWidget(self.gaze_x_label, 0, 3)

        metrics_layout.addWidget(QLabel("è§†çº¿Y:"), 1, 2)
        self.gaze_y_label = QLabel("0.00")
        metrics_layout.addWidget(self.gaze_y_label, 1, 3)

        metrics_layout.addWidget(QLabel("çœ¨çœ¼æ¬¡æ•°:"), 2, 2)
        self.blink_label = QLabel("0")
        metrics_layout.addWidget(self.blink_label, 2, 3)

        attention_layout.addLayout(score_layout)
        attention_layout.addLayout(state_layout)
        attention_layout.addWidget(self.attention_progress)
        attention_layout.addLayout(metrics_layout)
        attention_group.setLayout(attention_layout)
        layout.addWidget(attention_group)

        # æƒ…ç»ªåˆ†æç»„
        emotion_group = QGroupBox("ğŸ˜Š æƒ…ç»ªåˆ†æ")
        emotion_layout = QVBoxLayout()

        # å½“å‰æƒ…ç»ª
        current_emotion_layout = QHBoxLayout()
        current_emotion_layout.addWidget(QLabel("å½“å‰æƒ…ç»ª:"))
        self.emotion_label = QLabel("ä¸­æ€§")
        self.emotion_label.setStyleSheet("font-size: 20px; font-weight: bold; color: #2c3e50;")
        current_emotion_layout.addWidget(self.emotion_label)
        current_emotion_layout.addStretch()

        # æƒ…ç»ªä¿¡å¿ƒ
        confidence_layout = QHBoxLayout()
        confidence_layout.addWidget(QLabel("ç½®ä¿¡åº¦:"))
        self.confidence_label = QLabel("0%")
        self.confidence_label.setStyleSheet("font-size: 16px; font-weight: bold; color: #7f8c8d;")
        confidence_layout.addWidget(self.confidence_label)
        confidence_layout.addStretch()

        # æƒ…ç»ªæ¦‚ç‡æ¡
        self.emotion_bars = {}
        emotions = ["ç”Ÿæ°”", "åŒæ¶", "ææƒ§", "å¿«ä¹", "æ‚²ä¼¤", "æƒŠè®¶", "ä¸­æ€§"]

        for emotion in emotions:
            emotion_bar_layout = QHBoxLayout()
            emotion_bar_layout.addWidget(QLabel(f"{emotion}:"))

            progress_bar = QProgressBar()
            progress_bar.setRange(0, 100)
            progress_bar.setValue(0)
            progress_bar.setTextVisible(True)
            progress_bar.setFormat("%v%")
            progress_bar.setMaximumHeight(20)

            self.emotion_bars[emotion] = progress_bar
            emotion_bar_layout.addWidget(progress_bar)
            emotion_layout.addLayout(emotion_bar_layout)

        emotion_layout.addLayout(current_emotion_layout)
        emotion_layout.addLayout(confidence_layout)
        emotion_group.setLayout(emotion_layout)
        layout.addWidget(emotion_group)

        layout.addStretch()
        self.attention_emotion_tab.setLayout(layout)

    def setup_calibration_tab(self):
        """è®¾ç½®æ ¡å‡†é€‰é¡¹å¡"""
        layout = QVBoxLayout()
        layout.setSpacing(10)

        # æ ¡å‡†çŠ¶æ€ç»„
        calibration_group = QGroupBox("ğŸ¯ æ ¡å‡†çŠ¶æ€")
        calibration_layout = QVBoxLayout()

        self.calibration_status_label = QLabel("æœªæ ¡å‡†")
        self.calibration_status_label.setStyleSheet("font-size: 14px; font-weight: bold; color: #7f8c8d;")

        self.calibration_progress = QProgressBar()
        self.calibration_progress.setRange(0, 100)
        self.calibration_progress.setValue(0)
        self.calibration_progress.setTextVisible(True)
        self.calibration_progress.setFormat("æ ¡å‡†è¿›åº¦: %p%")

        self.calibration_instruction = QLabel("ç‚¹å‡»'å¼€å§‹æ ¡å‡†'æŒ‰é’®è¿›è¡Œæ ¡å‡†")
        self.calibration_instruction.setStyleSheet("color: #95a5a6; font-size: 11px;")
        self.calibration_instruction.setWordWrap(True)

        self.calibration_info = QLabel("")
        self.calibration_info.setStyleSheet("color: #34495e; font-size: 10px;")
        self.calibration_info.setWordWrap(True)

        calibration_layout.addWidget(self.calibration_status_label)
        calibration_layout.addWidget(self.calibration_progress)
        calibration_layout.addWidget(self.calibration_instruction)
        calibration_layout.addWidget(self.calibration_info)

        # æ ¡å‡†æ§åˆ¶æŒ‰é’®
        calibration_buttons = QHBoxLayout()
        self.calibration_reset_btn = QPushButton("é‡ç½®æ ¡å‡†")
        self.calibration_reset_btn.setStyleSheet("background-color: #95a5a6;")
        self.calibration_reset_btn.clicked.connect(self.reset_calibration)
        self.calibration_reset_btn.setEnabled(False)

        self.calibration_auto_btn = QPushButton("è‡ªåŠ¨é¢éƒ¨å»ºæ¨¡")
        self.calibration_auto_btn.setStyleSheet("background-color: #3498db;")
        self.calibration_auto_btn.clicked.connect(self.auto_facial_modeling)
        self.calibration_auto_btn.setEnabled(False)

        calibration_buttons.addWidget(self.calibration_reset_btn)
        calibration_buttons.addWidget(self.calibration_auto_btn)
        calibration_buttons.addStretch()

        calibration_layout.addLayout(calibration_buttons)
        calibration_group.setLayout(calibration_layout)
        layout.addWidget(calibration_group)

        # æ ¡å‡†ç»“æœä¿¡æ¯
        result_group = QGroupBox("ğŸ“‹ æ ¡å‡†ç»“æœ")
        result_layout = QVBoxLayout()

        self.calibration_result_label = QLabel("æš‚æ— æ ¡å‡†ç»“æœ")
        self.calibration_result_label.setStyleSheet("color: #7f8c8d; font-size: 11px;")
        self.calibration_result_label.setWordWrap(True)

        result_layout.addWidget(self.calibration_result_label)
        result_group.setLayout(result_layout)
        layout.addWidget(result_group)

        layout.addStretch()
        self.calibration_tab.setLayout(layout)

    def setup_control_tab(self):
        """è®¾ç½®æ§åˆ¶é€‰é¡¹å¡"""
        layout = QVBoxLayout()
        layout.setSpacing(10)

        # æ˜¾ç¤ºè®¾ç½®ç»„
        display_group = QGroupBox("ğŸ‘ï¸ æ˜¾ç¤ºè®¾ç½®")
        display_layout = QVBoxLayout()

        self.show_attention_check = QCheckBox("æ˜¾ç¤ºæ³¨æ„åŠ›å åŠ ")
        self.show_attention_check.setChecked(True)
        self.show_attention_check.stateChanged.connect(self.toggle_attention_overlay)

        self.show_emotion_check = QCheckBox("æ˜¾ç¤ºæƒ…ç»ªå åŠ ")
        self.show_emotion_check.setChecked(True)
        self.show_emotion_check.stateChanged.connect(self.toggle_emotion_overlay)

        self.show_landmarks_check = QCheckBox("æ˜¾ç¤ºç‰¹å¾ç‚¹")
        self.show_landmarks_check.setChecked(True)

        display_layout.addWidget(self.show_attention_check)
        display_layout.addWidget(self.show_emotion_check)
        display_layout.addWidget(self.show_landmarks_check)
        display_group.setLayout(display_layout)
        layout.addWidget(display_group)

        # è¯­éŸ³æ§åˆ¶ç»„
        voice_group = QGroupBox("ğŸ”Š è¯­éŸ³æ§åˆ¶")
        voice_layout = QVBoxLayout()

        self.voice_check = QCheckBox("å¯ç”¨è¯­éŸ³æé†’")
        self.voice_check.setChecked(True)
        self.voice_check.stateChanged.connect(self.toggle_voice)

        voice_button_layout = QHBoxLayout()
        self.test_voice_btn = QPushButton("æµ‹è¯•è¯­éŸ³")
        self.test_voice_btn.clicked.connect(self.test_voice)
        self.test_voice_btn.setMaximumWidth(100)

        voice_button_layout.addWidget(self.test_voice_btn)
        voice_button_layout.addStretch()

        voice_layout.addWidget(self.voice_check)
        voice_layout.addLayout(voice_button_layout)
        voice_group.setLayout(voice_layout)
        layout.addWidget(voice_group)

        # åˆ†æè®¾ç½®ç»„
        analysis_group = QGroupBox("ğŸ” åˆ†æè®¾ç½®")
        analysis_layout = QVBoxLayout()

        self.smooth_check = QCheckBox("å¹³æ»‘åˆ†æ")
        self.smooth_check.setChecked(True)

        self.auto_reset_check = QCheckBox("è‡ªåŠ¨é‡ç½®")
        self.auto_reset_check.setChecked(False)

        analysis_layout.addWidget(self.smooth_check)
        analysis_layout.addWidget(self.auto_reset_check)
        analysis_group.setLayout(analysis_layout)
        layout.addWidget(analysis_group)

        layout.addStretch()
        self.control_tab.setLayout(layout)

    def setup_stats_tab(self):
        """è®¾ç½®ç»Ÿè®¡é€‰é¡¹å¡ï¼ˆå®Œæ•´ç‰ˆï¼‰"""
        layout = QVBoxLayout()
        layout.setSpacing(10)

        # åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯ç»„
        stats_group = QGroupBox("ğŸ“Š åŸºæœ¬ç»Ÿè®¡")
        stats_layout = QGridLayout()

        # ç¬¬ä¸€è¡Œï¼šå¹³å‡åˆ†æ•°å’Œæœ€é«˜åˆ†æ•°
        stats_layout.addWidget(QLabel("å¹³å‡åˆ†æ•°:"), 0, 0)
        self.avg_attention_label = QLabel("0")
        stats_layout.addWidget(self.avg_attention_label, 0, 1)

        stats_layout.addWidget(QLabel("æœ€é«˜åˆ†æ•°:"), 0, 2)
        self.max_attention_label = QLabel("0")
        stats_layout.addWidget(self.max_attention_label, 0, 3)

        # ç¬¬äºŒè¡Œï¼šæœ€ä½åˆ†æ•°å’Œä¸“æ³¨æ¯”ä¾‹
        stats_layout.addWidget(QLabel("æœ€ä½åˆ†æ•°:"), 1, 0)
        self.min_attention_label = QLabel("0")  # æ·»åŠ ç¼ºå¤±çš„æ ‡ç­¾
        stats_layout.addWidget(self.min_attention_label, 1, 1)

        stats_layout.addWidget(QLabel("ä¸“æ³¨æ¯”ä¾‹:"), 1, 2)
        self.focus_percent_label = QLabel("0%")
        stats_layout.addWidget(self.focus_percent_label, 1, 3)

        # ç¬¬ä¸‰è¡Œï¼šè¶‹åŠ¿å’Œçœ¨çœ¼é¢‘ç‡
        stats_layout.addWidget(QLabel("è¶‹åŠ¿:"), 2, 0)
        self.trend_label = QLabel("ç¨³å®š")
        stats_layout.addWidget(self.trend_label, 2, 1)

        stats_layout.addWidget(QLabel("çœ¨çœ¼é¢‘ç‡:"), 2, 2)
        self.blink_rate_label = QLabel("0/åˆ†é’Ÿ")
        stats_layout.addWidget(self.blink_rate_label, 2, 3)

        # ç¬¬å››è¡Œï¼šä¸»å¯¼æƒ…ç»ª
        stats_layout.addWidget(QLabel("ä¸»å¯¼æƒ…ç»ª:"), 3, 0)
        self.dominant_emotion_label = QLabel("æœªçŸ¥")
        stats_layout.addWidget(self.dominant_emotion_label, 3, 1)

        stats_layout.addWidget(QLabel("æƒ…ç»ªç¨³å®šæ€§:"), 3, 2)
        self.emotion_stability_label = QLabel("0%")
        stats_layout.addWidget(self.emotion_stability_label, 3, 3)

        stats_group.setLayout(stats_layout)
        layout.addWidget(stats_group)

        # è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ç»„
        detail_stats_group = QGroupBox("ğŸ“ˆ è¯¦ç»†ç»Ÿè®¡")
        detail_stats_layout = QGridLayout()

        # ç¬¬ä¸€è¡Œ
        detail_stats_layout.addWidget(QLabel("æ³¨æ„åŠ›ç¨³å®šæ€§:"), 0, 0)
        self.attention_stability_label = QLabel("0%")
        detail_stats_layout.addWidget(self.attention_stability_label, 0, 1)

        detail_stats_layout.addWidget(QLabel("è§†çº¿åç§»åº¦:"), 0, 2)
        self.gaze_deviation_label = QLabel("0.00")
        detail_stats_layout.addWidget(self.gaze_deviation_label, 0, 3)

        # ç¬¬äºŒè¡Œ
        detail_stats_layout.addWidget(QLabel("å¤´éƒ¨ç¨³å®šæ€§:"), 1, 0)
        self.head_stability_label = QLabel("0%")
        detail_stats_layout.addWidget(self.head_stability_label, 1, 1)

        detail_stats_layout.addWidget(QLabel("ä¸“æ³¨æ—¶é•¿:"), 1, 2)
        self.focus_duration_label = QLabel("0ç§’")
        detail_stats_layout.addWidget(self.focus_duration_label, 1, 3)

        # ç¬¬ä¸‰è¡Œ
        detail_stats_layout.addWidget(QLabel("åˆ†å¿ƒæ¬¡æ•°:"), 2, 0)
        self.distraction_count_label = QLabel("0")
        detail_stats_layout.addWidget(self.distraction_count_label, 2, 1)

        detail_stats_layout.addWidget(QLabel("é‡æ–°ä¸“æ³¨ç‡:"), 2, 2)
        self.refocus_rate_label = QLabel("0%")
        detail_stats_layout.addWidget(self.refocus_rate_label, 2, 3)

        # ç¬¬å››è¡Œï¼šç§¯æ/æ¶ˆææ¯”ä¾‹
        detail_stats_layout.addWidget(QLabel("ç§¯ææ¯”ä¾‹:"), 3, 0)
        self.positive_ratio_label = QLabel("0%")
        detail_stats_layout.addWidget(self.positive_ratio_label, 3, 1)

        detail_stats_layout.addWidget(QLabel("æ¶ˆææ¯”ä¾‹:"), 3, 2)
        self.negative_ratio_label = QLabel("0%")
        detail_stats_layout.addWidget(self.negative_ratio_label, 3, 3)

        detail_stats_group.setLayout(detail_stats_layout)
        layout.addWidget(detail_stats_group)

        # æƒ…ç»ªç»Ÿè®¡ç»„
        emotion_stats_group = QGroupBox("ğŸ˜Š æƒ…ç»ªç»Ÿè®¡")
        emotion_stats_layout = QGridLayout()

        # ç¬¬ä¸€è¡Œ
        emotion_stats_layout.addWidget(QLabel("é¢éƒ¨æ•°é‡:"), 0, 0)
        self.face_count_label = QLabel("0")
        emotion_stats_layout.addWidget(self.face_count_label, 0, 1)

        emotion_stats_layout.addWidget(QLabel("æƒ…ç»ªå˜åŒ–é¢‘ç‡:"), 0, 2)
        self.emotion_change_freq_label = QLabel("0æ¬¡/åˆ†é’Ÿ")
        emotion_stats_layout.addWidget(self.emotion_change_freq_label, 0, 3)

        # ç¬¬äºŒè¡Œ
        emotion_stats_layout.addWidget(QLabel("ä¸­æ€§æ—¶é•¿:"), 1, 0)
        self.neutral_duration_label = QLabel("0%")
        emotion_stats_layout.addWidget(self.neutral_duration_label, 1, 1)

        emotion_stats_layout.addWidget(QLabel("æç«¯æƒ…ç»ª:"), 1, 2)
        self.extreme_emotion_label = QLabel("æ— ")
        self.extreme_emotion_label.setStyleSheet("font-weight: bold; color: #27ae60;")
        emotion_stats_layout.addWidget(self.extreme_emotion_label, 1, 3)

        # ç¬¬ä¸‰è¡Œï¼šæƒ…ç»ªä¸€è‡´æ€§
        emotion_stats_layout.addWidget(QLabel("æƒ…ç»ªä¸€è‡´æ€§:"), 2, 0)
        self.emotion_consistency_label = QLabel("é«˜")
        emotion_stats_layout.addWidget(self.emotion_consistency_label, 2, 1)

        # æ·»åŠ å¤šåŠ¨ç—‡ç‰¹å¾åˆ†æç»„
        adhd_group = QGroupBox("ğŸ” å¤šåŠ¨ç—‡ç‰¹å¾åˆ†æ")
        adhd_layout = QGridLayout()

        # ç¬¬ä¸€è¡Œ
        adhd_layout.addWidget(QLabel("æ³¨æ„åŠ›ä¸é›†ä¸­æ¯”ä¾‹:"), 0, 0)
        self.inattention_ratio_label = QLabel("0%")
        adhd_layout.addWidget(self.inattention_ratio_label, 0, 1)

        adhd_layout.addWidget(QLabel("æ´»åŠ¨è¿‡åº¦æŒ‡æ•°:"), 0, 2)
        self.hyperactivity_label = QLabel("0")
        adhd_layout.addWidget(self.hyperactivity_label, 0, 3)

        # ç¬¬äºŒè¡Œ
        adhd_layout.addWidget(QLabel("æƒ…ç»ªæ³¢åŠ¨æŒ‡æ•°:"), 1, 0)
        self.emotion_volatility_label = QLabel("0")
        adhd_layout.addWidget(self.emotion_volatility_label, 1, 1)

        adhd_layout.addWidget(QLabel("æ€»ä½“é£é™©ç­‰çº§:"), 1, 2)
        self.risk_level_label = QLabel("æ­£å¸¸")
        self.risk_level_label.setStyleSheet("font-weight: bold; color: #27ae60;")
        adhd_layout.addWidget(self.risk_level_label, 1, 3)

        # ç¬¬ä¸‰è¡Œï¼šä¸“æ³¨æ¨¡å¼
        adhd_layout.addWidget(QLabel("ä¸“æ³¨æ¨¡å¼:"), 2, 0)
        self.focus_pattern_label = QLabel("åˆ†æä¸­")
        adhd_layout.addWidget(self.focus_pattern_label, 2, 1)

        adhd_layout.addWidget(QLabel("ADHDç‰¹å¾:"), 2, 2)
        self.adhd_features_label = QLabel("æ— ")
        adhd_layout.addWidget(self.adhd_features_label, 2, 3)

        adhd_group.setLayout(adhd_layout)
        layout.addWidget(adhd_group)

        layout.addStretch()
        self.stats_tab.setLayout(layout)

    def setup_alert_tab(self):
        """è®¾ç½®è­¦æŠ¥é€‰é¡¹å¡"""
        layout = QVBoxLayout()

        # è­¦æŠ¥å’Œæ—¥å¿—ç»„
        alert_group = QGroupBox("âš ï¸ è­¦æŠ¥ä¸æ—¥å¿—")
        alert_layout = QVBoxLayout()

        self.alert_text = QTextEdit()
        self.alert_text.setReadOnly(True)
        self.alert_text.setStyleSheet("font-size: 11px; background-color: #f8f9fa;")

        alert_layout.addWidget(self.alert_text)
        alert_group.setLayout(alert_layout)
        layout.addWidget(alert_group)

        self.alert_tab.setLayout(layout)

    def start_calibration(self):
        """å¼€å§‹æ ¡å‡†"""
        if not self.is_playing:
            QMessageBox.warning(self, "è­¦å‘Š", "è¯·å…ˆå¯åŠ¨æ‘„åƒå¤´æˆ–åŠ è½½è§†é¢‘")
            return

        self.is_calibrating = True
        self.calibration_step = 0
        self.calibration_system.start_calibration()

        # æ›´æ–°UI
        self.calibrate_btn.setText("ğŸ”„ æ ¡å‡†ä¸­...")
        self.calibrate_btn.setStyleSheet("background-color: #f39c12;")
        self.calibration_status_label.setText("æ ¡å‡†ä¸­...")
        self.calibration_instruction.setText("è¯·æ³¨è§†å±å¹•ä¸­å¤®çš„çº¢ç‚¹")
        self.calibration_progress.setValue(0)
        self.calibration_info.setText("æ­¥éª¤ 1/5: æ³¨è§†ä¸­å¿ƒç‚¹")

        self.add_alert("å¼€å§‹æ ¡å‡†ï¼Œè¯·æŒ‰ç…§æç¤ºæ³¨è§†å±å¹•ä¸Šçš„ç‚¹", "info")

    def reset_calibration(self):
        """é‡ç½®æ ¡å‡†"""
        self.calibration_system.reset_calibration()
        self.facial_modeling.reset_calibration()
        self.attention_scoring.reset()

        self.is_calibrating = False
        self.calibrate_btn.setText("ğŸ¯ å¼€å§‹æ ¡å‡†")
        self.calibrate_btn.setStyleSheet("background-color: #9b59b6;")
        self.calibration_status_label.setText("æœªæ ¡å‡†")
        self.calibration_instruction.setText("ç‚¹å‡»'å¼€å§‹æ ¡å‡†'æŒ‰é’®è¿›è¡Œæ ¡å‡†")
        self.calibration_progress.setValue(0)
        self.calibration_info.setText("")

        self.add_alert("æ ¡å‡†å·²é‡ç½®", "info")

    def auto_facial_modeling(self):
        """è‡ªåŠ¨é¢éƒ¨å»ºæ¨¡"""
        if not self.is_playing:
            QMessageBox.warning(self, "è­¦å‘Š", "è¯·å…ˆå¯åŠ¨æ‘„åƒå¤´æˆ–åŠ è½½è§†é¢‘")
            return

        self.add_alert("å¼€å§‹è‡ªåŠ¨é¢éƒ¨å»ºæ¨¡ï¼Œè¯·ä¿æŒæ­£é¢æ³¨è§†æ‘„åƒå¤´", "info")

        # å¯ç”¨ç¦ç”¨æŒ‰é’®
        self.calibration_auto_btn.setEnabled(False)
        self.calibration_auto_btn.setText("å»ºæ¨¡ä¸­...")

        # åœ¨åå°çº¿ç¨‹ä¸­è¿›è¡Œé¢éƒ¨å»ºæ¨¡
        modeling_thread = threading.Thread(target=self._perform_facial_modeling)
        modeling_thread.daemon = True
        modeling_thread.start()

    def _perform_facial_modeling(self):
        """æ‰§è¡Œé¢éƒ¨å»ºæ¨¡ï¼ˆåœ¨çº¿ç¨‹ä¸­è¿è¡Œï¼‰"""
        frames_collected = 0
        modeling_success = False

        try:
            for i in range(30):  # æ”¶é›†30å¸§
                if not self.is_playing:
                    break

                # æ¨¡æ‹Ÿè·å–å¸§å¹¶å»ºæ¨¡
                time.sleep(0.1)
                frames_collected += 1

                # ä½¿ç”¨ä¿¡å·æ›´æ–°è¿›åº¦ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
                self.modeling_progress_updated.emit(frames_collected)

            if frames_collected >= 20:  # è‡³å°‘æ”¶é›†20å¸§
                modeling_success = True

        except Exception as e:
            print(f"é¢éƒ¨å»ºæ¨¡é”™è¯¯: {e}")

        # ä½¿ç”¨ä¿¡å·é€šçŸ¥å®Œæˆ
        self.modeling_finished.emit(modeling_success)

    def update_modeling_progress(self, frames_collected):
        """æ›´æ–°å»ºæ¨¡è¿›åº¦ï¼ˆåœ¨ä¸»çº¿ç¨‹ä¸­æ‰§è¡Œï¼‰"""
        progress = frames_collected * 100 // 30
        self.calibration_progress.setValue(progress)
        self.calibration_info.setText(f"æ­£åœ¨é‡‡é›†é¢éƒ¨æ•°æ®: {frames_collected}/30 å¸§")

    def finish_modeling(self, success):
        """å®Œæˆå»ºæ¨¡ï¼ˆåœ¨ä¸»çº¿ç¨‹ä¸­æ‰§è¡Œï¼‰"""
        if success:
            self.add_alert("è‡ªåŠ¨é¢éƒ¨å»ºæ¨¡æˆåŠŸ", "info")
            self.calibration_status_label.setText("é¢éƒ¨å»ºæ¨¡å®Œæˆ")
            self.calibration_instruction.setText("é¢éƒ¨å»ºæ¨¡å®Œæˆï¼Œå¯ä»¥è¿›è¡Œæ ¡å‡†")
        else:
            self.add_alert("è‡ªåŠ¨é¢éƒ¨å»ºæ¨¡å¤±è´¥", "warning")
            self.calibration_status_label.setText("é¢éƒ¨å»ºæ¨¡å¤±è´¥")
            self.calibration_instruction.setText("é¢éƒ¨å»ºæ¨¡å¤±è´¥ï¼Œè¯·é‡è¯•")

        # æ¢å¤æŒ‰é’®çŠ¶æ€
        self.calibration_auto_btn.setEnabled(True)
        self.calibration_auto_btn.setText("è‡ªåŠ¨é¢éƒ¨å»ºæ¨¡")

    def start_camera(self):
        """å¯åŠ¨æ‘„åƒå¤´"""
        try:
            if self.camera is not None:
                self.stop_video()

            self.camera = cv2.VideoCapture(0, cv2.CAP_DSHOW)

            if not self.camera.isOpened():
                self.camera = cv2.VideoCapture(0)

            if not self.camera.isOpened():
                QMessageBox.critical(self, "é”™è¯¯", "æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
                return

            # è®¾ç½®æ‘„åƒå¤´å‚æ•°
            self.camera.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
            self.camera.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
            self.camera.set(cv2.CAP_PROP_FPS, 15)

            self.is_live = True
            self.is_playing = True
            self.video_path = None
            self.video_capture = self.camera

            self.record_btn.setEnabled(True)
            self.pause_btn.setEnabled(True)
            self.calibrate_btn.setEnabled(True)  # å¯ç”¨æ ¡å‡†æŒ‰é’®
            self.calibration_reset_btn.setEnabled(True)
            self.calibration_auto_btn.setEnabled(True)

            self.camera_btn.setText("ğŸ“· åœæ­¢æ‘„åƒå¤´")
            self.camera_btn.setStyleSheet("background-color: #e74c3c;")

            self.reset_analysis()
            self.session_start_time = datetime.now()
            self.frame_count = 0

            self.timer.start(100)  # 10 FPS

            self.add_alert("æ‘„åƒå¤´å¯åŠ¨æˆåŠŸ", "info")

        except Exception as e:
            QMessageBox.critical(self, "é”™è¯¯", f"å¯åŠ¨æ‘„åƒå¤´å¤±è´¥: {str(e)}")

    def upload_video(self):
        """ä¸Šä¼ è§†é¢‘æ–‡ä»¶"""
        try:
            file_path, _ = QFileDialog.getOpenFileName(
                self, "é€‰æ‹©è§†é¢‘æ–‡ä»¶",
                "", "è§†é¢‘æ–‡ä»¶ (*.mp4 *.avi *.mov *.mkv)"
            )

            if not file_path:
                return

            self.stop_video()

            self.video_capture = cv2.VideoCapture(file_path)
            if not self.video_capture.isOpened():
                QMessageBox.critical(self, "é”™è¯¯", "æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶")
                return

            self.video_path = file_path
            self.is_live = False
            self.is_playing = True

            self.record_btn.setEnabled(True)
            self.pause_btn.setEnabled(True)
            self.calibrate_btn.setEnabled(True)  # å¯ç”¨æ ¡å‡†æŒ‰é’®
            self.calibration_reset_btn.setEnabled(True)
            self.calibration_auto_btn.setEnabled(True)

            self.video_btn.setText("ğŸ“ åœæ­¢è§†é¢‘")
            self.video_btn.setStyleSheet("background-color: #e74c3c;")

            self.reset_analysis()
            self.session_start_time = datetime.now()
            self.frame_count = 0

            self.timer.start(33)  # ~30 FPS for videos

            filename = os.path.basename(file_path)
            self.add_alert(f"è§†é¢‘åŠ è½½æˆåŠŸ: {filename}", "info")

        except Exception as e:
            QMessageBox.critical(self, "é”™è¯¯", f"åŠ è½½è§†é¢‘å¤±è´¥: {str(e)}")

    def stop_video(self):
        """åœæ­¢è§†é¢‘"""
        self.timer.stop()

        if self.camera:
            self.camera.release()
            self.camera = None

        if self.video_capture and not self.is_live:
            self.video_capture.release()
            self.video_capture = None

        if self.video_writer:
            self.video_writer.release()
            self.video_writer = None

        self.is_playing = False
        self.is_recording = False

        self.record_btn.setText("â— Start Recording")
        self.record_btn.setStyleSheet("background-color: #e74c3c;")
        self.record_btn.setEnabled(False)

        self.pause_btn.setText("â¸ï¸ Pause")
        self.pause_btn.setEnabled(False)

        self.camera_btn.setText("ğŸ“· Start Camera")
        self.camera_btn.setStyleSheet("background-color: #27ae60;")

        self.video_btn.setText("ğŸ“ Upload Video")
        self.video_btn.setStyleSheet("background-color: #e67e22;")

        # æ˜¾ç¤ºé»‘è‰²ç”»é¢ï¼Œä¸æ˜¾ç¤ºæ–‡å­—
        black_pixmap = QPixmap(900, 500)
        black_pixmap.fill(Qt.black)
        self.video_label.setPixmap(black_pixmap)

    def toggle_pause(self):
        """åˆ‡æ¢æš‚åœçŠ¶æ€"""
        if not self.is_playing:
            self.is_playing = True
            self.pause_btn.setText("â¸ï¸ æš‚åœ")
            self.timer.start(100 if self.is_live else 33)
            self.add_alert("è§†é¢‘æ¢å¤æ’­æ”¾", "info")
        else:
            self.is_playing = False
            self.pause_btn.setText("â–¶ï¸ æ¢å¤")
            self.timer.stop()
            self.add_alert("è§†é¢‘æš‚åœ", "info")

    def toggle_recording(self):
        """åˆ‡æ¢å½•åˆ¶çŠ¶æ€"""
        if not self.is_recording:
            # å¼€å§‹å½•åˆ¶
            self.is_recording = True
            self.record_btn.setText("â¹ï¸ åœæ­¢å½•åˆ¶")
            self.record_btn.setStyleSheet("background-color: #2c3e50;")

            # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"å¤šåŠ¨ç—‡åˆ†æ_{timestamp}.avi"

            # è·å–å¸§å°ºå¯¸
            ret, frame = self.video_capture.read()
            if ret:
                h, w = frame.shape[:2]
                fourcc = cv2.VideoWriter_fourcc(*'XVID')
                self.video_writer = cv2.VideoWriter(filename, fourcc, 10.0, (w, h))
                # å°†å¸§å†™å›å»
                self.video_capture.set(cv2.CAP_PROP_POS_FRAMES,
                                       self.video_capture.get(cv2.CAP_PROP_POS_FRAMES) - 1)

            self.add_alert("å¼€å§‹å½•åˆ¶è§†é¢‘", "info")

        else:
            # åœæ­¢å½•åˆ¶
            self.is_recording = False
            self.record_btn.setText("â— å¼€å§‹å½•åˆ¶")
            self.record_btn.setStyleSheet("background-color: #e74c3c;")

            if self.video_writer:
                self.video_writer.release()
                self.video_writer = None

            self.add_alert("å½•åˆ¶å·²åœæ­¢", "info")

    def update_frame(self):
        """æ›´æ–°è§†é¢‘å¸§"""
        if not self.is_playing or self.video_capture is None:
            return

        try:
            ret, frame = self.video_capture.read()

            if not ret:
                if not self.is_live:
                    self.add_alert("è§†é¢‘æ’­æ”¾ç»“æŸ", "info")
                    self.stop_video()
                return

            self.frame_count += 1

            # è°ƒæ•´å¸§å°ºå¯¸
            frame = cv2.resize(frame, (640, 480))
            display_frame = frame.copy()

            # åˆ†ææ³¨æ„åŠ›å’Œæƒ…ç»ª
            attention_state = self.attention_analyzer.analyze_frame(frame)
            emotion_state = self.emotion_analyzer.analyze_frame(frame)

            # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
            attention_score = self.attention_scoring.calculate_attention_score(
                attention_state,
                emotion_state
            )
            attention_state["attention_score"] = attention_score
            attention_state["optimized_score"] = attention_score  # é¢å¤–ä¿å­˜ï¼Œç”¨äºåŒºåˆ†
            score_analysis = self.attention_scoring.get_score_analysis()
            # ä¿å­˜å½“å‰çŠ¶æ€ä»¥ä¾¿åœ¨update_statusä¸­ä½¿ç”¨
            self.attention_state = attention_state
            self.score_analysis = score_analysis  # ä¿å­˜åˆ†æç»“æœ

            # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
            if hasattr(self, 'attention_scoring'):
                attention_score = self.attention_scoring.calculate_attention_score(attention_state, emotion_state)
                attention_state["attention_score"] = attention_score
            else:
                attention_score = self.attention_analyzer.calculate_attention_score()
                attention_state["attention_score"] = attention_score

            # æ£€æŸ¥æ ¡å‡†çŠ¶æ€
            if self.is_calibrating:
                # å¤„ç†æ ¡å‡†å¸§
                gaze_data = {
                    "gaze_x": attention_state.get("gaze_x", 0),
                    "gaze_y": attention_state.get("gaze_y", 0)
                }

                cal_result = self.calibration_system.process_calibration_frame(frame, gaze_data)

                if cal_result:
                    if cal_result.get("status") == "å®Œæˆ":
                        self.is_calibrating = False
                        self.calibrate_btn.setText("ğŸ¯ æ ¡å‡†å®Œæˆ")
                        self.calibrate_btn.setStyleSheet("background-color: #27ae60;")
                        self.calibration_status_label.setText("å·²æ ¡å‡†")
                        self.calibration_instruction.setText("æ ¡å‡†å®Œæˆï¼")
                        self.calibration_progress.setValue(100)

                        # ä¿å­˜æ ¡å‡†ç»“æœ
                        results = cal_result.get("results", {})
                        tolerance = results.get("tolerance", 0.2)
                        self.calibration_info.setText(f"æ ¡å‡†å®Œæˆï¼è§†çº¿å®¹å·®: {tolerance:.3f}")

                        self.add_alert("æ ¡å‡†å®Œæˆ", "info")

                    else:
                        # æ›´æ–°æ ¡å‡†è¿›åº¦
                        current_step = cal_result.get("current_step", "center")
                        progress = cal_result.get("progress", 0) * 100

                        # æ›´æ–°UI
                        self.calibration_progress.setValue(int(progress))
                        self.calibration_info.setText(f"æ­¥éª¤ {self.calibration_step + 1}/5: {current_step}")

                        # æ›´æ–°æ ¡å‡†æ­¥éª¤æ˜¾ç¤º
                        if "ç»§ç»­" in cal_result.get("status", ""):
                            self.calibration_step += 1

            # åœ¨å¸§ä¸Šç»˜åˆ¶ç»“æœ
            if self.show_attention_overlay or self.show_emotion_overlay:
                display_frame = self.draw_analysis_overlay(display_frame, attention_state, emotion_state)

            # å¦‚æœæ­£åœ¨æ ¡å‡†ï¼Œç»˜åˆ¶æ ¡å‡†ç‚¹
            if self.is_calibrating:
                display_frame = self.draw_calibration_point(display_frame)

            # æ›´æ–°å›¾è¡¨æ•°æ®
            self.realtime_charts.update_data(attention_state, emotion_state)

            # è½¬æ¢ä¸ºQtå›¾åƒå¹¶æ˜¾ç¤º
            rgb_image = cv2.cvtColor(display_frame, cv2.COLOR_BGR2RGB)
            h, w, ch = rgb_image.shape
            bytes_per_line = ch * w
            qt_image = QImage(rgb_image.data, w, h, bytes_per_line, QImage.Format_RGB888)
            pixmap = QPixmap.fromImage(qt_image)

            scaled_pixmap = pixmap.scaled(
                self.video_label.size(),
                Qt.KeepAspectRatio,
                Qt.SmoothTransformation
            )
            self.video_label.setPixmap(scaled_pixmap)

            # æ›´æ–°UIæ˜¾ç¤º
            self.update_attention_display(attention_state)
            self.update_emotion_display(emotion_state)

            # æ›´æ–°è¯¦ç»†ç»Ÿè®¡
            self.update_detailed_stats(attention_state, emotion_state)

            # æ£€æŸ¥è­¦æŠ¥æ¡ä»¶
            self.check_alerts(attention_state, emotion_state)

            # æ£€æŸ¥è§†çº¿æ˜¯å¦åœ¨å®¹å·®èŒƒå›´å†…
            if self.calibration_system.reference_gaze_center != (0, 0):
                gaze_in_tolerance = self.calibration_system.check_gaze_within_tolerance(
                    attention_state.get("gaze_x", 0),
                    attention_state.get("gaze_y", 0)
                )

                if not gaze_in_tolerance and self.frame_count % 20 == 0:
                    self.add_alert("è§†çº¿åç¦»æ­£å¸¸èŒƒå›´", "warning")

            # è®°å½•æ•°æ®
            if self.is_recording and self.video_writer:
                try:
                    self.video_writer.write(display_frame)

                    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
                    data_point = {
                        "timestamp": timestamp,
                        "frame": self.frame_count,
                        "attention": attention_state,
                        "emotion": emotion_state
                    }
                    self.record_data.append(data_point)
                except Exception as e:
                    print(f"å½•åˆ¶é”™è¯¯: {e}")

            # è¯­éŸ³æé†’
            if self.voice_enabled and self.voice_system and self.frame_count % 30 == 0:
                self.check_voice_reminders(attention_state, emotion_state)

        except Exception as e:
            print(f"å¸§æ›´æ–°é”™è¯¯: {e}")
            traceback.print_exc()

    def draw_calibration_point(self, frame):
        """ç»˜åˆ¶æ ¡å‡†ç‚¹"""
        try:
            h, w = frame.shape[:2]

            # æ ¹æ®å½“å‰æ ¡å‡†æ­¥éª¤ç¡®å®šç‚¹ä½ç½®
            cal_status = self.calibration_system.get_calibration_status()
            current_step = cal_status.get("current_step", "center")

            if current_step == "center":
                point_x, point_y = w // 2, h // 2
            elif current_step == "top_left":
                point_x, point_y = w // 4, h // 4
            elif current_step == "top_right":
                point_x, point_y = 3 * w // 4, h // 4
            elif current_step == "bottom_left":
                point_x, point_y = w // 4, 3 * h // 4
            elif current_step == "bottom_right":
                point_x, point_y = 3 * w // 4, 3 * h // 4
            else:
                point_x, point_y = w // 2, h // 2

            # ç»˜åˆ¶å¤–åœ†
            cv2.circle(frame, (point_x, point_y), 15, (0, 0, 255), 3)

            # ç»˜åˆ¶å†…åœ†
            cv2.circle(frame, (point_x, point_y), 5, (0, 255, 255), -1)

            # æ˜¾ç¤ºæ ¡å‡†æ­¥éª¤
            step_text = f"æ ¡å‡†æ­¥éª¤: {current_step}"
            cv2.putText(frame, step_text, (10, h - 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)

            return frame

        except Exception as e:
            print(f"ç»˜åˆ¶æ ¡å‡†ç‚¹é”™è¯¯: {e}")
            return frame

    def update_detailed_stats(self, attention_state, emotion_state):
        """æ›´æ–°è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""
        try:
            # è·å–åˆ†æ•°åˆ†æ
            score_analysis = self.attention_scoring.get_score_analysis() if hasattr(self, 'attention_scoring') else {}

            # æ›´æ–°æ³¨æ„åŠ›è¯¦ç»†ç»Ÿè®¡
            if hasattr(self, 'attention_stability_label'):
                self.attention_stability_label.setText(f"{score_analysis.get('stability_level', 'æœªçŸ¥')}")

            if hasattr(self, 'gaze_deviation_label'):
                gaze_x = attention_state.get("gaze_x", 0)
                gaze_y = attention_state.get("gaze_y", 0)
                gaze_magnitude = math.sqrt(gaze_x ** 2 + gaze_y ** 2)
                self.gaze_deviation_label.setText(f"{gaze_magnitude:.3f}")

            if hasattr(self, 'head_stability_label'):
                self.head_stability_label.setText(f"{score_analysis.get('stability_level', 'æœªçŸ¥')}")

            if hasattr(self, 'focus_duration_label'):
                focus_duration = self.calculate_focus_duration()
                self.focus_duration_label.setText(f"{focus_duration:.1f}ç§’")

            if hasattr(self, 'distraction_count_label'):
                # è®¡ç®—åˆ†å¿ƒæ¬¡æ•°ï¼ˆæ³¨æ„åŠ›åˆ†æ•° < 50 çš„å¸§æ•°ï¼‰
                if hasattr(self, 'attention_scoring'):
                    scores = list(self.attention_scoring.score_history)
                    distracted_frames = sum(1 for score in scores if score < 50)
                    self.distraction_count_label.setText(f"{distracted_frames}")
                else:
                    self.distraction_count_label.setText("0")

            if hasattr(self, 'refocus_rate_label'):
                # è®¡ç®—é‡æ–°ä¸“æ³¨ç‡ï¼ˆç®€åŒ–è®¡ç®—ï¼‰
                self.refocus_rate_label.setText("0%")

            # æ›´æ–°æƒ…ç»ªè¯¦ç»†ç»Ÿè®¡
            if hasattr(self, 'emotion_change_freq_label'):
                emotion_stats = self.emotion_analyzer.get_emotion_stats()
                self.emotion_change_freq_label.setText(f"{emotion_stats.get('emotion_changes', 0)}æ¬¡/åˆ†é’Ÿ")

            if hasattr(self, 'neutral_duration_label'):
                neutral_ratio = self.calculate_neutral_duration()
                self.neutral_duration_label.setText(f"{neutral_ratio:.1f}%")

            if hasattr(self, 'extreme_emotion_label'):
                extreme_emotions = self.check_extreme_emotions()
                if extreme_emotions:
                    self.extreme_emotion_label.setText(f"{', '.join(extreme_emotions)}")
                    self.extreme_emotion_label.setStyleSheet("font-weight: bold; color: #e74c3c;")
                else:
                    self.extreme_emotion_label.setText("æ— ")
                    self.extreme_emotion_label.setStyleSheet("font-weight: bold; color: #27ae60;")

        except Exception as e:
            print(f"æ›´æ–°è¯¦ç»†ç»Ÿè®¡é”™è¯¯: {e}")

    def update_charts_widgets(self):
        """æ›´æ–°å›¾è¡¨å°éƒ¨ä»¶"""
        try:
            # åˆ›å»ºQPixmapæ¥ç»˜åˆ¶å›¾è¡¨
            attention_pixmap = QPixmap(320, 160)
            attention_pixmap.fill(Qt.white)

            gaze_pixmap = QPixmap(320, 160)
            gaze_pixmap.fill(Qt.white)

            eye_pixmap = QPixmap(320, 160)
            eye_pixmap.fill(Qt.white)

            # åˆ›å»ºQPainteræ¥ç»˜åˆ¶
            attention_painter = QPainter(attention_pixmap)
            gaze_painter = QPainter(gaze_pixmap)
            eye_painter = QPainter(eye_pixmap)

            # è®¾ç½®æŠ—é”¯é½¿
            attention_painter.setRenderHint(QPainter.Antialiasing)
            gaze_painter.setRenderHint(QPainter.Antialiasing)
            eye_painter.setRenderHint(QPainter.Antialiasing)

            # ç»˜åˆ¶å›¾è¡¨
            self.realtime_charts.draw_attention_chart(attention_painter, 10, 10, 310, 150)
            self.realtime_charts.draw_gaze_chart(gaze_painter, 10, 10, 310, 150)
            self.realtime_charts.draw_eye_chart(eye_painter, 10, 10, 310, 150)

            # ç»“æŸç»˜åˆ¶
            attention_painter.end()
            gaze_painter.end()
            eye_painter.end()

            # è®¾ç½®åˆ°æ ‡ç­¾
            self.attention_chart_widget.setPixmap(attention_pixmap)
            self.gaze_chart_widget.setPixmap(gaze_pixmap)
            self.eye_chart_widget.setPixmap(eye_pixmap)

            # è·å–ç»Ÿè®¡ä¿¡æ¯å¹¶æ›´æ–°æ ‡é¢˜
            stats = self.realtime_charts.get_statistics()
            if stats:
                current_score = stats["attention"]["current"]
                self.attention_chart_title.setText(f"æ³¨æ„åŠ›åˆ†æ•°: {current_score:.1f}")

                gaze_x = stats["gaze"]["x_mean"]
                gaze_y = stats["gaze"]["y_mean"]
                self.gaze_chart_title.setText(f"è§†çº¿è¿½è¸ª (X:{gaze_x:.2f}, Y:{gaze_y:.2f})")

                ear_mean = stats["eye"]["ear_mean"]
                self.eye_chart_title.setText(f"çœ¼éƒ¨ç‰¹å¾ (EAR:{ear_mean:.2f})")

        except Exception as e:
            print(f"æ›´æ–°å›¾è¡¨é”™è¯¯: {e}")

    def draw_analysis_overlay(self, frame, attention_state, emotion_state):
        """åœ¨å¸§ä¸Šç»˜åˆ¶åˆ†æç»“æœï¼ˆè‹±æ–‡æ˜¾ç¤ºï¼‰"""
        try:
            h, w = frame.shape[:2]

            # ç»˜åˆ¶æ³¨æ„åŠ›ä¿¡æ¯
            if self.show_attention_overlay:
                # æ³¨æ„åŠ›æ ‡ç­¾å’Œåˆ†æ•°
                label = attention_state.get("attention_label", "æœªçŸ¥")
                score = attention_state.get("attention_score", 0)

                # æ ¹æ®æ ‡ç­¾è®¾ç½®é¢œè‰²å’Œè‹±æ–‡æ–‡æœ¬
                if label == "ä¸“æ³¨":
                    color = (0, 255, 0)  # ç»¿è‰²
                    label_en = "Focused"
                elif label == "çœ¼ç›é—­åˆ":
                    color = (0, 0, 255)  # çº¢è‰²
                    label_en = "Eyes Closed"
                elif label == "è§†çº¿åç¦»":
                    color = (0, 165, 255)  # æ©™è‰²
                    label_en = "Head Turned"
                elif label == "è§†çº¿åç§»":
                    color = (0, 255, 255)  # é»„è‰²
                    label_en = "Gaze Offset"
                elif label == "æœªæ£€æµ‹åˆ°é¢éƒ¨":
                    color = (128, 128, 128)  # ç°è‰²
                    label_en = "No Face Detected"
                else:
                    color = (128, 128, 128)  # ç°è‰²
                    label_en = label

                # åœ¨å·¦ä¸Šè§’ç»˜åˆ¶ä¿¡æ¯
                cv2.putText(frame, f"Attention: {label_en}", (10, 30),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
                cv2.putText(frame, f"Score: {score:.0f}", (10, 60),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)

                # ç»˜åˆ¶è¯¦ç»†æŒ‡æ ‡ - ä½¿ç”¨è‹±æ–‡ä¸”ç¡®ä¿å­—ç¬¦ç¼–ç æ­£ç¡®
                cv2.putText(frame,
                            f"EAR: {attention_state.get('ear_left', 0):.2f}/{attention_state.get('ear_right', 0):.2f}",
                            (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

                # ä½¿ç”¨è‹±æ–‡å­—ç¬¦ç¡®ä¿æ²¡æœ‰ä¹±ç 
                yaw_value = attention_state.get('yaw', 0)
                pitch_value = attention_state.get('pitch', 0)

                cv2.putText(frame, f"Yaw: {yaw_value:+.1f} deg",
                            (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                cv2.putText(frame, f"Pitch: {pitch_value:+.1f} deg",
                            (10, 130), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                cv2.putText(frame, f"Blinks: {attention_state.get('blink_count', 0)}",
                            (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

            # ç»˜åˆ¶æƒ…ç»ªä¿¡æ¯ï¼ˆè‹±æ–‡æ˜¾ç¤ºï¼‰
            if self.show_emotion_overlay:
                emotion = emotion_state.get("emotion", "æœªçŸ¥")
                confidence = emotion_state.get("confidence", 0)
                face_count = emotion_state.get("face_count", 0)

                # å°†ä¸­æ–‡æƒ…ç»ªæ ‡ç­¾æ˜ å°„ä¸ºè‹±æ–‡
                emotion_map = {
                    "ç”Ÿæ°”": "Angry",
                    "åŒæ¶": "Disgust",
                    "ææƒ§": "Fear",
                    "å¿«ä¹": "Happy",
                    "æ‚²ä¼¤": "Sad",
                    "æƒŠè®¶": "Surprise",
                    "ä¸­æ€§": "Neutral",
                    "æœªæ£€æµ‹åˆ°é¢éƒ¨": "No Face",
                    "é”™è¯¯": "Error"
                }

                emotion_en = emotion_map.get(emotion, emotion)

                # è·å–æƒ…ç»ªé¢œè‰²
                emotion_colors = self.emotion_analyzer.emotion_colors
                color = emotion_colors.get(emotion, (200, 200, 200))

                # åœ¨å³ä¸Šè§’ç»˜åˆ¶æƒ…ç»ªä¿¡æ¯
                cv2.putText(frame, f"Emotion: {emotion_en}", (w - 200, 30),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
                cv2.putText(frame, f"Confidence: {confidence * 100:.0f}%", (w - 200, 60),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
                cv2.putText(frame, f"Faces: {face_count}", (w - 200, 90),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)

                # ç»˜åˆ¶é¢éƒ¨è¾¹ç•Œæ¡†å’Œç‰¹å¾ç‚¹
                if self.show_landmarks_check.isChecked():
                    face_boxes = emotion_state.get("face_boxes", [])
                    face_shapes = emotion_state.get("face_shapes", [])

                    for i, (x, y, w_box, h_box) in enumerate(face_boxes):
                        # ç»˜åˆ¶è¾¹ç•Œæ¡†
                        cv2.rectangle(frame, (x, y), (x + w_box, y + h_box), color, 2)

                        # ç»˜åˆ¶é¢éƒ¨ç¼–å·
                        cv2.putText(frame, f"Face {i + 1}", (x, y - 10),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)

                        # ç»˜åˆ¶ç‰¹å¾ç‚¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                        if i < len(face_shapes):
                            shape = face_shapes[i]
                            for (sx, sy) in shape:
                                cv2.circle(frame, (sx, y), 1, color, -1)

            # ç»˜åˆ¶æ—¶é—´æˆ³
            timestamp = datetime.now().strftime("%H:%M:%S")
            cv2.putText(frame, f"Time: {timestamp}", (w - 150, h - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

            # ç»˜åˆ¶å¸§è®¡æ•°
            cv2.putText(frame, f"Frame: {self.frame_count}", (10, h - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

            # ç»˜åˆ¶æ ¡å‡†çŠ¶æ€ï¼ˆå¦‚æœå·²æ ¡å‡†ï¼‰
            if hasattr(self, 'calibration_system') and self.calibration_system.get_calibration_status()[
                "is_calibrated"]:
                cv2.putText(frame, "Calibrated", (w - 150, h - 40),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)

            return frame

        except Exception as e:
            print(f"ç»˜åˆ¶å åŠ å±‚é”™è¯¯: {e}")
            return frame

    def update_attention_display(self, attention_state):
        """æ›´æ–°æ³¨æ„åŠ›æ˜¾ç¤ºï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        try:
            # æ›´æ–°åˆ†æ•°
            score = attention_state.get("attention_score", 0)
            self.attention_score_label.setText(f"{score:.0f}")

            # ä»æ–°çš„åˆ†æç³»ç»Ÿä¸­è·å–æ³¨æ„åŠ›æ°´å¹³
            if hasattr(self, 'score_analysis'):
                attention_level = self.score_analysis.get("attention_level", "æœªçŸ¥")
                self.attention_state_label.setText(attention_level)
            else:
                # å›é€€åˆ°åŸæ¥çš„é€»è¾‘
                label = attention_state.get("attention_label", "æœªçŸ¥")
                self.attention_state_label.setText(label)

            # è®¾ç½®çŠ¶æ€é¢œè‰²
            if hasattr(self, 'score_analysis'):
                attention_level = self.score_analysis.get("attention_level", "ä¸€èˆ¬")
                color_map = {
                    "éå¸¸ä¸“æ³¨": "#27ae60",  # ç»¿è‰²
                    "ä¸“æ³¨": "#2ecc71",  # æµ…ç»¿
                    "ä¸€èˆ¬": "#f39c12",  # æ©™è‰²
                    "è½»åº¦åˆ†å¿ƒ": "#e67e22",  # æ·±æ©™
                    "ä¸­åº¦åˆ†å¿ƒ": "#e74c3c",  # çº¢è‰²
                    "ä¸¥é‡åˆ†å¿ƒ": "#c0392b"  # æ·±çº¢
                }
                color = color_map.get(attention_level, "#7f8c8d")
                self.attention_state_label.setStyleSheet(f"font-size: 16px; font-weight: bold; color: {color};")
            else:
                # åŸæ¥çš„é¢œè‰²è®¾ç½®é€»è¾‘
                label = attention_state.get("attention_label", "æœªçŸ¥")
                if label == "ä¸“æ³¨":
                    color = "#27ae60"
                elif label == "çœ¼ç›é—­åˆ":
                    color = "#e74c3c"
                elif label == "è§†çº¿åç¦»":
                    color = "#f39c12"
                elif label == "è§†çº¿åç§»":
                    color = "#f1c40f"
                else:
                    color = "#7f8c8d"
                self.attention_state_label.setStyleSheet(f"font-size: 16px; font-weight: bold; color: {color};")

            # æ›´æ–°è¿›åº¦æ¡
            self.attention_progress.setValue(int(score))

            # æ›´æ–°è¯¦ç»†æŒ‡æ ‡
            ear_left = attention_state.get('ear_left', 0)
            ear_right = attention_state.get('ear_right', 0)
            self.ear_label.setText(f"{ear_left:.2f}/{ear_right:.2f}")

            yaw_value = attention_state.get('yaw', 0)
            pitch_value = attention_state.get('pitch', 0)

            self.yaw_label.setText(f"{yaw_value:+.1f}Â°")
            self.pitch_label.setText(f"{pitch_value:+.1f}Â°")
            self.gaze_x_label.setText(f"{attention_state.get('gaze_x', 0):.2f}")
            self.gaze_y_label.setText(f"{attention_state.get('gaze_y', 0):.2f}")
            self.blink_label.setText(f"{attention_state.get('blink_count', 0)}")

            # å¦‚æœå¯ç”¨äº†å¤šåŠ¨ç—‡ç‰¹å¾æ£€æµ‹ï¼Œæ˜¾ç¤ºç›¸å…³ä¿¡æ¯
            if hasattr(self, 'score_analysis') and 'adhd_features' in self.score_analysis:
                adhd_features = self.score_analysis['adhd_features']
                if adhd_features:
                    # å¯ä»¥åœ¨UIä¸­æ·»åŠ æ–°çš„æ ‡ç­¾æ¥æ˜¾ç¤ºè¿™äº›ä¿¡æ¯
                    risk_level = adhd_features.get('risk_level', 'æœªçŸ¥')
                    self.add_alert(f"æ³¨æ„åŠ›é£é™©ç­‰çº§: {risk_level}", "info")

        except Exception as e:
            print(f"æ›´æ–°æ³¨æ„åŠ›æ˜¾ç¤ºé”™è¯¯: {e}")

    def update_emotion_display(self, emotion_state):
        """æ›´æ–°æƒ…ç»ªæ˜¾ç¤º"""
        try:
            # ä¿å­˜å½“å‰æƒ…ç»ªçŠ¶æ€
            self.current_emotion_state = emotion_state

            # æ›´æ–°å½“å‰æƒ…ç»ª
            emotion = emotion_state.get("emotion", "æœªçŸ¥")
            self.emotion_label.setText(emotion)

            # è®¾ç½®æƒ…ç»ªé¢œè‰²
            emotion_colors = {
                "ç”Ÿæ°”": "#e74c3c",
                "åŒæ¶": "#27ae60",
                "ææƒ§": "#9b59b6",
                "å¿«ä¹": "#f1c40f",
                "æ‚²ä¼¤": "#3498db",
                "æƒŠè®¶": "#e67e22",
                "ä¸­æ€§": "#95a5a6",
                "æœªæ£€æµ‹åˆ°é¢éƒ¨": "#7f8c8d",
                "é”™è¯¯": "#e74c3c"
            }

            color = emotion_colors.get(emotion, "#7f8c8d")
            self.emotion_label.setStyleSheet(f"font-size: 20px; font-weight: bold; color: {color};")

            # æ›´æ–°ä¿¡å¿ƒ
            confidence = emotion_state.get("confidence", 0)
            self.confidence_label.setText(f"{confidence * 100:.0f}%")

            # æ›´æ–°æƒ…ç»ªæ¦‚ç‡æ¡
            probabilities = emotion_state.get("probabilities", [0.0] * 7)
            emotion_labels = ["ç”Ÿæ°”", "åŒæ¶", "ææƒ§", "å¿«ä¹", "æ‚²ä¼¤", "æƒŠè®¶", "ä¸­æ€§"]

            for i, emotion_name in enumerate(emotion_labels):
                if emotion_name in self.emotion_bars:
                    prob = probabilities[i] * 100
                    self.emotion_bars[emotion_name].setValue(int(prob))

        except Exception as e:
            print(f"æ›´æ–°æƒ…ç»ªæ˜¾ç¤ºé”™è¯¯: {e}")
            traceback.print_exc()

    def update_status(self):
        """æ›´æ–°çŠ¶æ€ä¿¡æ¯ï¼ˆä¿®å¤ç‰ˆæœ¬ï¼‰"""
        try:
            current_time = datetime.now()

            # 1. è·å–åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
            attention_stats = self.attention_analyzer.get_attention_stats()
            emotion_stats = self.emotion_analyzer.get_emotion_stats()

            # 2. è·å–ä¼˜åŒ–åçš„æ³¨æ„åŠ›åˆ†æç»“æœ
            if hasattr(self, 'attention_scoring'):
                score_analysis = self.attention_scoring.get_score_analysis()
                self.score_analysis = score_analysis  # ä¿å­˜ä¾›å…¶ä»–åœ°æ–¹ä½¿ç”¨
            else:
                score_analysis = {}

            # 3. æ›´æ–°åŸºç¡€æ³¨æ„åŠ›ç»Ÿè®¡
            # å¹³å‡åˆ†æ•°
            if score_analysis and 'statistics' in score_analysis:
                stats = score_analysis['statistics']
                if hasattr(self, 'avg_attention_label'):
                    self.avg_attention_label.setText(f"{stats.get('recent_avg', 0):.1f}")
                if hasattr(self, 'max_attention_label'):
                    self.max_attention_label.setText(f"{stats.get('recent_max', 0):.1f}")
                if hasattr(self, 'min_attention_label'):
                    self.min_attention_label.setText(f"{stats.get('recent_min', 0):.1f}")
            else:
                # å›é€€åˆ°åŸæ¥çš„ç»Ÿè®¡
                if hasattr(self, 'avg_attention_label'):
                    self.avg_attention_label.setText(f"{attention_stats.get('avg_score', 0)}")
                if hasattr(self, 'max_attention_label'):
                    self.max_attention_label.setText(f"{attention_stats.get('max_score', 0)}")
                if hasattr(self, 'min_attention_label'):
                    self.min_attention_label.setText(f"{attention_stats.get('min_score', 0)}")

            # æ³¨æ„åŠ›è¶‹åŠ¿
            if score_analysis and 'statistics' in score_analysis:
                stats = score_analysis['statistics']
                if hasattr(self, 'trend_label'):
                    self.trend_label.setText(f"{stats.get('trend', 'ç¨³å®š')}")
            else:
                if hasattr(self, 'trend_label'):
                    self.trend_label.setText(f"{attention_stats.get('trend', 'ç¨³å®š')}")

            # ä¸“æ³¨ç™¾åˆ†æ¯”
            if hasattr(self, 'focus_percent_label'):
                self.focus_percent_label.setText(f"{attention_stats.get('focus_percentage', 0):.1f}%")

            # çœ¨çœ¼é¢‘ç‡
            if hasattr(self, 'blink_rate_label'):
                self.blink_rate_label.setText(f"{attention_stats.get('blink_rate', 0):.1f}/åˆ†é’Ÿ")

            # 4. æ›´æ–°æƒ…ç»ªç»Ÿè®¡
            if hasattr(self, 'dominant_emotion_label'):
                self.dominant_emotion_label.setText(f"{emotion_stats.get('dominant_emotion', 'æœªçŸ¥')}")
            if hasattr(self, 'emotion_stability_label'):
                self.emotion_stability_label.setText(f"{emotion_stats.get('emotion_stability', 0)}%")
            if hasattr(self, 'positive_ratio_label'):
                self.positive_ratio_label.setText(f"{emotion_stats.get('positive_ratio', 0)}%")
            if hasattr(self, 'negative_ratio_label'):
                self.negative_ratio_label.setText(f"{emotion_stats.get('negative_ratio', 0)}%")

            # 5. æ›´æ–°è¯¦ç»†æ³¨æ„åŠ›ç»Ÿè®¡
            # æ³¨æ„åŠ›ç¨³å®šæ€§
            if score_analysis and 'statistics' in score_analysis:
                stats = score_analysis['statistics']
                stability_index = stats.get('stability_index', 0)
                if hasattr(self, 'attention_stability_label'):
                    self.attention_stability_label.setText(f"{stability_index:.1f}%")

                # ä¸€è‡´æ€§åˆ†æ•°
                consistency_score = stats.get('consistency_score', 0)
                if hasattr(self, 'refocus_rate_label'):
                    self.refocus_rate_label.setText(f"{consistency_score:.1f}%")
            else:
                if hasattr(self, 'attention_stability_label'):
                    self.attention_stability_label.setText(f"{attention_stats.get('trend', 'ç¨³å®š')}")
                if hasattr(self, 'refocus_rate_label'):
                    self.refocus_rate_label.setText("0%")

            # è§†çº¿åç§»åº¦
            if hasattr(self, 'gaze_deviation_label'):
                if hasattr(self, 'attention_state'):
                    gaze_x = self.attention_state.get("gaze_x", 0)
                    gaze_y = self.attention_state.get("gaze_y", 0)
                    gaze_magnitude = math.sqrt(gaze_x ** 2 + gaze_y ** 2)
                    self.gaze_deviation_label.setText(f"{gaze_magnitude:.3f}")
                else:
                    self.gaze_deviation_label.setText("0.000")

            # å¤´éƒ¨ç¨³å®šæ€§ï¼ˆä½¿ç”¨å¤šåŠ¨ç—‡ç‰¹å¾ä¸­çš„æ´»åŠ¨è¿‡åº¦æŒ‡æ•°ï¼‰
            if score_analysis and 'adhd_features' in score_analysis:
                adhd_features = score_analysis['adhd_features']
                hyperactivity_index = adhd_features.get('hyperactivity_index', 0)
                if hasattr(self, 'head_stability_label'):
                    self.head_stability_label.setText(f"{100 - hyperactivity_index:.1f}%")
            else:
                if hasattr(self, 'head_stability_label'):
                    self.head_stability_label.setText("0%")

            # 6. æ›´æ–°ä¸“æ³¨æ—¶é•¿ç»Ÿè®¡
            if score_analysis and 'focus_analysis' in score_analysis:
                focus_analysis = score_analysis['focus_analysis']

                # å¹³å‡ä¸“æ³¨æ—¶é•¿
                avg_duration = focus_analysis.get('avg_duration', 0)
                if hasattr(self, 'focus_duration_label'):
                    self.focus_duration_label.setText(f"{avg_duration:.1f}ç§’")

                # æœ€é•¿ä¸“æ³¨æ—¶é•¿
                longest_duration = focus_analysis.get('longest_duration', 0)
                if longest_duration > 0 and hasattr(self, 'distraction_count_label'):
                    self.distraction_count_label.setText(f"{longest_duration:.1f}ç§’")

                # ä¸“æ³¨ä¸­æ–­æ¬¡æ•°
                interruptions = focus_analysis.get('interruptions', 0)
                if interruptions > 0 and hasattr(self, 'distraction_count_label'):
                    self.distraction_count_label.setText(f"{interruptions}æ¬¡")

                # ä¸“æ³¨æ¨¡å¼
                focus_pattern = focus_analysis.get('pattern', 'åˆ†æä¸­')
                if hasattr(self, 'focus_pattern_label'):
                    self.focus_pattern_label.setText(focus_pattern)
            else:
                # è®¡ç®—åŸºç¡€ä¸“æ³¨æ—¶é•¿
                focus_duration = self.calculate_focus_duration()
                if hasattr(self, 'focus_duration_label'):
                    self.focus_duration_label.setText(f"{focus_duration:.1f}ç§’")
                if hasattr(self, 'distraction_count_label'):
                    self.distraction_count_label.setText("0")
                if hasattr(self, 'refocus_rate_label'):
                    self.refocus_rate_label.setText("0%")

            # 7. æ›´æ–°æƒ…ç»ªè¯¦ç»†ç»Ÿè®¡
            # æƒ…ç»ªå˜åŒ–é¢‘ç‡
            if hasattr(self, 'emotion_change_freq_label'):
                self.emotion_change_freq_label.setText(f"{emotion_stats.get('emotion_changes', 0)}æ¬¡/åˆ†é’Ÿ")

            # ä¸­æ€§æ—¶é•¿æ¯”ä¾‹
            neutral_ratio = self.calculate_neutral_duration()
            if hasattr(self, 'neutral_duration_label'):
                self.neutral_duration_label.setText(f"{neutral_ratio:.1f}%")

            # æç«¯æƒ…ç»ªæ£€æµ‹
            extreme_emotions = self.check_extreme_emotions()
            if hasattr(self, 'extreme_emotion_label'):
                if extreme_emotions:
                    self.extreme_emotion_label.setText(f"{', '.join(extreme_emotions)}")
                    self.extreme_emotion_label.setStyleSheet("font-weight: bold; color: #e74c3c;")
                else:
                    self.extreme_emotion_label.setText("æ— ")
                    self.extreme_emotion_label.setStyleSheet("font-weight: bold; color: #27ae60;")

            # 8. æ›´æ–°é¢éƒ¨æ•°é‡
            face_count = emotion_stats.get('face_count', 0)
            if hasattr(self, 'face_count_label'):
                self.face_count_label.setText(f"{face_count}")

            # 9. æ›´æ–°å¤šåŠ¨ç—‡ç‰¹å¾åˆ†æ
            if score_analysis and 'adhd_features' in score_analysis:
                adhd_features = score_analysis['adhd_features']

                # æ³¨æ„åŠ›ä¸é›†ä¸­æ¯”ä¾‹
                inattention_ratio = adhd_features.get('inattention_ratio', 0)
                if hasattr(self, 'inattention_ratio_label'):
                    self.inattention_ratio_label.setText(f"{inattention_ratio:.1f}%")

                # æ´»åŠ¨è¿‡åº¦æŒ‡æ•°
                hyperactivity_index = adhd_features.get('hyperactivity_index', 0)
                if hasattr(self, 'hyperactivity_label'):
                    self.hyperactivity_label.setText(f"{hyperactivity_index:.1f}")

                # æƒ…ç»ªæ³¢åŠ¨æŒ‡æ•°
                emotion_volatility = adhd_features.get('emotion_volatility', 0)
                if hasattr(self, 'emotion_volatility_label'):
                    self.emotion_volatility_label.setText(f"{emotion_volatility:.1f}")

                # æ€»ä½“é£é™©ç­‰çº§
                risk_level = adhd_features.get('risk_level', 'æ­£å¸¸')
                if hasattr(self, 'risk_level_label'):
                    self.risk_level_label.setText(risk_level)

                    # æ ¹æ®é£é™©ç­‰çº§è®¾ç½®é¢œè‰²
                    if risk_level == "é«˜é£é™©":
                        self.risk_level_label.setStyleSheet("font-weight: bold; color: #e74c3c;")
                        if self.frame_count % 60 == 0:  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
                            self.add_alert("æ£€æµ‹åˆ°é«˜é£é™©æ³¨æ„åŠ›é—®é¢˜", "warning")
                    elif risk_level == "ä¸­é£é™©":
                        self.risk_level_label.setStyleSheet("font-weight: bold; color: #f39c12;")
                        if self.frame_count % 120 == 0:
                            self.add_alert("æ£€æµ‹åˆ°ä¸­åº¦æ³¨æ„åŠ›é£é™©", "info")
                    elif risk_level == "ä½é£é™©":
                        self.risk_level_label.setStyleSheet("font-weight: bold; color: #f1c40f;")
                    else:
                        self.risk_level_label.setStyleSheet("font-weight: bold; color: #27ae60;")

                # ADHDç‰¹å¾æ£€æµ‹
                if hasattr(self, 'adhd_features_label'):
                    features_detected = []
                    if inattention_ratio > 30:
                        features_detected.append("æ³¨æ„åŠ›ä¸é›†ä¸­")
                    if hyperactivity_index > 50:
                        features_detected.append("æ´»åŠ¨è¿‡åº¦")
                    if emotion_volatility > 30:
                        features_detected.append("æƒ…ç»ªæ³¢åŠ¨")

                    if features_detected:
                        self.adhd_features_label.setText(", ".join(features_detected[:2]))
                        self.adhd_features_label.setStyleSheet("font-weight: bold; color: #e74c3c;")
                    else:
                        self.adhd_features_label.setText("æ— ")
                        self.adhd_features_label.setStyleSheet("font-weight: bold; color: #27ae60;")

                # æ£€æµ‹åˆ°çš„æ¨¡å¼
                pattern_detected = adhd_features.get('pattern_detected', False)
                if pattern_detected and self.frame_count % 90 == 0:
                    self.add_alert("æ£€æµ‹åˆ°æ³¨æ„åŠ›åˆ†æ•£æ¨¡å¼", "info")

            # 10. æ›´æ–°æ ¡å‡†çŠ¶æ€
            if hasattr(self, 'calibration_system'):
                cal_status = self.calibration_system.get_calibration_status()

                if cal_status.get('is_calibrated'):
                    if hasattr(self, 'calibration_status_label'):
                        self.calibration_status_label.setText("å·²æ ¡å‡†")

                    if hasattr(self, 'calibration_info'):
                        ref_x, ref_y = cal_status['reference_center']
                        tolerance = cal_status['tolerance']
                        self.calibration_info.setText(
                            f"å‚è€ƒä¸­å¿ƒ: ({ref_x:.3f}, {ref_y:.3f}) | å®¹å·®: {tolerance:.3f}"
                        )

                    if hasattr(self, 'calibrate_btn'):
                        self.calibrate_btn.setText("âœ… å·²æ ¡å‡†")
                        self.calibrate_btn.setStyleSheet("background-color: #27ae60;")

                    # æ›´æ–°æ ¡å‡†ç»“æœä¿¡æ¯
                    if hasattr(self, 'calibration_result_label'):
                        self.calibration_result_label.setText(
                            f"æ ¡å‡†çŠ¶æ€: å·²å®Œæˆ\n"
                            f"å‚è€ƒè§†çº¿ä¸­å¿ƒ: ({ref_x:.3f}, {ref_y:.3f})\n"
                            f"è§†çº¿å®¹å·®: {tolerance:.3f}\n"
                            f"æ ¡å‡†ç‚¹: {len(cal_status.get('calibration_results', {}))}"
                        )
                else:
                    if hasattr(self, 'calibration_status_label'):
                        self.calibration_status_label.setText("æœªæ ¡å‡†")

                    if hasattr(self, 'calibration_result_label'):
                        self.calibration_result_label.setText("æ ¡å‡†çŠ¶æ€: æœªå®Œæˆ\nè¯·ç‚¹å‡»'å¼€å§‹æ ¡å‡†'è¿›è¡Œæ ¡å‡†")

            # 11. æ›´æ–°ä¼šè¯æ—¶é—´ä¿¡æ¯
            if self.session_start_time:
                elapsed = current_time - self.session_start_time
                hours, remainder = divmod(elapsed.seconds, 3600)
                minutes, seconds = divmod(remainder, 60)

                # è®¡ç®—å¸§ç‡
                if elapsed.total_seconds() > 0:
                    fps = self.frame_count / elapsed.total_seconds()
                else:
                    fps = 0

                # æ›´æ–°è§†é¢‘ä¿¡æ¯æ ‡ç­¾
                time_text = f"{hours:02d}:{minutes:02d}:{seconds:02d}"

                if self.is_live:
                    source_text = "å®æ—¶æ‘„åƒå¤´"
                elif self.video_path:
                    filename = os.path.basename(self.video_path)
                    source_text = filename
                else:
                    source_text = "å°±ç»ª"

                # æ·»åŠ æ³¨æ„åŠ›æ°´å¹³ä¿¡æ¯
                attention_level = "æœªçŸ¥"
                if score_analysis and 'attention_level' in score_analysis:
                    attention_level = score_analysis['attention_level']

                if self.is_playing:
                    status_text = f"{source_text} | {time_text} | å¸§: {self.frame_count} | FPS: {fps:.1f} | æ³¨æ„åŠ›: {attention_level}"
                else:
                    status_text = f"å°±ç»ª | ä¸Šæ¬¡ä¼šè¯: {time_text} | æ€»å¸§æ•°: {self.frame_count}"

                self.video_info_label.setText(status_text)

                # è‡ªåŠ¨æ£€æŸ¥é•¿æ—¶é—´è¿è¡Œï¼ˆè¶…è¿‡30åˆ†é’Ÿå»ºè®®ä¼‘æ¯ï¼‰
                if elapsed.total_seconds() > 1800 and self.frame_count % 300 == 0:  # 30åˆ†é’Ÿï¼Œæ¯5åˆ†é’Ÿæé†’
                    self.add_alert(f"æ£€æµ‹å·²è¿è¡Œ{minutes}åˆ†é’Ÿï¼Œå»ºè®®ä¼‘æ¯ä¸€ä¸‹", "info")
                    if self.voice_enabled:
                        self.voice_system.speak("å·²ç»è¿ç»­æ£€æµ‹30åˆ†é’Ÿï¼Œå»ºè®®ä¼‘æ¯ä¸€ä¸‹")

            # 12. ç”Ÿæˆå®æ—¶å»ºè®®
            if score_analysis and 'recommendations' in score_analysis:
                recommendations = score_analysis['recommendations']
                if recommendations and len(recommendations) > 0 and self.frame_count % 180 == 0:  # æ¯3åˆ†é’Ÿ
                    random_recommendation = random.choice(recommendations)
                    self.add_alert(f"å»ºè®®: {random_recommendation}", "info")

            # 13. æ›´æ–°æ³¨æ„åŠ›è¯„åˆ†è¿›åº¦æ¡çš„é¢œè‰²
            if hasattr(self, 'attention_score_label'):
                current_score_text = self.attention_score_label.text()
                try:
                    current_score = float(current_score_text)
                    self.update_progress_bar_color(current_score)
                except ValueError:
                    pass

            # 14. æ›´æ–°å›¾è¡¨ç»Ÿè®¡ä¿¡æ¯
            if hasattr(self, 'realtime_charts'):
                chart_stats = self.realtime_charts.get_statistics()
                if chart_stats:
                    # åœ¨è¿™é‡Œå¯ä»¥æ›´æ–°å›¾è¡¨ç›¸å…³çš„ç»Ÿè®¡æ˜¾ç¤º
                    pass

            # 15. æ£€æŸ¥ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µï¼ˆå¯é€‰ï¼‰
            if self.frame_count % 600 == 0:  # æ¯10ç§’æ£€æŸ¥ä¸€æ¬¡
                self.check_system_resources()

        except Exception as e:
            print(f"æ›´æ–°çŠ¶æ€é”™è¯¯: {e}")
            traceback.print_exc()

            # é”™è¯¯æ—¶æ˜¾ç¤ºåŸºæœ¬çŠ¶æ€
            try:
                if hasattr(self, 'video_info_label'):
                    self.video_info_label.setText("ç³»ç»Ÿé”™è¯¯ - å°è¯•é‡æ–°è¿æ¥")
                if hasattr(self, 'attention_state_label'):
                    self.attention_state_label.setText("ç³»ç»Ÿé”™è¯¯")
                    self.attention_state_label.setStyleSheet("font-size: 16px; font-weight: bold; color: #e74c3c;")
            except:
                pass

    def update_progress_bar_color(self, score):
        """æ ¹æ®åˆ†æ•°æ›´æ–°è¿›åº¦æ¡é¢œè‰²"""
        try:
            if score >= 80:
                color = "#27ae60"  # ç»¿è‰²
            elif score >= 60:
                color = "#f1c40f"  # é»„è‰²
            elif score >= 40:
                color = "#e67e22"  # æ©™è‰²
            else:
                color = "#e74c3c"  # çº¢è‰²

            # è®¾ç½®è¿›åº¦æ¡æ ·å¼
            style = f"""
                QProgressBar {{
                    border: 1px solid #d1d9e6;
                    border-radius: 4px;
                    text-align: center;
                    background-color: white;
                }}
                QProgressBar::chunk {{
                    border-radius: 4px;
                    background-color: {color};
                }}
            """
            if hasattr(self, 'attention_progress'):
                self.attention_progress.setStyleSheet(style)

        except Exception as e:
            print(f"æ›´æ–°è¿›åº¦æ¡é¢œè‰²é”™è¯¯: {e}")

    def check_system_resources(self):
        """æ£€æŸ¥ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ"""
        try:
            import psutil
            cpu_percent = psutil.cpu_percent(interval=0.1)
            memory = psutil.virtual_memory()
            memory_percent = memory.percent

            # å¦‚æœèµ„æºä½¿ç”¨è¿‡é«˜ï¼Œè®°å½•è­¦å‘Š
            if cpu_percent > 80 or memory_percent > 80:
                self.add_alert(f"ç³»ç»Ÿèµ„æºä½¿ç”¨åé«˜ - CPU: {cpu_percent:.1f}%, å†…å­˜: {memory_percent:.1f}%", "warning")

        except ImportError:
            # psutil æœªå®‰è£…ï¼Œè·³è¿‡èµ„æºæ£€æŸ¥
            pass
        except Exception as e:
            print(f"æ£€æŸ¥ç³»ç»Ÿèµ„æºé”™è¯¯: {e}")

    def calculate_focus_duration(self):
        """è®¡ç®—ä¸“æ³¨æ—¶é•¿ï¼ˆå…¼å®¹å‡½æ•°ï¼‰"""
        try:
            if hasattr(self, 'attention_scoring') and hasattr(self.attention_scoring, 'focus_sessions'):
                if self.attention_scoring.focus_sessions:
                    total_duration = sum(session.get('duration', 0)
                                         for session in self.attention_scoring.focus_sessions)
                    return total_duration / len(self.attention_scoring.focus_sessions)

            # å›é€€è®¡ç®—
            if hasattr(self, 'attention_scoring') and hasattr(self.attention_scoring, 'current_focus_duration'):
                return self.attention_scoring.current_focus_duration

            return 0
        except:
            return 0

    def calculate_neutral_duration(self):
        """è®¡ç®—ä¸­æ€§æƒ…ç»ªæ—¶é•¿æ¯”ä¾‹ï¼ˆå…¼å®¹å‡½æ•°ï¼‰"""
        try:
            if hasattr(self, 'emotion_analyzer') and hasattr(self.emotion_analyzer, 'emotion_history'):
                neutral_frames = sum(1 for emotion in self.emotion_analyzer.emotion_history
                                     if emotion == "ä¸­æ€§")
                total_frames = len(self.emotion_analyzer.emotion_history)
                if total_frames > 0:
                    return (neutral_frames / total_frames) * 100

            return 0
        except:
            return 0

    def check_extreme_emotions(self):
        """æ£€æŸ¥æç«¯æƒ…ç»ªï¼ˆå…¼å®¹å‡½æ•°ï¼‰"""
        extreme_emotions = []
        try:
            if hasattr(self, 'emotion_analyzer') and hasattr(self.emotion_analyzer, 'emotion_history'):
                recent_emotions = list(self.emotion_analyzer.emotion_history)[-30:] if len(
                    self.emotion_analyzer.emotion_history) >= 30 else list(self.emotion_analyzer.emotion_history)

                for emotion in ["ç”Ÿæ°”", "ææƒ§"]:
                    count = recent_emotions.count(emotion)
                    if count >= 10:
                        extreme_emotions.append(emotion)
        except:
            pass

        return extreme_emotions

    def check_alerts(self, attention_state, emotion_state):
        """æ£€æŸ¥è­¦æŠ¥æ¡ä»¶"""
        try:
            attention_label = attention_state.get("attention_label", "æœªçŸ¥")
            emotion = emotion_state.get("emotion", "ä¸­æ€§")
            face_detected = attention_state.get("face_detected", False)

            current_time = datetime.now().strftime("%H:%M:%S")

            # æ³¨æ„åŠ›ç›¸å…³è­¦æŠ¥
            if attention_label == "çœ¼ç›é—­åˆ" and self.frame_count % 30 == 0:
                self.add_alert(f"{current_time} - æ£€æµ‹åˆ°çœ¼ç›é—­åˆ", "warning")

            elif attention_label == "è§†çº¿åç¦»" and self.frame_count % 45 == 0:
                self.add_alert(f"{current_time} - è§†çº¿åç¦»å±å¹•", "warning")

            elif attention_label == "è§†çº¿åç§»" and self.frame_count % 60 == 0:
                self.add_alert(f"{current_time} - è§†çº¿åç¦»ä¸­å¿ƒ", "info")

            elif attention_label == "ä¸“æ³¨" and self.frame_count % 90 == 0:
                self.add_alert(f"{current_time} - æ³¨æ„åŠ›ä¿æŒè‰¯å¥½", "positive")

            # æƒ…ç»ªç›¸å…³è­¦æŠ¥
            if emotion in ["ç”Ÿæ°”", "ææƒ§", "æ‚²ä¼¤"] and self.frame_count % 40 == 0:
                self.add_alert(f"{current_time} - æ£€æµ‹åˆ°è´Ÿé¢æƒ…ç»ª: {emotion}", "warning")

            elif emotion == "å¿«ä¹" and self.frame_count % 50 == 0:
                self.add_alert(f"{current_time} - æ­£é¢æƒ…ç»ª: å¿«ä¹", "positive")

            # é¢éƒ¨æ£€æµ‹è­¦æŠ¥
            if not face_detected and self.frame_count % 60 == 0:
                self.add_alert(f"{current_time} - æœªæ£€æµ‹åˆ°é¢éƒ¨", "warning")

        except Exception as e:
            print(f"æ£€æŸ¥è­¦æŠ¥é”™è¯¯: {e}")

    def check_voice_reminders(self, attention_state, emotion_state):
        """æ£€æŸ¥è¯­éŸ³æé†’ï¼ˆä½¿ç”¨æ–°çš„åˆ†æç»“æœï¼‰"""
        if not self.voice_enabled or not self.voice_system.engine:
            return

        # è·å–æ–°çš„åˆ†æç»“æœ
        if hasattr(self, 'score_analysis'):
            attention_level = self.score_analysis.get("attention_level", "ä¸€èˆ¬")
            risk_level = self.score_analysis.get("adhd_features", {}).get("risk_level", "æ­£å¸¸")

            # æ ¹æ®æ–°çš„åˆ†æç»“æœç”Ÿæˆæé†’
            if attention_level in ["ä¸­åº¦åˆ†å¿ƒ", "ä¸¥é‡åˆ†å¿ƒ"]:
                self.voice_system.speak("æ³¨æ„åŠ›åˆ†æ•£äº†ï¼Œè¯·é‡æ–°é›†ä¸­æ³¨æ„åŠ›")

            elif risk_level == "é«˜é£é™©":
                self.voice_system.speak("æ£€æµ‹åˆ°æ³¨æ„åŠ›é—®é¢˜ï¼Œå»ºè®®ä¼‘æ¯ä¸€ä¸‹")

            elif attention_level == "éå¸¸ä¸“æ³¨" and self.frame_count % 100 == 0:
                self.voice_system.speak("å¤ªæ£’äº†ï¼ç»§ç»­ä¿æŒä¸“æ³¨ï¼")

        else:
            # åŸæ¥çš„é€»è¾‘
            attention_label = attention_state.get("attention_label", "æœªçŸ¥")
            emotion = emotion_state.get("emotion", "ä¸­æ€§")

            if attention_label == "çœ¼ç›é—­åˆ":
                self.voice_system.speak("è¯·çå¼€çœ¼ç›ï¼Œçœ‹ç€å±å¹•")
            elif attention_label == "è§†çº¿åç¦»":
                self.voice_system.speak("è¯·çœ‹ç€å±å¹•")

    def add_alert(self, message, alert_type="info"):
        """æ·»åŠ è­¦æŠ¥æ¶ˆæ¯"""
        try:
            # é¿å…é‡å¤è­¦æŠ¥
            if len(self.alerts) > 0 and message in self.alerts[-1]:
                return

            self.alerts.append(message)

            # é™åˆ¶è­¦æŠ¥æ•°é‡
            if len(self.alerts) > 50:
                self.alerts.pop(0)

            # åœ¨æ–‡æœ¬æ¡†ä¸­æ˜¾ç¤º
            cursor = self.alert_text.textCursor()
            cursor.movePosition(QTextCursor.End)

            # è®¾ç½®é¢œè‰²
            if alert_type == "warning":
                self.alert_text.setTextColor(QColor("#e74c3c"))
            elif alert_type == "positive":
                self.alert_text.setTextColor(QColor("#27ae60"))
            else:
                self.alert_text.setTextColor(QColor("#3498db"))

            self.alert_text.insertPlainText(f"{message}\n")

            # æ»šåŠ¨åˆ°åº•éƒ¨
            self.alert_text.verticalScrollBar().setValue(
                self.alert_text.verticalScrollBar().maximum()
            )

        except Exception as e:
            print(f"æ·»åŠ è­¦æŠ¥é”™è¯¯: {e}")

    def toggle_attention_overlay(self, state):
        """åˆ‡æ¢æ³¨æ„åŠ›è¦†ç›–å±‚æ˜¾ç¤º"""
        self.show_attention_overlay = (state == Qt.Checked)

    def toggle_emotion_overlay(self, state):
        """åˆ‡æ¢æƒ…ç»ªè¦†ç›–å±‚æ˜¾ç¤º"""
        self.show_emotion_overlay = (state == Qt.Checked)

    def toggle_voice(self, state):
        """åˆ‡æ¢è¯­éŸ³åŠŸèƒ½"""
        self.voice_enabled = (state == Qt.Checked)
        status = "å¯ç”¨" if self.voice_enabled else "ç¦ç”¨"
        self.add_alert(f"è¯­éŸ³æé†’å·²{status}", "info")

    def test_voice(self):
        """æµ‹è¯•è¯­éŸ³åŠŸèƒ½"""
        if self.voice_system.engine:
            self.voice_system.speak("è¯­éŸ³æµ‹è¯•æˆåŠŸã€‚å¤šåŠ¨ç—‡æ£€æµ‹ç³»ç»Ÿå·²å°±ç»ªã€‚")
            self.add_alert("è¯­éŸ³æµ‹è¯•å®Œæˆ", "info")
        else:
            self.add_alert("è¯­éŸ³ç³»ç»Ÿä¸å¯ç”¨", "warning")

    def reset_analysis(self):
        """é‡ç½®åˆ†æ"""
        self.attention_analyzer.reset()
        self.emotion_analyzer.reset()

        self.attention_stats_history.clear()
        self.emotion_stats_history.clear()
        self.record_data.clear()
        self.alerts.clear()
        self.alert_text.clear()

        self.frame_count = 0
        self.session_start_time = datetime.now()

        # é‡ç½®UIæ˜¾ç¤º
        self.attention_score_label.setText("0")
        self.attention_state_label.setText("åˆå§‹åŒ–ä¸­")
        self.emotion_label.setText("ä¸­æ€§")
        self.confidence_label.setText("0%")

        for bar in self.emotion_bars.values():
            bar.setValue(0)

        self.add_alert("åˆ†æå·²é‡ç½®", "info")

    def export_report(self):
        """å¯¼å‡ºæŠ¥å‘Š"""
        try:
            if not self.record_data:
                QMessageBox.warning(self, "è­¦å‘Š", "æ²¡æœ‰æ•°æ®å¯å¯¼å‡º")
                return

            # é€‰æ‹©ä¿å­˜ä½ç½®
            filename, _ = QFileDialog.getSaveFileName(
                self, "ä¿å­˜åˆ†ææŠ¥å‘Š",
                f"å¤šåŠ¨ç—‡åˆ†ææŠ¥å‘Š_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                "JSONæ–‡ä»¶ (*.json)"
            )

            if not filename:
                return

            # å‡†å¤‡æŠ¥å‘Šæ•°æ®
            attention_stats = self.attention_analyzer.get_attention_stats()
            emotion_stats = self.emotion_analyzer.get_emotion_stats()
            score_analysis = self.attention_scoring.get_score_analysis()
            calibration_status = self.calibration_system.get_calibration_status()
            chart_stats = self.realtime_charts.get_statistics()
            # è·å–æ–°çš„åˆ†æç»“æœ
            score_analysis = self.attention_scoring.get_score_analysis()

            report = {
                "report_info": {
                "title": "å¤šåŠ¨ç—‡å„¿ç«¥æ³¨æ„åŠ›åˆ†ææŠ¥å‘Šï¼ˆä¼˜åŒ–ç‰ˆï¼‰",
                "ç”Ÿæˆæ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "ç‰ˆæœ¬": "5.0",
                "åˆ†ææ—¶é•¿": (
                    datetime.now() - self.session_start_time).total_seconds()
                    if self.session_start_time else 0
            },
                "session_info": {
                    "æ¥æº": "å®æ—¶æ‘„åƒå¤´" if self.is_live else f"è§†é¢‘: {self.video_path}",
                    "å¼€å§‹æ—¶é—´": self.session_start_time.strftime(
                        "%Y-%m-%d %H:%M:%S") if self.session_start_time else "æœªçŸ¥",
                    "ç»“æŸæ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "æ€»å¸§æ•°": self.frame_count,
                    "å½•åˆ¶æ—¶é•¿": f"{self.frame_count / 10:.1f} ç§’" if self.is_live else f"{self.frame_count / 30:.1f} ç§’",
                    "æ ¡å‡†çŠ¶æ€": calibration_status
                },
                "attention_analysis": {
                "æœ€ç»ˆåˆ†æ•°": score_analysis.get("current_score", 0),
                "æ³¨æ„åŠ›æ°´å¹³": score_analysis.get("attention_level", "æœªçŸ¥"),
                "ç»Ÿè®¡ä¿¡æ¯": score_analysis.get("statistics", {}),
                "ä¸“æ³¨åˆ†æ": score_analysis.get("focus_analysis", {}),
                "å¤šåŠ¨ç—‡ç‰¹å¾": score_analysis.get("adhd_features", {}),
                "å»ºè®®": score_analysis.get("recommendations", [])
            },
                "emotion_analysis": {
                    "æœ€ç»ˆæƒ…ç»ª": self.emotion_analyzer.current_emotion,
                    "ç»Ÿè®¡ä¿¡æ¯": emotion_stats,
                    "æƒ…ç»ªåˆ†å¸ƒ": {
                        emotion: prob for emotion, prob in zip(
                            ["ç”Ÿæ°”", "åŒæ¶", "ææƒ§", "å¿«ä¹", "æ‚²ä¼¤", "æƒŠè®¶", "ä¸­æ€§"],
                            self.emotion_analyzer.emotion_probabilities
                        )
                    }
                },
                "calibration_results": {
                    "is_calibrated": calibration_status.get("is_calibrated", False),
                    "reference_center": calibration_status.get("reference_center", (0, 0)),
                    "tolerance": calibration_status.get("tolerance", 0.2),
                    "calibration_data": self.calibration_system.calibration_results
                },
                "adhd_indicators": {
                    "æ³¨æ„åŠ›ç¼ºé™·": attention_stats.get("focus_percentage", 0) < 50,
                    "æ´»åŠ¨è¿‡åº¦": self.attention_analyzer.blinks > 20 and attention_stats.get("blink_rate", 0) > 30,
                    "æƒ…ç»ªä¸ç¨³å®š": emotion_stats.get("emotion_stability", 0) < 70,
                    "è§†çº¿ç¨³å®šæ€§é—®é¢˜": chart_stats.get("gaze", {}).get("x_std", 0) > 0.1 if chart_stats else False,
                    "æ€»ä½“é£é™©": self.calculate_overall_risk(attention_stats, emotion_stats, chart_stats)
                },
                "recommendations": self.generate_recommendations(attention_stats, emotion_stats, chart_stats),
                "chart_statistics": chart_stats,
                "sample_data": self.record_data[:100] if len(self.record_data) > 100 else self.record_data,
                "alerts": self.alerts[-20:]
            }

            # ä¿å­˜æŠ¥å‘Š
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)

            QMessageBox.information(self, "æˆåŠŸ",
                                    f"æŠ¥å‘Šä¿å­˜æˆåŠŸï¼\n\n"
                                    f"æ–‡ä»¶: {filename}\n"
                                    f"åˆ†æå¸§æ•°: {self.frame_count}\n"
                                    f"æ³¨æ„åŠ›åˆ†æ•°: {score_analysis.get('current_score', 0):.1f}\n"
                                    f"ä¸»å¯¼æƒ…ç»ª: {self.emotion_analyzer.current_emotion}\n"
                                    f"æ ¡å‡†çŠ¶æ€: {'å·²æ ¡å‡†' if calibration_status.get('is_calibrated') else 'æœªæ ¡å‡†'}")

        except Exception as e:
            QMessageBox.critical(self, "é”™è¯¯", f"å¯¼å‡ºæŠ¥å‘Šå¤±è´¥: {str(e)}")

    def generate_recommendations(self, attention_stats, emotion_stats):
        """ç”Ÿæˆå»ºè®®"""
        recommendations = []

        # æ³¨æ„åŠ›ç›¸å…³å»ºè®®
        focus_percentage = attention_stats.get("focus_percentage", 0)
        if focus_percentage < 40:
            recommendations.append("æ£€æµ‹åˆ°ä¸¥é‡æ³¨æ„åŠ›ç¼ºé™·ã€‚å»ºè®®æ¯15-20åˆ†é’Ÿå®‰æ’ç»“æ„æ€§ä¼‘æ¯ã€‚")
        elif focus_percentage < 60:
            recommendations.append("æ£€æµ‹åˆ°ä¸­åº¦æ³¨æ„åŠ›é—®é¢˜ã€‚å°è¯•å‡å°‘ç¯å¢ƒå¹²æ‰°ã€‚")
        else:
            recommendations.append("æ³¨æ„åŠ›æ°´å¹³ä¿æŒè‰¯å¥½ã€‚ç»§ç»­å½“å‰ç­–ç•¥ã€‚")

        # æƒ…ç»ªç›¸å…³å»ºè®®
        positive_ratio = emotion_stats.get("positive_ratio", 0)
        if positive_ratio < 40:
            recommendations.append("ç§¯ææƒ…ç»ªæ¯”ä¾‹è¾ƒä½ã€‚å»ºè®®å¢åŠ æ›´å¤šæœ‰è¶£å’Œå¥–åŠ±æ€§çš„æ´»åŠ¨ã€‚")

        emotion_stability = emotion_stats.get("emotion_stability", 0)
        if emotion_stability < 60:
            recommendations.append("è§‚å¯Ÿåˆ°æƒ…ç»ªä¸ç¨³å®šã€‚å»ºè®®æ•™æˆæƒ…ç»ªè°ƒèŠ‚æŠ€å·§ã€‚")

        # ç»¼åˆå»ºè®®
        blink_rate = attention_stats.get("blink_rate", 0)
        if blink_rate > 25:
            recommendations.append("çœ¨çœ¼é¢‘ç‡è¾ƒé«˜ï¼Œå¯èƒ½è¡¨ç¤ºç–²åŠ³æˆ–å‹åŠ›ã€‚ç¡®ä¿å……åˆ†ä¼‘æ¯ã€‚")

        if self.emotion_analyzer.current_emotion in ["ç”Ÿæ°”", "ææƒ§", "æ‚²ä¼¤"]:
            recommendations.append("æ£€æµ‹åˆ°è´Ÿé¢æƒ…ç»ªã€‚å»ºè®®æä¾›æƒ…æ„Ÿæ”¯æŒå’Œåº”å¯¹ç­–ç•¥ã€‚")

        # å¤šåŠ¨ç—‡ç‰¹å®šå»ºè®®
        recommendations.append("é’ˆå¯¹å¤šåŠ¨ç—‡å„¿ç«¥ï¼šä½¿ç”¨è§†è§‰æ—¶é—´è¡¨ã€è®¡æ—¶å™¨å’Œé¢‘ç¹çš„ç§¯æå¼ºåŒ–ã€‚")
        recommendations.append("å°†ä»»åŠ¡åˆ†è§£ä¸ºå°æ­¥éª¤ï¼Œå¹¶æä¾›å³æ—¶åé¦ˆã€‚")
        recommendations.append("å…è®¸æ´»åŠ¨ä¼‘æ¯ï¼Œå¦‚æœéœ€è¦å¯æä¾›é€‚å½“çš„ç©å…·ã€‚")

        return recommendations

    def closeEvent(self, event):
        """å…³é—­çª—å£äº‹ä»¶"""
        reply = QMessageBox.question(
            self, "ç¡®è®¤é€€å‡º",
            "ç¡®å®šè¦é€€å‡ºå—ï¼Ÿæ‰€æœ‰æœªä¿å­˜çš„æ•°æ®å°†ä¼šä¸¢å¤±ã€‚",
            QMessageBox.Yes | QMessageBox.No, QMessageBox.No
        )

        if reply == QMessageBox.Yes:
            self.stop_video()

            if self.voice_system:
                self.voice_system.stop()

            event.accept()
        else:
            event.ignore()


# ============================================================================
# é¢éƒ¨å»ºæ¨¡
# ============================================================================

class FacialModeling:
    """é¢éƒ¨å»ºæ¨¡åŠŸèƒ½ - ç”¨äºæ ¡å‡†å’Œä¸ªæ€§åŒ–è®¾ç½®"""

    def __init__(self):
        self.face_mesh = mp_face_mesh.FaceMesh(
            static_image_mode=True,
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )

        # å­˜å‚¨ç”¨æˆ·çš„é¢éƒ¨ç‰¹å¾
        self.user_profile = {
            "face_landmarks": None,
            "reference_points": {
                "neutral_gaze": (0.0, 0.0),  # ä¸­æ€§è§†çº¿ä½ç½®
                "eye_size": (0.0, 0.0),  # çœ¼ç›å°ºå¯¸
                "pupil_distance": 0.0,  # ç³å­”è·ç¦»
                "head_pose_neutral": (0.0, 0.0, 0.0)  # ä¸­æ€§å¤´éƒ¨å§¿æ€
            },
            "calibration_data": [],
            "is_calibrated": False
        }

    def extract_face_features(self, frame):
        """æå–é¢éƒ¨ç‰¹å¾"""
        try:
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            h, w = frame.shape[:2]

            results = self.face_mesh.process(rgb_frame)

            if results.multi_face_landmarks:
                landmarks = results.multi_face_landmarks[0].landmark

                # è½¬æ¢ä¸ºnumpyæ•°ç»„
                points = []
                for lm in landmarks:
                    x, y = int(lm.x * w), int(lm.y * h)
                    points.append((x, y))

                points = np.array(points)

                # æå–å…³é”®ç‰¹å¾ç‚¹
                features = {
                    "face_points": points,
                    "left_eye": points[L_EYE],
                    "right_eye": points[R_EYE],
                    "left_iris": points[LEFT_IRIS],
                    "right_iris": points[RIGHT_IRIS],
                    "nose": points[1],  # é¼»å°–
                    "mouth_left": points[61],
                    "mouth_right": points[291]
                }

                # è®¡ç®—çœ¼éƒ¨ç‰¹å¾
                left_eye_h = np.linalg.norm(features["left_eye"][1] - features["left_eye"][4])
                right_eye_h = np.linalg.norm(features["right_eye"][1] - features["right_eye"][4])

                # è®¡ç®—ç³å­”è·ç¦»
                left_pupil = features["left_iris"].mean(axis=0)
                right_pupil = features["right_iris"].mean(axis=0)
                pupil_distance = np.linalg.norm(left_pupil - right_pupil)

                # è®¡ç®—è§†çº¿æ–¹å‘
                gaze_vector = self.calculate_gaze_vector(features)

                return {
                    "features": features,
                    "eye_size": (left_eye_h, right_eye_h),
                    "pupil_distance": pupil_distance,
                    "gaze_direction": gaze_vector,
                    "valid": True
                }

            return {"valid": False}

        except Exception as e:
            print(f"é¢éƒ¨ç‰¹å¾æå–é”™è¯¯: {e}")
            return {"valid": False}

    def calculate_gaze_vector(self, features):
        """è®¡ç®—è§†çº¿æ–¹å‘å‘é‡"""
        try:
            # è®¡ç®—å·¦çœ¼è§†çº¿
            eye_bbox_l = cv2.boundingRect(features["left_eye"])
            iris_center_l = features["left_iris"].mean(axis=0)

            # è®¡ç®—å³çœ¼è§†çº¿
            eye_bbox_r = cv2.boundingRect(features["right_eye"])
            iris_center_r = features["right_iris"].mean(axis=0)

            # å½’ä¸€åŒ–è§†çº¿å‘é‡
            def normalize_gaze(bbox, iris_center):
                center_x = bbox[0] + bbox[2] / 2
                center_y = bbox[1] + bbox[3] / 2

                dx = (iris_center[0] - center_x) / (bbox[2] / 2)
                dy = (iris_center[1] - center_y) / (bbox[3] / 2)

                return (dx, dy)

            gaze_l = normalize_gaze(eye_bbox_l, iris_center_l)
            gaze_r = normalize_gaze(eye_bbox_r, iris_center_r)

            # å¹³å‡åŒçœ¼è§†çº¿
            gaze_x = (gaze_l[0] + gaze_r[0]) / 2
            gaze_y = (gaze_l[1] + gaze_r[1]) / 2

            return (gaze_x, gaze_y)

        except Exception as e:
            print(f"è®¡ç®—è§†çº¿å‘é‡é”™è¯¯: {e}")
            return (0.0, 0.0)

    def calibrate(self, frame, calibration_type="neutral"):
        """æ ¡å‡†é¢éƒ¨ç‰¹å¾"""
        result = self.extract_face_features(frame)

        if result["valid"]:
            if calibration_type == "neutral":
                # è®°å½•ä¸­æ€§è§†çº¿
                self.user_profile["reference_points"]["neutral_gaze"] = result["gaze_direction"]
                self.user_profile["reference_points"]["eye_size"] = result["eye_size"]
                self.user_profile["reference_points"]["pupil_distance"] = result["pupil_distance"]

                # æ·»åŠ åˆ°æ ¡å‡†æ•°æ®
                self.user_profile["calibration_data"].append({
                    "type": "neutral",
                    "gaze": result["gaze_direction"],
                    "eye_size": result["eye_size"],
                    "timestamp": time.time()
                })

                # è®¡ç®—å¹³å‡å€¼
                if len(self.user_profile["calibration_data"]) >= 5:
                    self.calculate_calibration_average()
                    self.user_profile["is_calibrated"] = True

                return True

        return False

    def calculate_calibration_average(self):
        """è®¡ç®—æ ¡å‡†æ•°æ®çš„å¹³å‡å€¼"""
        if not self.user_profile["calibration_data"]:
            return

        neutral_frames = [d for d in self.user_profile["calibration_data"] if d["type"] == "neutral"]

        if neutral_frames:
            # è®¡ç®—è§†çº¿å¹³å‡å€¼
            gaze_x_list = [d["gaze"][0] for d in neutral_frames]
            gaze_y_list = [d["gaze"][1] for d in neutral_frames]

            avg_gaze_x = np.mean(gaze_x_list[-5:])  # å–æœ€å5å¸§
            avg_gaze_y = np.mean(gaze_y_list[-5:])

            self.user_profile["reference_points"]["neutral_gaze"] = (avg_gaze_x, avg_gaze_y)

            # è®¡ç®—çœ¼éƒ¨å°ºå¯¸å¹³å‡å€¼
            left_eye_list = [d["eye_size"][0] for d in neutral_frames]
            right_eye_list = [d["eye_size"][1] for d in neutral_frames]

            avg_left_eye = np.mean(left_eye_list[-5:])
            avg_right_eye = np.mean(right_eye_list[-5:])

            self.user_profile["reference_points"]["eye_size"] = (avg_left_eye, avg_right_eye)

    def get_calibration_status(self):
        """è·å–æ ¡å‡†çŠ¶æ€"""
        status = {
            "is_calibrated": self.user_profile["is_calibrated"],
            "calibration_frames": len(self.user_profile["calibration_data"]),
            "neutral_gaze": self.user_profile["reference_points"]["neutral_gaze"],
            "remaining_frames": max(0, 5 - len(self.user_profile["calibration_data"]))
        }
        return status

    def reset_calibration(self):
        """é‡ç½®æ ¡å‡†æ•°æ®"""
        self.user_profile = {
            "face_landmarks": None,
            "reference_points": {
                "neutral_gaze": (0.0, 0.0),
                "eye_size": (0.0, 0.0),
                "pupil_distance": 0.0,
                "head_pose_neutral": (0.0, 0.0, 0.0)
            },
            "calibration_data": [],
            "is_calibrated": False
        }


# ============================================================================
#  æ³¨æ„åŠ›å¾—åˆ†è®¡ç®—æœºåˆ¶
# ============================================================================

# ============================================================================
# ä¼˜åŒ–çš„æ³¨æ„åŠ›å¾—åˆ†è®¡ç®—æœºåˆ¶
# ============================================================================

class OptimizedAttentionScoringSystem:
    """ä¼˜åŒ–çš„æ³¨æ„åŠ›å¾—åˆ†è®¡ç®—ç³»ç»Ÿï¼ˆé’ˆå¯¹å¤šåŠ¨ç—‡å„¿ç«¥ï¼‰"""

    def __init__(self):
        # å¤šåŠ¨ç—‡å„¿ç«¥ç‰¹æœ‰çš„æ³¨æ„åŠ›ç‰¹å¾æƒé‡
        self.weights = {
            "eye_openness": 50,  # çœ¼ç›çå¼€ç¨‹åº¦
            "gaze_stability": 20,  # è§†çº¿ç¨³å®šæ€§
            "head_stability": 10,  # å¤´éƒ¨ç¨³å®šæ€§
            "focus_duration": 10,  # æŒç»­ä¸“æ³¨æ—¶é—´ï¼ˆå¤šåŠ¨ç—‡å…³é”®æŒ‡æ ‡ï¼‰
            "blink_pattern": 5,  # çœ¨çœ¼æ¨¡å¼
            "motor_restlessness": 5  # åŠ¨ä½œä¸å®‰ï¼ˆæ–°å¢æŒ‡æ ‡ï¼‰
        }

        # é’ˆå¯¹å¤šåŠ¨ç—‡å„¿ç«¥çš„ä¼˜åŒ–å‚æ•°
        self.scoring_params = {
            # çœ¼éƒ¨å‚æ•°ï¼ˆå¤šåŠ¨ç—‡å„¿ç«¥å¯èƒ½çœ¨çœ¼æ›´é¢‘ç¹ï¼‰
            "ear_optimal": 0.22,
            "ear_good_threshold": 0.20,
            "ear_fair_threshold": 0.18,
            "ear_bad_threshold": 0.16,
            "ear_asymmetry_threshold": 0.05,  # å·¦å³çœ¼EARå·®å¼‚é˜ˆå€¼

            # è§†çº¿å‚æ•°ï¼ˆå¤šåŠ¨ç—‡å„¿ç«¥è§†çº¿æ›´ä¸ç¨³å®šï¼‰
            "gaze_optimal": 0.15,
            "gaze_good_threshold": 0.25,
            "gaze_fair_threshold": 0.35,
            "gaze_bad_threshold": 0.50,
            "gaze_speed_threshold": 0.8,  # è§†çº¿ç§»åŠ¨é€Ÿåº¦é˜ˆå€¼

            # å¤´éƒ¨å§¿æ€å‚æ•°ï¼ˆå¤šåŠ¨ç—‡å„¿ç«¥å¤´éƒ¨ç§»åŠ¨æ›´é¢‘ç¹ï¼‰
            "head_optimal": 8.0,
            "head_good_threshold": 15.0,
            "head_fair_threshold": 25.0,
            "head_bad_threshold": 35.0,
            "head_speed_threshold": 10.0,  # å¤´éƒ¨ç§»åŠ¨é€Ÿåº¦é˜ˆå€¼

            # çœ¨çœ¼å‚æ•°ï¼ˆå¤šåŠ¨ç—‡å„¿ç«¥çœ¨çœ¼æ¨¡å¼å¼‚å¸¸ï¼‰
            "blink_optimal_min": 10,
            "blink_optimal_max": 30,
            "blink_too_fast": 40,
            "blink_too_slow": 5,
            "blink_cluster_threshold": 5,  # è¿ç»­çœ¨çœ¼é˜ˆå€¼

            # ä¸“æ³¨æ—¶é•¿å‚æ•°
            "short_focus_threshold": 2.0,  # çŸ­æ—¶ä¸“æ³¨é˜ˆå€¼ï¼ˆç§’ï¼‰
            "medium_focus_threshold": 5.0,  # ä¸­ç­‰ä¸“æ³¨é˜ˆå€¼
            "long_focus_threshold": 10.0,  # é•¿æ—¶ä¸“æ³¨é˜ˆå€¼

            # åŠ¨ä½œä¸å®‰å‚æ•°
            "motor_threshold": 0.3,  # åŠ¨ä½œä¸å®‰é˜ˆå€¼
            "micro_movement_freq": 3.0  # å¾®å°åŠ¨ä½œé¢‘ç‡é˜ˆå€¼ï¼ˆæ¬¡/ç§’ï¼‰
        }

        # å†å²æ•°æ®è®°å½•ï¼ˆå¢åŠ æ—¶é—´æˆ³ï¼‰
        self.gaze_history = deque(maxlen=300)
        self.head_pose_history = deque(maxlen=300)
        self.ear_history = deque(maxlen=300)
        self.attention_history = deque(maxlen=600)
        self.score_history = deque(maxlen=600)
        self.timestamps = deque(maxlen=600)  # æ—¶é—´æˆ³è®°å½•

        # ä¸“æ³¨çŠ¶æ€è®°å½•ï¼ˆå¢å¼ºç‰ˆï¼‰
        self.focus_start_time = None
        self.current_focus_duration = 0
        self.longest_focus_duration = 0
        self.focus_interruptions = 0
        self.focus_sessions = []  # è®°å½•æ¯æ¬¡ä¸“æ³¨ä¼šè¯
        self.focus_quality_history = deque(maxlen=100)  # ä¸“æ³¨è´¨é‡å†å²

        # åŠ¨ä½œä¸å®‰è®°å½•
        self.motor_movements = deque(maxlen=100)  # åŠ¨ä½œè®°å½•
        self.micro_movement_count = 0  # å¾®å°åŠ¨ä½œè®¡æ•°
        self.last_head_position = None
        self.head_movement_speed_history = deque(maxlen=50)

        # çœ¨çœ¼æ¨¡å¼åˆ†æ
        self.blink_timestamps = deque(maxlen=100)  # çœ¨çœ¼æ—¶é—´æˆ³
        self.blink_clusters = []  # çœ¨çœ¼ç°‡è®°å½•
        self.current_blink_cluster = 0

        # è‡ªé€‚åº”é˜ˆå€¼ï¼ˆæ ¹æ®ç”¨æˆ·è¡¨ç°åŠ¨æ€è°ƒæ•´ï¼‰
        self.adaptive_params = {
            "user_ear_baseline": 0.22,
            "user_gaze_stability": 0.2,
            "user_head_stability": 10.0,
            "learning_rate": 0.01  # å­¦ä¹ ç‡
        }

        # å¤šåŠ¨ç—‡ç‰¹å¾æ£€æµ‹
        self.adhd_features = {
            "inattention_count": 0,
            "hyperactivity_count": 0,
            "impulsivity_events": [],
            "pattern_recognition": []
        }

    def calculate_attention_score(self, attention_state, emotion_state=None):
        """è®¡ç®—ç»¼åˆæ³¨æ„åŠ›åˆ†æ•°ï¼ˆé’ˆå¯¹å¤šåŠ¨ç—‡å„¿ç«¥ä¼˜åŒ–ï¼‰"""
        try:
            current_time = time.time()
            self.timestamps.append(current_time)

            # è·å–å½“å‰çŠ¶æ€
            gaze_x = attention_state.get("gaze_x", 0)
            gaze_y = attention_state.get("gaze_y", 0)
            yaw = attention_state.get("yaw", 0)
            pitch = attention_state.get("pitch", 0)
            ear_left = attention_state.get("ear_left", 0)
            ear_right = attention_state.get("ear_right", 0)
            attention_label = attention_state.get("attention_label", "æœªçŸ¥")
            face_detected = attention_state.get("face_detected", False)

            # 1. çœ¼ç›ç‰¹å¾è¯„åˆ† (0-25åˆ†)
            eye_score = self.calculate_eye_score_optimized(
                ear_left, ear_right, attention_label
            )

            # 2. è§†çº¿ç¨³å®šæ€§è¯„åˆ† (0-20åˆ†)
            gaze_score = self.calculate_gaze_score_optimized(
                gaze_x, gaze_y, attention_label
            )

            # 3. å¤´éƒ¨ç¨³å®šæ€§è¯„åˆ† (0-15åˆ†)
            head_score = self.calculate_head_score_optimized(
                yaw, pitch, attention_label
            )

            # 4. æŒç»­ä¸“æ³¨æ—¶é—´è¯„åˆ† (0-20åˆ†) - å¤šåŠ¨ç—‡å…³é”®æŒ‡æ ‡
            duration_score = self.calculate_duration_score_optimized(
                attention_label, current_time
            )

            # 5. çœ¨çœ¼æ¨¡å¼è¯„åˆ† (0-10åˆ†)
            blink_score = self.calculate_blink_score_optimized(
                ear_left, ear_right, current_time
            )

            # 6. åŠ¨ä½œä¸å®‰è¯„åˆ† (0-10åˆ†)
            motor_score = self.calculate_motor_score(
                yaw, pitch, current_time
            )

            # è®¡ç®—åŸºç¡€æ€»åˆ†
            base_score = (
                    eye_score + gaze_score + head_score +
                    duration_score + blink_score + motor_score
            )

            # 7. æƒ…ç»ªå½±å“è°ƒæ•´ï¼ˆè€ƒè™‘å¤šåŠ¨ç—‡å„¿ç«¥æƒ…ç»ªæ•æ„Ÿæ€§ï¼‰
            emotion_adjustment = self.calculate_emotion_adjustment_optimized(
                emotion_state, attention_label
            )

            # 8. å¤šåŠ¨ç—‡ç‰¹å¾æ£€æµ‹ä¸è°ƒæ•´
            adhd_adjustment = self.detect_adhd_features(
                attention_state, emotion_state, current_time
            )

            # 9. é¢éƒ¨æ£€æµ‹çŠ¶æ€è°ƒæ•´
            if not face_detected:
                base_score = max(0, base_score * 0.6)  # æœªæ£€æµ‹åˆ°é¢éƒ¨ï¼Œæƒ©ç½šæ›´å¤§

            # è®¡ç®—æœ€ç»ˆåˆ†æ•°
            total_score = base_score + emotion_adjustment + adhd_adjustment

            # åº”ç”¨éçº¿æ€§å˜æ¢ï¼Œçªå‡ºä¸´ç•ŒåŒºåŸŸ
            total_score = self.apply_nonlinear_scaling(total_score)

            # é™åˆ¶åœ¨0-100èŒƒå›´å†…
            total_score = max(0, min(100, total_score))

            # æ›´æ–°è‡ªé€‚åº”å‚æ•°
            self.update_adaptive_params(
                ear_left, ear_right, gaze_x, gaze_y, yaw, pitch
            )

            # æ›´æ–°ä¸“æ³¨çŠ¶æ€
            self.update_focus_state_optimized(
                attention_label, total_score, current_time
            )

            # æ›´æ–°å†å²è®°å½•
            self.update_history_optimized(
                attention_state, total_score, current_time
            )

            # è®°å½•ä¸“æ³¨è´¨é‡
            focus_quality = self.calculate_focus_quality(
                eye_score, gaze_score, head_score, duration_score
            )
            self.focus_quality_history.append(focus_quality)

            return total_score

        except Exception as e:
            print(f"è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°é”™è¯¯: {e}")
            traceback.print_exc()
            return 50  # è¿”å›å®‰å…¨ä¸­é—´åˆ†æ•°

    def calculate_eye_score_optimized(self, ear_left, ear_right, attention_label):
        """ä¼˜åŒ–çš„çœ¼ç›ç‰¹å¾è¯„åˆ†"""
        if attention_label == "çœ¼ç›é—­åˆ":
            return 0

        ear_avg = (ear_left + ear_right) / 2
        ear_asymmetry = abs(ear_left - ear_right)

        # åŸºç¡€è¯„åˆ†ï¼ˆè€ƒè™‘EARå€¼ï¼‰
        if ear_avg >= self.scoring_params["ear_optimal"]:
            base_score = 20
        elif ear_avg >= self.scoring_params["ear_good_threshold"]:
            base_score = 16
        elif ear_avg >= self.scoring_params["ear_fair_threshold"]:
            base_score = 12
        elif ear_avg >= self.scoring_params["ear_bad_threshold"]:
            base_score = 6
        else:
            base_score = 0

        # æƒ©ç½šå·¦å³çœ¼ä¸å¯¹ç§°ï¼ˆå¯èƒ½è¡¨ç¤ºæ–œè§†æˆ–ç–²åŠ³ï¼‰
        if ear_asymmetry > self.scoring_params["ear_asymmetry_threshold"]:
            asymmetry_penalty = min(5, ear_asymmetry * 20)
            base_score -= asymmetry_penalty

        # å¥–åŠ±çœ¼ç›ç¨³å®šæ€§ï¼ˆEARå€¼æ³¢åŠ¨å°ï¼‰
        if len(self.ear_history) >= 30:
            recent_ears = list(self.ear_history)[-30:]
            ear_std = np.std(recent_ears)
            if ear_std < 0.02:  # éå¸¸ç¨³å®š
                base_score += 3
            elif ear_std < 0.04:  # ç¨³å®š
                base_score += 2

        return max(0, base_score)

    def calculate_gaze_score_optimized(self, gaze_x, gaze_y, attention_label):
        """ä¼˜åŒ–çš„è§†çº¿ç¨³å®šæ€§è¯„åˆ†"""
        if attention_label in ["è§†çº¿åç¦»", "è§†çº¿åç§»"]:
            return 0

        gaze_magnitude = math.sqrt(gaze_x ** 2 + gaze_y ** 2)
        current_gaze = (gaze_x, gaze_y)

        # åŸºç¡€è¯„åˆ†ï¼ˆè€ƒè™‘è§†çº¿åç§»ï¼‰
        if gaze_magnitude <= self.scoring_params["gaze_optimal"]:
            base_score = 15
        elif gaze_magnitude <= self.scoring_params["gaze_good_threshold"]:
            base_score = 12
        elif gaze_magnitude <= self.scoring_params["gaze_fair_threshold"]:
            base_score = 9
        elif gaze_magnitude <= self.scoring_params["gaze_bad_threshold"]:
            base_score = 5
        else:
            base_score = 0

        # æƒ©ç½šè§†çº¿ç§»åŠ¨é€Ÿåº¦ï¼ˆå¿«é€Ÿæ‰«è§†å¯èƒ½æ˜¯æ³¨æ„åŠ›ä¸é›†ä¸­ï¼‰
        if len(self.gaze_history) >= 2:
            recent_gaze = list(self.gaze_history)[-2:]
            if len(recent_gaze) == 2:
                prev_gaze_mag = recent_gaze[0]
                gaze_speed = abs(gaze_magnitude - prev_gaze_mag)

                if gaze_speed > self.scoring_params["gaze_speed_threshold"]:
                    speed_penalty = min(5, gaze_speed * 3)
                    base_score -= speed_penalty

        # å¥–åŠ±è§†çº¿ç¨³å®šæ€§ï¼ˆé•¿æ—¶é—´ä¿æŒç¨³å®šï¼‰
        if len(self.gaze_history) >= 60:  # 2ç§’å†å²
            recent_gazes = list(self.gaze_history)[-60:]
            gaze_std = np.std(recent_gazes)
            if gaze_std < 0.1:  # éå¸¸ç¨³å®š
                base_score += 3
            elif gaze_std < 0.2:  # ç¨³å®š
                base_score += 2

        return max(0, base_score)

    def calculate_head_score_optimized(self, yaw, pitch, attention_label):
        """ä¼˜åŒ–çš„å¤´éƒ¨ç¨³å®šæ€§è¯„åˆ†"""
        if attention_label == "è§†çº¿åç¦»":
            return 0

        # è®¡ç®—å¤´éƒ¨åç§»çš„åˆæˆå€¼
        head_offset = math.sqrt(yaw ** 2 + pitch ** 2)
        current_head_pos = (yaw, pitch)

        # åŸºç¡€è¯„åˆ†
        if head_offset <= self.scoring_params["head_optimal"]:
            base_score = 12
        elif head_offset <= self.scoring_params["head_good_threshold"]:
            base_score = 10
        elif head_offset <= self.scoring_params["head_fair_threshold"]:
            base_score = 8
        elif head_offset <= self.scoring_params["head_bad_threshold"]:
            base_score = 4
        else:
            base_score = 0

        # æƒ©ç½šå¤´éƒ¨ç§»åŠ¨é€Ÿåº¦ï¼ˆå¤šåŠ¨ç—‡ç‰¹å¾ï¼‰
        if self.last_head_position is not None:
            prev_yaw, prev_pitch = self.last_head_position
            head_movement = math.sqrt(
                (yaw - prev_yaw) ** 2 + (pitch - prev_pitch) ** 2
            )

            if head_movement > self.scoring_params["head_speed_threshold"]:
                movement_penalty = min(4, head_movement * 2)
                base_score -= movement_penalty

            # è®°å½•ç§»åŠ¨é€Ÿåº¦
            if len(self.timestamps) >= 2:
                time_diff = self.timestamps[-1] - self.timestamps[-2]
                if time_diff > 0:
                    head_speed = head_movement / time_diff
                    self.head_movement_speed_history.append(head_speed)

        # æ›´æ–°å¤´éƒ¨ä½ç½®
        self.last_head_position = (yaw, pitch)

        # å¥–åŠ±å¤´éƒ¨ç¨³å®šæ€§
        if len(self.head_pose_history) >= 60:
            recent_heads = list(self.head_pose_history)[-60:]
            head_std = np.std(recent_heads)
            if head_std < 5.0:  # éå¸¸ç¨³å®š
                base_score += 2
            elif head_std < 10.0:  # ç¨³å®š
                base_score += 1

        return max(0, base_score)

    def calculate_duration_score_optimized(self, attention_label, current_time):
        """ä¼˜åŒ–çš„æŒç»­ä¸“æ³¨æ—¶é—´è¯„åˆ†ï¼ˆå¤šåŠ¨ç—‡å…³é”®æŒ‡æ ‡ï¼‰"""
        if self.focus_start_time is None:
            return 5  # åŸºç¡€åˆ†

        focus_duration = current_time - self.focus_start_time

        # å¤šåŠ¨ç—‡å„¿ç«¥é€šå¸¸ä¸“æ³¨æ—¶é—´è¾ƒçŸ­ï¼Œé€‚å½“è°ƒæ•´è¯„åˆ†æ ‡å‡†
        if focus_duration >= self.scoring_params["long_focus_threshold"]:
            return 18  # é•¿æ—¶ä¸“æ³¨ï¼ˆä¼˜ç§€ï¼‰
        elif focus_duration >= self.scoring_params["medium_focus_threshold"]:
            return 14  # ä¸­ç­‰ä¸“æ³¨ï¼ˆè‰¯å¥½ï¼‰
        elif focus_duration >= self.scoring_params["short_focus_threshold"]:
            return 9  # çŸ­æ—¶ä¸“æ³¨ï¼ˆä¸€èˆ¬ï¼‰
        else:
            return 4  # çŸ­æš‚ä¸“æ³¨

    def calculate_blink_score_optimized(self, ear_left, ear_right, current_time):
        """ä¼˜åŒ–çš„çœ¨çœ¼æ¨¡å¼è¯„åˆ†"""
        if len(self.ear_history) < 30:
            return 5

        # æ£€æµ‹çœ¨çœ¼ï¼ˆEARå€¼ä½äºé˜ˆå€¼ï¼‰
        if ear_left < self.scoring_params["ear_bad_threshold"] and \
                ear_right < self.scoring_params["ear_bad_threshold"]:
            self.blink_timestamps.append(current_time)
            self.current_blink_cluster += 1
        else:
            # å¦‚æœè¶…è¿‡ä¸€å®šæ—¶é—´æ²¡æœ‰çœ¨çœ¼ï¼Œç»“æŸå½“å‰çœ¨çœ¼ç°‡
            if self.current_blink_cluster > 0:
                if len(self.blink_timestamps) > 0:
                    last_blink = self.blink_timestamps[-1]
                    if current_time - last_blink > 0.5:  # 0.5ç§’å†…æ²¡æœ‰æ–°çœ¨çœ¼
                        if self.current_blink_cluster >= self.scoring_params["blink_cluster_threshold"]:
                            self.blink_clusters.append(self.current_blink_cluster)
                        self.current_blink_cluster = 0

        # è®¡ç®—æœ€è¿‘10ç§’çš„çœ¨çœ¼é¢‘ç‡
        recent_timestamps = [
            ts for ts in self.blink_timestamps
            if current_time - ts <= 10
        ]
        blink_rate = len(recent_timestamps) / 10.0 * 60  # è½¬æ¢ä¸ºæ¯åˆ†é’Ÿ

        # è¯„åˆ†ï¼ˆå¤šåŠ¨ç—‡å„¿ç«¥çœ¨çœ¼é¢‘ç‡å¯èƒ½åé«˜ï¼‰
        if (blink_rate >= self.scoring_params["blink_optimal_min"] and
                blink_rate <= self.scoring_params["blink_optimal_max"]):
            base_score = 8
        elif blink_rate > self.scoring_params["blink_too_fast"]:
            # çœ¨çœ¼è¿‡å¿«ï¼ˆå¯èƒ½æ˜¯ç–²åŠ³æˆ–ç„¦è™‘ï¼‰
            base_score = 3
        elif blink_rate < self.scoring_params["blink_too_slow"]:
            # çœ¨çœ¼è¿‡å°‘ï¼ˆå¯èƒ½æ˜¯è¿‡åº¦ä¸“æ³¨æˆ–ç–²åŠ³ï¼‰
            base_score = 4
        else:
            base_score = 6

        # æƒ©ç½šçœ¨çœ¼ç°‡ï¼ˆè¿ç»­å¿«é€Ÿçœ¨çœ¼ï¼‰
        if len(self.blink_clusters) > 0 and self.blink_clusters[-1] >= 3:
            base_score -= 2

        return max(0, base_score)

    def calculate_motor_score(self, yaw, pitch, current_time):
        """è®¡ç®—åŠ¨ä½œä¸å®‰è¯„åˆ†ï¼ˆå¤šåŠ¨ç—‡ç‰¹å¾ï¼‰"""
        if self.last_head_position is None:
            self.last_head_position = (yaw, pitch)
            return 5

        # è®¡ç®—å¤´éƒ¨å¾®å°ç§»åŠ¨
        prev_yaw, prev_pitch = self.last_head_position
        movement = math.sqrt((yaw - prev_yaw) ** 2 + (pitch - prev_pitch) ** 2)

        # è®°å½•å¾®å°åŠ¨ä½œ
        if movement > 0.5 and movement < 5.0:  # å¾®å°ç§»åŠ¨èŒƒå›´
            self.micro_movement_count += 1
            self.motor_movements.append({
                "timestamp": current_time,
                "movement": movement
            })

        # è®¡ç®—æœ€è¿‘5ç§’çš„å¾®å°åŠ¨ä½œé¢‘ç‡
        recent_movements = [
            m for m in self.motor_movements
            if current_time - m["timestamp"] <= 5
        ]
        movement_freq = len(recent_movements) / 5.0

        # è¯„åˆ†ï¼ˆåŠ¨ä½œä¸å®‰è¶Šå¤šï¼Œåˆ†æ•°è¶Šä½ï¼‰
        base_score = 8
        if movement_freq > self.scoring_params["micro_movement_freq"]:
            # åŠ¨ä½œä¸å®‰æ˜æ˜¾
            base_score -= 4
            self.adhd_features["hyperactivity_count"] += 1
        elif movement_freq > self.scoring_params["micro_movement_freq"] / 2:
            # ä¸­åº¦åŠ¨ä½œä¸å®‰
            base_score -= 2

        # æ›´æ–°å¤´éƒ¨ä½ç½®
        self.last_head_position = (yaw, pitch)

        return max(0, base_score)

    def calculate_emotion_adjustment_optimized(self, emotion_state, attention_label):
        """ä¼˜åŒ–çš„æƒ…ç»ªå½±å“è°ƒæ•´ï¼ˆè€ƒè™‘å¤šåŠ¨ç—‡å„¿ç«¥æƒ…ç»ªè°ƒèŠ‚å›°éš¾ï¼‰"""
        if not emotion_state:
            return 0

        emotion = emotion_state.get("emotion", "ä¸­æ€§")
        confidence = emotion_state.get("confidence", 0)

        # å¤šåŠ¨ç—‡å„¿ç«¥çš„æƒ…ç»ªæ•æ„Ÿæ€§è°ƒæ•´
        emotion_effects = {
            "ç”Ÿæ°”": -10,  # å¤šåŠ¨ç—‡å„¿ç«¥ç”Ÿæ°”æ—¶æ³¨æ„åŠ›æ›´å·®
            "ææƒ§": -8,  # ææƒ§å¯¼è‡´æ³¨æ„åŠ›åˆ†æ•£
            "æ‚²ä¼¤": -6,  # æ‚²ä¼¤å½±å“æ³¨æ„åŠ›ç»´æŒ
            "åŒæ¶": -4,  # åŒæ¶æœ‰è´Ÿé¢å½±å“
            "ä¸­æ€§": 0,  # ä¸­æ€§æƒ…ç»ªæœ€åˆ©äºæ³¨æ„åŠ›
            "æƒŠè®¶": +3,  # æƒŠè®¶å¯èƒ½çŸ­æš‚æé«˜æ³¨æ„åŠ›
            "å¿«ä¹": +6  # å¿«ä¹æƒ…ç»ªæœ‰åŠ©äºæ³¨æ„åŠ›ï¼Œä½†å¯èƒ½è¿‡åº¦å…´å¥‹
        }

        base_adjustment = emotion_effects.get(emotion, 0)

        # è€ƒè™‘æƒ…ç»ªå¼ºåº¦ï¼ˆç½®ä¿¡åº¦ï¼‰
        adjusted = base_adjustment * confidence

        # å¦‚æœæ˜¯å¿«ä¹æƒ…ç»ªä½†æ³¨æ„åŠ›æ ‡ç­¾ä¸º"ä¸“æ³¨"ï¼Œé¢å¤–å¥–åŠ±
        if emotion == "å¿«ä¹" and attention_label == "ä¸“æ³¨":
            adjusted += 2

        return adjusted

    def detect_adhd_features(self, attention_state, emotion_state, current_time):
        """æ£€æµ‹å¤šåŠ¨ç—‡ç‰¹å¾å¹¶è°ƒæ•´åˆ†æ•°"""
        adjustment = 0

        # 1. æ³¨æ„åŠ›ä¸é›†ä¸­ç‰¹å¾
        attention_label = attention_state.get("attention_label", "æœªçŸ¥")
        if attention_label in ["è§†çº¿åç¦»", "è§†çº¿åç§»", "çœ¼ç›é—­åˆ"]:
            self.adhd_features["inattention_count"] += 1

            # è¿ç»­åˆ†å¿ƒæƒ©ç½š
            if len(self.attention_history) >= 3:
                recent_labels = list(self.attention_history)[-3:]
                if all(label != "ä¸“æ³¨" for label in recent_labels):
                    adjustment -= 5
                elif sum(1 for label in recent_labels if label != "ä¸“æ³¨") >= 2:
                    adjustment -= 3

        # 2. æƒ…ç»ªä¸ç¨³å®šç‰¹å¾
        if emotion_state:
            emotion = emotion_state.get("emotion", "ä¸­æ€§")
            if emotion in ["ç”Ÿæ°”", "ææƒ§"]:
                # è®°å½•æƒ…ç»ªæ³¢åŠ¨äº‹ä»¶
                self.adhd_features["impulsivity_events"].append({
                    "timestamp": current_time,
                    "emotion": emotion
                })

                # é¢‘ç¹æƒ…ç»ªæ³¢åŠ¨æƒ©ç½š
                recent_events = [
                    e for e in self.adhd_features["impulsivity_events"]
                    if current_time - e["timestamp"] <= 30  # 30ç§’å†…
                ]
                if len(recent_events) >= 3:
                    adjustment -= 4

        # 3. æ¨¡å¼è¯†åˆ«ï¼ˆåˆ†å¿ƒ-é‡æ–°ä¸“æ³¨çš„å¾ªç¯æ¨¡å¼ï¼‰
        if len(self.attention_history) >= 20:
            recent_pattern = list(self.attention_history)[-20:]
            focus_transitions = sum(
                1 for i in range(1, len(recent_pattern))
                if recent_pattern[i] == "ä¸“æ³¨" and recent_pattern[i - 1] != "ä¸“æ³¨"
            )

            # é¢‘ç¹çš„æ³¨æ„åŠ›è½¬ç§»ï¼ˆå¯èƒ½æ˜¯æ³¨æ„åŠ›åˆ†æ•£ï¼‰
            if focus_transitions >= 5:
                adjustment -= 3
                self.adhd_features["pattern_recognition"].append({
                    "timestamp": current_time,
                    "pattern": "frequent_transitions"
                })

        return adjustment

    def apply_nonlinear_scaling(self, score):
        """åº”ç”¨éçº¿æ€§ç¼©æ”¾ï¼Œçªå‡ºä¸´ç•ŒåŒºåŸŸ"""
        if score >= 80:
            # é«˜åˆ†æ®µï¼šè½»å¾®å‹ç¼©
            return 80 + (score - 80) * 0.8
        elif score >= 60:
            # ä¸­ç­‰åˆ†æ®µï¼šä¿æŒçº¿æ€§
            return score
        elif score >= 40:
            # ä½åˆ†æ®µï¼šé€‚å½“æ”¾å¤§å·®å¼‚
            return 40 + (score - 40) * 1.2
        else:
            # å¾ˆä½åˆ†æ®µï¼šè¿›ä¸€æ­¥æ”¾å¤§å·®å¼‚
            return score * 1.5

    def update_adaptive_params(self, ear_left, ear_right, gaze_x, gaze_y, yaw, pitch):
        """æ ¹æ®ç”¨æˆ·è¡¨ç°è‡ªé€‚åº”è°ƒæ•´å‚æ•°"""
        # å­¦ä¹ ç‡
        alpha = self.adaptive_params["learning_rate"]

        # æ›´æ–°EARåŸºçº¿
        ear_avg = (ear_left + ear_right) / 2
        self.adaptive_params["user_ear_baseline"] = (
                (1 - alpha) * self.adaptive_params["user_ear_baseline"] +
                alpha * ear_avg
        )

        # æ›´æ–°è§†çº¿ç¨³å®šæ€§åŸºçº¿
        gaze_magnitude = math.sqrt(gaze_x ** 2 + gaze_y ** 2)
        self.adaptive_params["user_gaze_stability"] = (
                (1 - alpha) * self.adaptive_params["user_gaze_stability"] +
                alpha * gaze_magnitude
        )

        # æ›´æ–°å¤´éƒ¨ç¨³å®šæ€§åŸºçº¿
        head_offset = math.sqrt(yaw ** 2 + pitch ** 2)
        self.adaptive_params["user_head_stability"] = (
                (1 - alpha) * self.adaptive_params["user_head_stability"] +
                alpha * head_offset
        )

    def update_focus_state_optimized(self, attention_label, current_score, current_time):
        """ä¼˜åŒ–çš„ä¸“æ³¨çŠ¶æ€æ›´æ–°"""
        if attention_label == "ä¸“æ³¨" and current_score >= 65:  # é™ä½ä¸“æ³¨é˜ˆå€¼
            # è¿›å…¥æˆ–ä¿æŒä¸“æ³¨çŠ¶æ€
            if self.focus_start_time is None:
                self.focus_start_time = current_time
                self.current_focus_duration = 0

            self.current_focus_duration = current_time - self.focus_start_time

            # æ›´æ–°æœ€é•¿ä¸“æ³¨æ—¶é•¿
            if self.current_focus_duration > self.longest_focus_duration:
                self.longest_focus_duration = self.current_focus_duration

            # è®°å½•é«˜è´¨é‡ä¸“æ³¨
            if current_score >= 80 and self.current_focus_duration >= 3.0:
                self.focus_sessions.append({
                    "start": self.focus_start_time,
                    "duration": self.current_focus_duration,
                    "quality": current_score
                })
        else:
            # ä¸“æ³¨ä¸­æ–­
            if self.focus_start_time is not None:
                # è®°å½•ä¸­æ–­å‰çš„ä¸“æ³¨ä¼šè¯
                if self.current_focus_duration >= 1.0:  # è‡³å°‘ä¸“æ³¨1ç§’
                    self.focus_sessions.append({
                        "start": self.focus_start_time,
                        "duration": self.current_focus_duration,
                        "end": current_time,
                        "interrupted": True
                    })

                self.focus_interruptions += 1
                self.focus_start_time = None
                self.current_focus_duration = 0

    def update_history_optimized(self, attention_state, score, timestamp):
        """ä¼˜åŒ–çš„å†å²è®°å½•æ›´æ–°"""
        # è®°å½•å½“å‰åˆ†æ•°
        self.score_history.append(score)

        # è®°å½•æ³¨æ„åŠ›æ ‡ç­¾
        attention_label = attention_state.get("attention_label", "æœªçŸ¥")
        self.attention_history.append(attention_label)

        # è®°å½•å…¶ä»–æ•°æ®
        gaze_x = attention_state.get("gaze_x", 0)
        gaze_y = attention_state.get("gaze_y", 0)
        gaze_magnitude = math.sqrt(gaze_x ** 2 + gaze_y ** 2)
        self.gaze_history.append(gaze_magnitude)

        yaw = attention_state.get("yaw", 0)
        pitch = attention_state.get("pitch", 0)
        self.head_pose_history.append((yaw + pitch) / 2)

        ear_avg = (attention_state.get("ear_left", 0) + attention_state.get("ear_right", 0)) / 2
        self.ear_history.append(ear_avg)

    def calculate_focus_quality(self, eye_score, gaze_score, head_score, duration_score):
        """è®¡ç®—ä¸“æ³¨è´¨é‡æŒ‡æ•°"""
        # å½’ä¸€åŒ–å„ä¸ªåˆ†æ•°åˆ°0-1èŒƒå›´
        eye_norm = eye_score / 25.0
        gaze_norm = gaze_score / 20.0
        head_norm = head_score / 15.0
        duration_norm = duration_score / 20.0

        # åŠ æƒå¹³å‡
        weights = [0.25, 0.20, 0.15, 0.40]  # æŒç»­ä¸“æ³¨æ—¶é—´æƒé‡æœ€é«˜
        quality = (
                          eye_norm * weights[0] +
                          gaze_norm * weights[1] +
                          head_norm * weights[2] +
                          duration_norm * weights[3]
                  ) * 100

        return quality

    def get_score_analysis(self):
        """è·å–åˆ†æ•°è¯¦ç»†åˆ†æï¼ˆå¢å¼ºç‰ˆï¼‰"""
        if not self.score_history:
            return self.get_empty_analysis()

        try:
            scores = list(self.score_history)
            current_score = scores[-1] if scores else 0

            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            stats = self.calculate_statistics(scores)

            # è®¡ç®—ä¸“æ³¨è´¨é‡åˆ†æ
            focus_analysis = self.analyze_focus_patterns()

            # è®¡ç®—å¤šåŠ¨ç—‡ç‰¹å¾åˆ†æ
            adhd_analysis = self.analyze_adhd_features()

            # ç»„åˆåˆ†æç»“æœ
            analysis = {
                "current_score": round(current_score, 1),
                "statistics": stats,
                "focus_analysis": focus_analysis,
                "adhd_features": adhd_analysis,
                "attention_level": self.get_attention_level(current_score),
                "recommendations": self.generate_recommendations(
                    current_score, focus_analysis, adhd_analysis
                )
            }

            return analysis

        except Exception as e:
            print(f"è·å–åˆ†æ•°åˆ†æé”™è¯¯: {e}")
            return self.get_empty_analysis()

    def calculate_statistics(self, scores):
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        if len(scores) < 10:
            return {"error": "æ•°æ®ä¸è¶³"}

        try:
            recent_scores = scores[-30:] if len(scores) >= 30 else scores
            long_term_scores = scores[-300:] if len(scores) >= 300 else scores

            return {
                "recent_avg": round(np.mean(recent_scores), 1),
                "long_term_avg": round(np.mean(long_term_scores), 1),
                "recent_max": round(np.max(recent_scores), 1),
                "recent_min": round(np.min(recent_scores), 1),
                "recent_std": round(np.std(recent_scores), 1),
                "trend": self.calculate_trend(scores),
                "stability_index": self.calculate_stability_index(scores),
                "consistency_score": self.calculate_consistency_score(scores)
            }
        except Exception as e:
            print(f"è®¡ç®—ç»Ÿè®¡ä¿¡æ¯é”™è¯¯: {e}")
            return {}

    def calculate_trend(self, scores):
        """è®¡ç®—åˆ†æ•°è¶‹åŠ¿"""
        if len(scores) < 20:
            return "åˆ†æä¸­"

        try:
            recent = scores[-10:]
            earlier = scores[-20:-10] if len(scores) >= 20 else recent

            recent_avg = np.mean(recent)
            earlier_avg = np.mean(earlier)

            if recent_avg > earlier_avg + 5:
                return "ä¸Šå‡"
            elif recent_avg < earlier_avg - 5:
                return "ä¸‹é™"
            else:
                return "ç¨³å®š"
        except:
            return "æœªçŸ¥"

    def calculate_stability_index(self, scores):
        """è®¡ç®—ç¨³å®šæ€§æŒ‡æ•°"""
        if len(scores) < 30:
            return 0

        try:
            scores_array = np.array(scores[-30:])
            # è®¡ç®—å˜åŒ–ç‡çš„ç¨³å®šæ€§
            changes = np.diff(scores_array)
            stability = 1.0 - (np.std(changes) / 50.0)  # å½’ä¸€åŒ–
            return max(0, min(100, stability * 100))
        except:
            return 0

    def calculate_consistency_score(self, scores):
        """è®¡ç®—ä¸€è‡´æ€§åˆ†æ•°"""
        if len(scores) < 50:
            return 0

        try:
            scores_array = np.array(scores[-50:])
            # è®¡ç®—åœ¨å¹³å‡åˆ†Â±10åˆ†èŒƒå›´å†…çš„æ¯”ä¾‹
            mean_score = np.mean(scores_array)
            within_range = np.sum(np.abs(scores_array - mean_score) <= 10)
            consistency = within_range / len(scores_array)
            return round(consistency * 100, 1)
        except:
            return 0

    def analyze_focus_patterns(self):
        """åˆ†æä¸“æ³¨æ¨¡å¼"""
        if not self.focus_sessions:
            return {"total_sessions": 0, "avg_duration": 0, "pattern": "æ— ä¸“æ³¨è®°å½•"}

        try:
            durations = [s.get("duration", 0) for s in self.focus_sessions]
            qualities = [s.get("quality", 0) for s in self.focus_sessions if "quality" in s]

            avg_duration = np.mean(durations) if durations else 0
            avg_quality = np.mean(qualities) if qualities else 0

            # åˆ†æä¸“æ³¨æ¨¡å¼
            if len(durations) >= 5:
                # è®¡ç®—ä¸“æ³¨æ—¶é•¿åˆ†å¸ƒ
                short_focus = sum(1 for d in durations if d < 3.0)
                medium_focus = sum(1 for d in durations if 3.0 <= d < 10.0)
                long_focus = sum(1 for d in durations if d >= 10.0)

                total = len(durations)
                pattern = f"çŸ­æ—¶ä¸“æ³¨:{short_focus / total * 100:.0f}%, "
                pattern += f"ä¸­æ—¶ä¸“æ³¨:{medium_focus / total * 100:.0f}%, "
                pattern += f"é•¿æ—¶ä¸“æ³¨:{long_focus / total * 100:.0f}%"
            else:
                pattern = "æ•°æ®ä¸è¶³"

            return {
                "total_sessions": len(self.focus_sessions),
                "avg_duration": round(avg_duration, 1),
                "avg_quality": round(avg_quality, 1),
                "longest_duration": round(self.longest_focus_duration, 1),
                "interruptions": self.focus_interruptions,
                "pattern": pattern
            }
        except Exception as e:
            print(f"åˆ†æä¸“æ³¨æ¨¡å¼é”™è¯¯: {e}")
            return {"error": str(e)}

    def analyze_adhd_features(self):
        """åˆ†æå¤šåŠ¨ç—‡ç‰¹å¾"""
        try:
            # è®¡ç®—æ³¨æ„åŠ›ä¸é›†ä¸­æ¯”ä¾‹
            total_frames = len(self.attention_history)
            if total_frames == 0:
                return {}

            inattention_frames = sum(
                1 for label in self.attention_history
                if label != "ä¸“æ³¨" and label != "åˆå§‹åŒ–ä¸­"
            )
            inattention_ratio = inattention_frames / total_frames

            # è®¡ç®—åŠ¨ä½œä¸å®‰æŒ‡æ•°
            motor_index = 0
            if self.head_movement_speed_history:
                avg_speed = np.mean(list(self.head_movement_speed_history))
                motor_index = min(100, avg_speed * 10)

            # è®¡ç®—æƒ…ç»ªæ³¢åŠ¨æŒ‡æ•°
            emotion_volatility = 0
            if self.adhd_features["impulsivity_events"]:
                recent_events = [
                    e for e in self.adhd_features["impulsivity_events"]
                    if time.time() - e["timestamp"] <= 300  # 5åˆ†é’Ÿå†…
                ]
                emotion_volatility = len(recent_events) / 5.0 * 100  # æ¯åˆ†é’Ÿäº‹ä»¶æ•°Ã—100

            return {
                "inattention_ratio": round(inattention_ratio * 100, 1),
                "hyperactivity_index": round(motor_index, 1),
                "emotion_volatility": round(emotion_volatility, 1),
                "pattern_detected": len(self.adhd_features["pattern_recognition"]) > 0,
                "risk_level": self.calculate_adhd_risk_level(
                    inattention_ratio, motor_index, emotion_volatility
                )
            }
        except Exception as e:
            print(f"åˆ†æå¤šåŠ¨ç—‡ç‰¹å¾é”™è¯¯: {e}")
            return {}

    def calculate_adhd_risk_level(self, inattention_ratio, motor_index, emotion_volatility):
        """è®¡ç®—å¤šåŠ¨ç—‡é£é™©ç­‰çº§"""
        score = (
                        inattention_ratio * 0.4 +
                        (motor_index / 100) * 0.3 +
                        (emotion_volatility / 100) * 0.3
                ) * 100

        if score >= 70:
            return "é«˜é£é™©"
        elif score >= 50:
            return "ä¸­é£é™©"
        elif score >= 30:
            return "ä½é£é™©"
        else:
            return "æ­£å¸¸"

    def get_attention_level(self, score):
        """è·å–æ³¨æ„åŠ›æ°´å¹³æè¿°"""
        if score >= 85:
            return "éå¸¸ä¸“æ³¨"
        elif score >= 70:
            return "ä¸“æ³¨"
        elif score >= 55:
            return "ä¸€èˆ¬"
        elif score >= 40:
            return "è½»åº¦åˆ†å¿ƒ"
        elif score >= 25:
            return "ä¸­åº¦åˆ†å¿ƒ"
        else:
            return "ä¸¥é‡åˆ†å¿ƒ"

    def generate_recommendations(self, current_score, focus_analysis, adhd_analysis):
        """ç”Ÿæˆä¸ªæ€§åŒ–å»ºè®®"""
        recommendations = []

        # åŸºäºå½“å‰åˆ†æ•°
        if current_score < 40:
            recommendations.append("æ³¨æ„åŠ›æ°´å¹³è¾ƒä½ï¼Œå»ºè®®ä¼‘æ¯åé‡æ–°å¼€å§‹")
        elif current_score < 60:
            recommendations.append("æ³¨æ„åŠ›ä¸€èˆ¬ï¼Œå°è¯•å‡å°‘ç¯å¢ƒå¹²æ‰°")

        # åŸºäºä¸“æ³¨æ¨¡å¼
        if "avg_duration" in focus_analysis:
            avg_duration = focus_analysis["avg_duration"]
            if avg_duration < 3.0:
                recommendations.append("ä¸“æ³¨æŒç»­æ—¶é—´è¾ƒçŸ­ï¼Œå»ºè®®ä½¿ç”¨ç•ªèŒ„å·¥ä½œæ³•ï¼ˆ25åˆ†é’Ÿå·¥ä½œï¼Œ5åˆ†é’Ÿä¼‘æ¯ï¼‰")
            elif avg_duration < 10.0:
                recommendations.append("ä¸“æ³¨æ—¶é—´ä¸­ç­‰ï¼Œç»§ç»­ä¿æŒ")
            else:
                recommendations.append("ä¸“æ³¨æ—¶é—´è‰¯å¥½ï¼Œæ³¨æ„é€‚æ—¶ä¼‘æ¯")

        # åŸºäºå¤šåŠ¨ç—‡ç‰¹å¾
        if "risk_level" in adhd_analysis:
            risk_level = adhd_analysis["risk_level"]
            if risk_level == "é«˜é£é™©":
                recommendations.append("æ£€æµ‹åˆ°æ˜æ˜¾çš„å¤šåŠ¨ç—‡ç‰¹å¾ï¼Œå»ºè®®å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿ")
            elif risk_level == "ä¸­é£é™©":
                recommendations.append("æ£€æµ‹åˆ°éƒ¨åˆ†å¤šåŠ¨ç—‡ç‰¹å¾ï¼Œå»ºè®®è¿›è¡Œæ³¨æ„åŠ›è®­ç»ƒ")
            elif risk_level == "ä½é£é™©":
                recommendations.append("æœ‰è½»å¾®çš„å¤šåŠ¨ç—‡å€¾å‘ï¼Œä¿æŒè§‚å¯Ÿ")

        # é€šç”¨å»ºè®®
        recommendations.append("ç¡®ä¿å……è¶³ç¡çœ å’Œè§„å¾‹ä½œæ¯")
        recommendations.append("è¿›è¡Œé€‚å½“çš„ä½“è‚²é”»ç‚¼")
        recommendations.append("ä½¿ç”¨è®¡æ—¶å™¨å¸®åŠ©ç®¡ç†æ—¶é—´")

        return recommendations

    def get_empty_analysis(self):
        """è·å–ç©ºåˆ†æç»“æœ"""
        return {
            "current_score": 0,
            "statistics": {
                "recent_avg": 0,
                "long_term_avg": 0,
                "recent_max": 0,
                "recent_min": 0,
                "recent_std": 0,
                "trend": "åˆ†æä¸­",
                "stability_index": 0,
                "consistency_score": 0
            },
            "focus_analysis": {
                "total_sessions": 0,
                "avg_duration": 0,
                "avg_quality": 0,
                "longest_duration": 0,
                "interruptions": 0,
                "pattern": "æ— æ•°æ®"
            },
            "adhd_features": {},
            "attention_level": "æœªçŸ¥",
            "recommendations": ["ç­‰å¾…æ›´å¤šæ•°æ®..."]
        }

    def reset(self):
        """é‡ç½®åˆ†æ•°ç³»ç»Ÿ"""
        self.gaze_history.clear()
        self.head_pose_history.clear()
        self.ear_history.clear()
        self.attention_history.clear()
        self.score_history.clear()
        self.timestamps.clear()
        self.focus_quality_history.clear()
        self.motor_movements.clear()
        self.head_movement_speed_history.clear()
        self.blink_timestamps.clear()
        self.blink_clusters.clear()

        # é‡ç½®ä¸“æ³¨çŠ¶æ€
        self.focus_start_time = None
        self.current_focus_duration = 0
        self.longest_focus_duration = 0
        self.focus_interruptions = 0
        self.focus_sessions.clear()

        # é‡ç½®åŠ¨ä½œè®°å½•
        self.micro_movement_count = 0
        self.last_head_position = None
        self.current_blink_cluster = 0

        # é‡ç½®å¤šåŠ¨ç—‡ç‰¹å¾
        self.adhd_features = {
            "inattention_count": 0,
            "hyperactivity_count": 0,
            "impulsivity_events": [],
            "pattern_recognition": []
        }

# ============================================================================
#  æ ¡å‡†
# ============================================================================

class CalibrationSystem:
    """æ ¡å‡†ç³»ç»Ÿ - ç¡®ä¿æ­£å¸¸ä¸“æ³¨çŠ¶æ€"""

    def __init__(self):
        self.calibration_steps = [
            "center",  # ä¸­å¿ƒä½ç½®
            "top_left",  # å·¦ä¸Š
            "top_right",  # å³ä¸Š
            "bottom_left",  # å·¦ä¸‹
            "bottom_right"  # å³ä¸‹
        ]

        self.current_step = 0
        self.is_calibrating = False
        self.calibration_data = {step: [] for step in self.calibration_steps}
        self.calibration_results = {}
        self.reference_gaze_center = (0.0, 0.0)
        self.gaze_tolerance = 0.2

        # æ ¡å‡†æ–‡ä»¶è·¯å¾„
        self.calibration_file = "calibration_data.json"

        # å°è¯•åŠ è½½å·²æœ‰çš„æ ¡å‡†æ•°æ®
        self.load_calibration()

    def save_calibration(self):
        """ä¿å­˜æ ¡å‡†æ•°æ®åˆ°æ–‡ä»¶"""
        try:
            data = {
                "reference_gaze_center": self.reference_gaze_center,
                "gaze_tolerance": self.gaze_tolerance,
                "calibration_results": self.calibration_results,
                "save_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            with open(self.calibration_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"æ ¡å‡†æ•°æ®å·²ä¿å­˜åˆ°: {self.calibration_file}")
            return True
        except Exception as e:
            print(f"ä¿å­˜æ ¡å‡†æ•°æ®å¤±è´¥: {e}")
            return False

    def load_calibration(self):
        """ä»æ–‡ä»¶åŠ è½½æ ¡å‡†æ•°æ®"""
        try:
            if os.path.exists(self.calibration_file):
                with open(self.calibration_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                self.reference_gaze_center = tuple(data.get("reference_gaze_center", (0.0, 0.0)))
                self.gaze_tolerance = data.get("gaze_tolerance", 0.2)
                self.calibration_results = data.get("calibration_results", {})
                print(f"å·²åŠ è½½æ ¡å‡†æ•°æ® (ä¿å­˜æ—¶é—´: {data.get('save_time', 'æœªçŸ¥')})")
                return True
            else:
                print("æœªæ‰¾åˆ°æ ¡å‡†æ–‡ä»¶ï¼Œéœ€è¦é‡æ–°æ ¡å‡†")
                return False
        except Exception as e:
            print(f"åŠ è½½æ ¡å‡†æ•°æ®å¤±è´¥: {e}")
            return False

    def start_calibration(self):
        """å¼€å§‹æ ¡å‡†"""
        self.is_calibrating = True
        self.current_step = 0
        self.calibration_data = {step: [] for step in self.calibration_steps}
        self.calibration_results = {}

        return {
            "status": "å¼€å§‹",
            "current_step": self.calibration_steps[self.current_step],
            "instruction": "è¯·æ³¨è§†å±å¹•ä¸­å¤®çš„çº¢ç‚¹"
        }

    def process_calibration_frame(self, frame, gaze_data):
        """å¤„ç†æ ¡å‡†å¸§"""
        if not self.is_calibrating:
            return None

        step_name = self.calibration_steps[self.current_step]

        # è®°å½•å½“å‰æ­¥éª¤çš„æ•°æ®
        self.calibration_data[step_name].append({
            "gaze_x": gaze_data.get("gaze_x", 0),
            "gaze_y": gaze_data.get("gaze_y", 0),
            "timestamp": time.time()
        })

        # æ£€æŸ¥æ˜¯å¦æ”¶é›†äº†è¶³å¤Ÿæ•°æ®ï¼ˆçº¦2ç§’ï¼Œ60å¸§ï¼‰
        if len(self.calibration_data[step_name]) >= 60:
            self.complete_current_step()

            if self.current_step < len(self.calibration_steps) - 1:
                # è¿›å…¥ä¸‹ä¸€æ­¥
                self.current_step += 1
                next_step = self.calibration_steps[self.current_step]

                return {
                    "status": "ç»§ç»­",
                    "current_step": next_step,
                    "progress": (self.current_step + 1) / len(self.calibration_steps),
                    "instruction": self.get_instruction(next_step)
                }
            else:
                # æ‰€æœ‰æ­¥éª¤å®Œæˆ
                result = self.finalize_calibration()
                if result.get("success"):
                    # ä¿å­˜æ ¡å‡†ç»“æœ
                    self.save_calibration()
                return {
                    "status": "å®Œæˆ",
                    "progress": 1.0,
                    "results": self.calibration_results,
                    "success": result.get("success", False)
                }

        # ä»åœ¨å½“å‰æ­¥éª¤
        return {
            "status": "è¿›è¡Œä¸­",
            "current_step": step_name,
            "progress": len(self.calibration_data[step_name]) / 60,
            "samples": len(self.calibration_data[step_name]),
            "instruction": f"ä¿æŒæ³¨è§† {step_name}"
        }

    def finalize_calibration(self):
        """å®Œæˆæ ¡å‡†"""
        try:
            # è®¡ç®—ä¸­å¿ƒä½ç½®çš„è§†çº¿èŒƒå›´
            center_data = self.calibration_results.get("center", {})
            if center_data:
                gaze_x, gaze_y = center_data["average_gaze"]
                std_x, std_y = center_data["stability"]

                # è®¾ç½®å‚è€ƒä¸­å¿ƒç‚¹å’Œå®¹å·®
                self.reference_gaze_center = (gaze_x, gaze_y)

                # è®¡ç®—å®¹å·®èŒƒå›´ï¼ˆå¹³å‡å€¼ Â± 2å€æ ‡å‡†å·®ï¼‰
                self.gaze_tolerance = max(0.15, 2 * max(std_x, std_y))

                # è®¡ç®—å„ä¸ªæ–¹å‘çš„è§†çº¿åç§»
                offsets = {}
                for step in self.calibration_steps:
                    if step != "center" and step in self.calibration_results:
                        offset_x = abs(self.calibration_results[step]["average_gaze"][0] - gaze_x)
                        offset_y = abs(self.calibration_results[step]["average_gaze"][1] - gaze_y)
                        offsets[step] = (offset_x, offset_y)

                self.is_calibrating = False

                return {
                    "success": True,
                    "reference_center": self.reference_gaze_center,
                    "tolerance": self.gaze_tolerance,
                    "offsets": offsets
                }

            return {"success": False, "error": "ä¸­å¿ƒæ ¡å‡†æ•°æ®ç¼ºå¤±"}

        except Exception as e:
            print(f"å®Œæˆæ ¡å‡†é”™è¯¯: {e}")
            return {"success": False, "error": str(e)}

    def get_calibration_status(self):
        """è·å–æ ¡å‡†çŠ¶æ€"""
        status = {
            "is_calibrating": self.is_calibrating,
            "current_step": self.calibration_steps[self.current_step] if self.is_calibrating else "æ— ",
            "reference_center": self.reference_gaze_center,
            "tolerance": self.gaze_tolerance,
            "is_calibrated": self.reference_gaze_center != (0, 0)
        }

        if self.is_calibrating:
            step_name = self.calibration_steps[self.current_step]
            status["progress"] = len(self.calibration_data[step_name]) / 60
            status["samples"] = len(self.calibration_data[step_name])

        return status

    def get_instruction(self, step_name):
        """è·å–æ ¡å‡†æŒ‡ä»¤"""
        instructions = {
            "center": "è¯·æ³¨è§†å±å¹•ä¸­å¤®çš„çº¢ç‚¹",
            "top_left": "è¯·æ³¨è§†å±å¹•å·¦ä¸Šè§’çš„çº¢ç‚¹",
            "top_right": "è¯·æ³¨è§†å±å¹•å³ä¸Šè§’çš„çº¢ç‚¹",
            "bottom_left": "è¯·æ³¨è§†å±å¹•å·¦ä¸‹è§’çš„çº¢ç‚¹",
            "bottom_right": "è¯·æ³¨è§†å±å¹•å³ä¸‹è§’çš„çº¢ç‚¹"
        }
        return instructions.get(step_name, "è¯·æ³¨è§†çº¢ç‚¹")

    def complete_current_step(self):
        """å®Œæˆå½“å‰æ ¡å‡†æ­¥éª¤"""
        step_name = self.calibration_steps[self.current_step]

        if self.calibration_data[step_name]:
            # è®¡ç®—å¹³å‡å€¼
            gaze_x_list = [d["gaze_x"] for d in self.calibration_data[step_name][-30:]]  # å–å30å¸§
            gaze_y_list = [d["gaze_y"] for d in self.calibration_data[step_name][-30:]]

            avg_gaze_x = np.mean(gaze_x_list)
            avg_gaze_y = np.mean(gaze_y_list)

            # è®¡ç®—æ ‡å‡†å·®ï¼ˆè¯„ä¼°ç¨³å®šæ€§ï¼‰
            std_gaze_x = np.std(gaze_x_list)
            std_gaze_y = np.std(gaze_y_list)

            self.calibration_results[step_name] = {
                "average_gaze": (avg_gaze_x, avg_gaze_y),
                "stability": (std_gaze_x, std_gaze_y),
                "samples": len(self.calibration_data[step_name])
            }

    def check_gaze_within_tolerance(self, gaze_x, gaze_y):
        """æ£€æŸ¥è§†çº¿æ˜¯å¦åœ¨å®¹å·®èŒƒå›´å†…"""
        if not hasattr(self, 'reference_gaze_center') or self.reference_gaze_center == (0, 0):
            return True  # æœªæ ¡å‡†ï¼Œè¿”å›é»˜è®¤å€¼

        ref_x, ref_y = self.reference_gaze_center

        # è®¡ç®—æ¬§æ°è·ç¦»
        distance = math.sqrt((gaze_x - ref_x) ** 2 + (gaze_y - ref_y) ** 2)

        return distance <= self.gaze_tolerance

    def reset_calibration(self):
        """é‡ç½®æ ¡å‡†"""
        self.is_calibrating = False
        self.current_step = 0
        self.calibration_data = {step: [] for step in self.calibration_steps}
        self.calibration_results = {}
        self.reference_gaze_center = (0.0, 0.0)
        self.gaze_tolerance = 0.2


# ============================================================================
#  å®æ—¶å›¾è¡¨
# ============================================================================

class RealTimeCharts:
    """å®æ—¶å›¾è¡¨ç»˜åˆ¶ç³»ç»Ÿ"""

    def __init__(self):
        self.history_length = 100  # å­˜å‚¨100ä¸ªæ•°æ®ç‚¹
        self.chart_width = 380
        self.chart_height = 180

        # æ•°æ®å†å²
        self.attention_scores = deque(maxlen=self.history_length)
        self.gaze_x_values = deque(maxlen=self.history_length)
        self.gaze_y_values = deque(maxlen=self.history_length)
        self.ear_values = deque(maxlen=self.history_length)
        self.head_yaw_values = deque(maxlen=self.history_length)
        self.head_pitch_values = deque(maxlen=self.history_length)

        # å›¾è¡¨é¢œè‰²
        self.colors = {
            "attention": QColor(66, 134, 244),  # è“è‰²
            "gaze_x": QColor(244, 67, 54),  # çº¢è‰²
            "gaze_y": QColor(76, 175, 80),  # ç»¿è‰²
            "ear": QColor(255, 152, 0),  # æ©™è‰²
            "head": QColor(156, 39, 176),  # ç´«è‰²
            "grid": QColor(200, 200, 200, 100),  # ç½‘æ ¼çº¿
            "background": QColor(245, 245, 245)  # èƒŒæ™¯
        }

    def update_data(self, attention_state, emotion_state=None):
        """æ›´æ–°æ•°æ®"""
        try:
            # æ³¨æ„åŠ›åˆ†æ•°
            self.attention_scores.append(attention_state.get("attention_score", 0))

            # è§†çº¿æ•°æ®
            self.gaze_x_values.append(attention_state.get("gaze_x", 0))
            self.gaze_y_values.append(attention_state.get("gaze_y", 0))

            # EARå€¼
            ear_avg = (attention_state.get("ear_left", 0) + attention_state.get("ear_right", 0)) / 2
            self.ear_values.append(ear_avg)

            # å¤´éƒ¨å§¿æ€
            self.head_yaw_values.append(abs(attention_state.get("yaw", 0)))
            self.head_pitch_values.append(abs(attention_state.get("pitch", 0)))

        except Exception as e:
            print(f"æ›´æ–°å›¾è¡¨æ•°æ®é”™è¯¯: {e}")

    def draw_attention_chart(self, painter, x, y, width, height):
        """ç»˜åˆ¶æ³¨æ„åŠ›åˆ†æ•°å›¾è¡¨"""
        if not self.attention_scores:
            return self.draw_no_data(painter, x, y, width, height, "æ³¨æ„åŠ›åˆ†æ•°")

        try:
            # ç»˜åˆ¶èƒŒæ™¯
            painter.fillRect(x, y, width, height, self.colors["background"])

            # ç»˜åˆ¶ç½‘æ ¼çº¿
            pen = QPen(self.colors["grid"], 1)
            painter.setPen(pen)

            # å‚ç›´ç½‘æ ¼çº¿
            grid_x_count = 6
            for i in range(1, grid_x_count):
                grid_x = x + i * width // grid_x_count
                painter.drawLine(grid_x, y, grid_x, y + height)

            # æ°´å¹³ç½‘æ ¼çº¿ (0-100åˆ†)
            grid_y_count = 5
            for i in range(1, grid_y_count):
                grid_y = y + i * height // grid_y_count
                painter.drawLine(x, grid_y, x + width, grid_y)

            # ç»˜åˆ¶åˆ†æ•°å‚è€ƒçº¿
            pen = QPen(QColor(255, 152, 0, 150), 2, Qt.DashLine)
            painter.setPen(pen)

            # 70åˆ†çº¿ï¼ˆä¸“æ³¨é˜ˆå€¼ï¼‰
            threshold_y = y + height - 70 * height // 100
            painter.drawLine(x, threshold_y, x + width, threshold_y)

            # ç»˜åˆ¶åæ ‡è½´æ ‡ç­¾
            painter.setPen(Qt.black)
            painter.setFont(QFont("Microsoft YaHei", 8))

            # Yè½´æ ‡ç­¾ (åˆ†æ•°)
            for i in range(0, 101, 20):
                label_y = y + height - i * height // 100
                painter.drawText(x - 25, label_y + 4, f"{i}")

            # ç»˜åˆ¶æ›²çº¿
            if len(self.attention_scores) > 1:
                pen = QPen(self.colors["attention"], 3)
                painter.setPen(pen)

                points = []
                for i, score in enumerate(self.attention_scores):
                    # è®¡ç®—ç‚¹ä½ç½®
                    point_x = x + i * width // (len(self.attention_scores) - 1) if len(self.attention_scores) > 1 else x
                    point_y = int(y + height - score * height // 100)
                    points.append(QPoint(point_x, point_y))

                # ç»˜åˆ¶è¿çº¿
                for i in range(len(points) - 1):
                    painter.drawLine(points[i], points[i + 1])

                # ç»˜åˆ¶æ•°æ®ç‚¹
                painter.setBrush(QBrush(self.colors["attention"]))
                for point in points[-10:]:  # åªç»˜åˆ¶æœ€è¿‘10ä¸ªç‚¹
                    painter.drawEllipse(point, 3, 3)

            # æ·»åŠ æ ‡é¢˜
            painter.setFont(QFont("Microsoft YaHei", 10, QFont.Bold))
            painter.setPen(Qt.darkBlue)
            painter.drawText(x + 10, y + 20, "æ³¨æ„åŠ›åˆ†æ•°è¶‹åŠ¿")

            # æ˜¾ç¤ºå½“å‰åˆ†æ•°
            current_score = self.attention_scores[-1] if self.attention_scores else 0
            score_text = f"å½“å‰: {current_score:.1f}"
            painter.setFont(QFont("Microsoft YaHei", 9))

            if current_score >= 70:
                painter.setPen(Qt.darkGreen)
            elif current_score >= 50:
                painter.setPen(Qt.darkYellow)
            else:
                painter.setPen(Qt.darkRed)

            painter.drawText(x + width - 80, y + 20, score_text)

        except Exception as e:
            print(f"ç»˜åˆ¶æ³¨æ„åŠ›å›¾è¡¨é”™è¯¯: {e}")
            self.draw_error(painter, x, y, width, height, "å›¾è¡¨é”™è¯¯")

    def draw_gaze_chart(self, painter, x, y, width, height):
        """ç»˜åˆ¶è§†çº¿è¿½è¸ªå›¾è¡¨"""
        if not self.gaze_x_values or not self.gaze_y_values:
            return self.draw_no_data(painter, x, y, width, height, "è§†çº¿è¿½è¸ª")

        try:
            # ç»˜åˆ¶èƒŒæ™¯
            painter.fillRect(x, y, width, height, self.colors["background"])

            # è®¡ç®—ä¸­å¿ƒç‚¹
            center_x = x + width // 2
            center_y = y + height // 2

            # ç»˜åˆ¶åæ ‡ç³»
            pen = QPen(Qt.black, 1)
            painter.setPen(pen)

            # Xè½´å’ŒYè½´
            painter.drawLine(x, center_y, x + width, center_y)
            painter.drawLine(center_x, y, center_x, y + height)

            # ç»˜åˆ¶ç½‘æ ¼åœ†ï¼ˆè§†çº¿å®¹å·®èŒƒå›´ï¼‰
            for radius in [height // 4, height // 2, 3 * height // 4]:
                pen = QPen(QColor(200, 200, 200, 100), 1)
                painter.setPen(pen)
                painter.drawEllipse(center_x - radius, center_y - radius, radius * 2, radius * 2)

            # ç»˜åˆ¶è§†çº¿è½¨è¿¹
            if len(self.gaze_x_values) > 1 and len(self.gaze_y_values) > 1:
                # ç»˜åˆ¶Xè½´è§†çº¿
                pen = QPen(self.colors["gaze_x"], 2)
                painter.setPen(pen)

                for i in range(len(self.gaze_x_values)):
                    if i == 0:
                        continue

                    # è½¬æ¢è§†çº¿åæ ‡ä¸ºå›¾è¡¨åæ ‡
                    prev_x = center_x + self.gaze_x_values[i - 1] * width // 2
                    prev_y = center_y - self.gaze_y_values[i - 1] * height // 2
                    curr_x = center_x + self.gaze_x_values[i] * width // 2
                    curr_y = center_y - self.gaze_y_values[i] * height // 2

                    # é™åˆ¶åœ¨å›¾è¡¨èŒƒå›´å†…
                    prev_x = max(x, min(x + width, prev_x))
                    prev_y = max(y, min(y + height, prev_y))
                    curr_x = max(x, min(x + width, curr_x))
                    curr_y = max(y, min(y + height, curr_y))

                    painter.drawLine(int(prev_x), int(prev_y), int(curr_x), int(curr_y))

                # ç»˜åˆ¶å½“å‰è§†çº¿ç‚¹
                if self.gaze_x_values and self.gaze_y_values:
                    curr_x = center_x + self.gaze_x_values[-1] * width // 2
                    curr_y = center_y - self.gaze_y_values[-1] * height // 2

                    # ç»˜åˆ¶ç‚¹
                    painter.setBrush(QBrush(self.colors["gaze_x"]))
                    painter.drawEllipse(int(curr_x) - 4, int(curr_y) - 4, 8, 8)

            # æ·»åŠ æ ‡é¢˜
            painter.setFont(QFont("Microsoft YaHei", 10, QFont.Bold))
            painter.setPen(Qt.darkBlue)
            painter.drawText(x + 10, y + 20, "è§†çº¿è¿½è¸ª")

            # æ˜¾ç¤ºå½“å‰è§†çº¿åæ ‡
            if self.gaze_x_values and self.gaze_y_values:
                gaze_text = f"X: {self.gaze_x_values[-1]:.2f}, Y: {self.gaze_y_values[-1]:.2f}"
                painter.setFont(QFont("Microsoft YaHei", 8))
                painter.setPen(Qt.darkGray)
                painter.drawText(x + width - 120, y + height - 10, gaze_text)

        except Exception as e:
            print(f"ç»˜åˆ¶è§†çº¿å›¾è¡¨é”™è¯¯: {e}")
            self.draw_error(painter, x, y, width, height, "å›¾è¡¨é”™è¯¯")

    def draw_eye_chart(self, painter, x, y, width, height):
        """ç»˜åˆ¶çœ¼éƒ¨ç‰¹å¾å›¾è¡¨ï¼ˆå¸¦å›¾ä¾‹ï¼‰"""
        if not self.ear_values:
            return self.draw_no_data(painter, x, y, width, height, "çœ¼éƒ¨ç‰¹å¾")

        try:
            # ç»˜åˆ¶èƒŒæ™¯
            painter.fillRect(x, y, width, height, self.colors["background"])

            # ç»˜åˆ¶ç½‘æ ¼çº¿
            pen = QPen(self.colors["grid"], 1)
            painter.setPen(pen)

            # å‚ç›´ç½‘æ ¼çº¿
            grid_x_count = 5
            for i in range(1, grid_x_count):
                grid_x = x + i * width // grid_x_count
                painter.drawLine(grid_x, y, grid_x, y + height)

            # æ°´å¹³ç½‘æ ¼çº¿ (0-0.4 EAR)
            grid_y_count = 5
            for i in range(1, grid_y_count):
                grid_y = y + i * height // grid_y_count
                painter.drawLine(x, grid_y, x + width, grid_y)

            # ç»˜åˆ¶å‚è€ƒçº¿ï¼ˆçœ¨çœ¼é˜ˆå€¼ 0.21ï¼‰
            pen = QPen(QColor(244, 67, 54, 150), 2, Qt.DashLine)
            painter.setPen(pen)

            threshold_y = int(y + height - 0.21 * height // 0.4)
            painter.drawLine(x, threshold_y, x + width, threshold_y)

            # ç»˜åˆ¶å‚è€ƒçº¿æ ‡ç­¾
            painter.setPen(Qt.darkRed)
            painter.setFont(QFont("Microsoft YaHei", 7))
            painter.drawText(x + 5, threshold_y - 5, "Blink Threshold (0.21)")

            # ç»˜åˆ¶åæ ‡è½´æ ‡ç­¾
            painter.setPen(Qt.black)
            painter.setFont(QFont("Microsoft YaHei", 7))

            # Yè½´æ ‡ç­¾ (EARå€¼)
            for i in range(0, 5):
                ear_value = i * 0.1
                label_y = int(y + height - ear_value * height // 0.4)
                painter.drawText(x - 20, label_y + 3, f"{ear_value:.1f}")

            # ç»˜åˆ¶EARæ›²çº¿
            ear_points = []
            if len(self.ear_values) > 1:
                pen = QPen(self.colors["ear"], 2)
                painter.setPen(pen)

                for i, ear in enumerate(self.ear_values):
                    # è®¡ç®—ç‚¹ä½ç½®
                    point_x = x + i * width // (len(self.ear_values) - 1) if len(self.ear_values) > 1 else x
                    point_y = y + height - ear * height // 0.4
                    ear_points.append(QPoint(int(point_x), int(point_y)))

                # ç»˜åˆ¶è¿çº¿
                for i in range(len(ear_points) - 1):
                    painter.drawLine(ear_points[i], ear_points[i + 1])

            # ç»˜åˆ¶å¤´éƒ¨å§¿æ€æ›²çº¿ï¼ˆYawå’ŒPitchï¼‰
            yaw_points = []
            pitch_points = []

            if len(self.head_yaw_values) > 1 and len(self.head_pitch_values) > 1:
                # Yaw (åè½¬) - è“è‰²
                yaw_pen = QPen(QColor(30, 144, 255), 1.5)  # è“è‰²
                painter.setPen(yaw_pen)

                for i, yaw_value in enumerate(self.head_yaw_values):
                    if i == 0:
                        continue
                    prev_x = x + (i - 1) * width // (len(self.head_yaw_values) - 1)
                    prev_y = y + height - yaw_value * 3  # ç¼©æ”¾å› å­
                    curr_x = x + i * width // (len(self.head_yaw_values) - 1)
                    curr_y = y + height - self.head_yaw_values[i] * 3
                    painter.drawLine(int(prev_x), int(prev_y), int(curr_x), int(curr_y))
                    yaw_points.append(QPoint(int(curr_x), int(curr_y)))

                # Pitch (ä¿¯ä»°) - æ©™è‰²
                pitch_pen = QPen(QColor(255, 165, 0), 1.5)  # æ©™è‰²
                painter.setPen(pitch_pen)

                for i, pitch_value in enumerate(self.head_pitch_values):
                    if i == 0:
                        continue
                    prev_x = x + (i - 1) * width // (len(self.head_pitch_values) - 1)
                    prev_y = y + height - pitch_value * 3  # ç¼©æ”¾å› å­
                    curr_x = x + i * width // (len(self.head_pitch_values) - 1)
                    curr_y = y + height - self.head_pitch_values[i] * 3
                    painter.drawLine(int(prev_x), int(prev_y), int(curr_x), int(curr_y))
                    pitch_points.append(QPoint(int(curr_x), int(curr_y)))

            # æ·»åŠ å›¾ä¾‹
            legend_x = x + 10
            legend_y = y + 15

            # EARå›¾ä¾‹
            painter.setPen(QPen(QColor(255, 152, 0), 2))
            painter.drawLine(legend_x, legend_y, legend_x + 30, legend_y)
            painter.setPen(Qt.black)
            painter.drawText(legend_x + 35, legend_y + 4, "EAR")

            # Yawå›¾ä¾‹
            legend_y += 20
            painter.setPen(QPen(QColor(30, 144, 255), 2))
            painter.drawLine(legend_x, legend_y, legend_x + 30, legend_y)
            painter.setPen(Qt.black)
            painter.drawText(legend_x + 35, legend_y + 4, "Yaw")

            # Pitchå›¾ä¾‹
            legend_y += 20
            painter.setPen(QPen(QColor(255, 165, 0), 2))
            painter.drawLine(legend_x, legend_y, legend_x + 30, legend_y)
            painter.setPen(Qt.black)
            painter.drawText(legend_x + 35, legend_y + 4, "Pitch")

            # æ·»åŠ æ ‡é¢˜
            painter.setFont(QFont("Microsoft YaHei", 9, QFont.Bold))
            painter.setPen(Qt.darkBlue)
            painter.drawText(x + 10, y + 15, "çœ¼éƒ¨ä¸å¤´éƒ¨ç‰¹å¾")

            # æ˜¾ç¤ºå½“å‰EARå€¼
            current_ear = self.ear_values[-1] if self.ear_values else 0
            ear_status = "Open" if current_ear > 0.21 else "Closed"
            ear_color = Qt.darkGreen if current_ear > 0.21 else Qt.darkRed
            ear_text = f"EAR: {current_ear:.2f} ({ear_status})"
            painter.setFont(QFont("Microsoft YaHei", 8))
            painter.setPen(ear_color)
            painter.drawText(x + width - 120, y + 15, ear_text)

            # æ˜¾ç¤ºå½“å‰å¤´éƒ¨å§¿æ€
            if self.head_yaw_values and self.head_pitch_values:
                current_yaw = self.head_yaw_values[-1]
                current_pitch = self.head_pitch_values[-1]
                head_text = f"Head: Yaw={current_yaw:.1f}Â°, Pitch={current_pitch:.1f}Â°"
                painter.setFont(QFont("Microsoft YaHei", 7))
                painter.setPen(Qt.darkGray)
                painter.drawText(x + width - 200, y + height - 10, head_text)

        except Exception as e:
            print(f"ç»˜åˆ¶çœ¼éƒ¨å›¾è¡¨é”™è¯¯: {e}")
            self.draw_error(painter, x, y, width, height, "Chart Error")

    def draw_no_data(self, painter, x, y, width, height, title):
        """ç»˜åˆ¶æ— æ•°æ®æç¤º"""
        painter.fillRect(x, y, width, height, QColor(240, 240, 240))

        painter.setPen(Qt.gray)
        painter.setFont(QFont("Microsoft YaHei", 12))

        text_x = x + width // 2 - 100
        text_y = y + height // 2

        painter.drawText(text_x, text_y, f"ç­‰å¾…{title}æ•°æ®...")

    def draw_error(self, painter, x, y, width, height, message):
        """ç»˜åˆ¶é”™è¯¯æç¤º"""
        painter.fillRect(x, y, width, height, QColor(255, 230, 230))

        painter.setPen(Qt.red)
        painter.setFont(QFont("Microsoft YaHei", 10))

        text_x = x + width // 2 - 40
        text_y = y + height // 2

        painter.drawText(text_x, text_y, message)

    def get_statistics(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if not self.attention_scores:
            return {}

        try:
            scores = list(self.attention_scores)
            gaze_x = list(self.gaze_x_values)
            gaze_y = list(self.gaze_y_values)
            ears = list(self.ear_values)

            stats = {
                "attention": {
                    "current": scores[-1] if scores else 0,
                    "average": np.mean(scores) if scores else 0,
                    "max": np.max(scores) if scores else 0,
                    "min": np.min(scores) if scores else 0,
                    "std": np.std(scores) if scores else 0
                },
                "gaze": {
                    "x_mean": np.mean(gaze_x) if gaze_x else 0,
                    "y_mean": np.mean(gaze_y) if gaze_y else 0,
                    "x_std": np.std(gaze_x) if gaze_x else 0,
                    "y_std": np.std(gaze_y) if gaze_y else 0
                },
                "eye": {
                    "ear_mean": np.mean(ears) if ears else 0,
                    "ear_std": np.std(ears) if ears else 0,
                    "blink_frames": sum(1 for ear in ears if ear < 0.21) if ears else 0
                }
            }

            return stats

        except Exception as e:
            print(f"è·å–ç»Ÿè®¡ä¿¡æ¯é”™è¯¯: {e}")
            return {}

# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    """ä¸»å‡½æ•°"""
    app = QApplication(sys.argv)
    app.setStyle('Fusion')

    # è®¾ç½®åº”ç”¨ä¿¡æ¯
    app.setApplicationName("å¤šåŠ¨ç—‡å„¿ç«¥æ³¨æ„åŠ›ä¸æƒ…ç»ªæ£€æµ‹ç³»ç»Ÿ")
    app.setApplicationDisplayName("å¤šåŠ¨ç—‡æ£€æµ‹ç³»ç»Ÿ v4.0")

    # åˆ›å»ºä¸»çª—å£
    window = ADHDDetectionSystem()
    window.show()

    # æ˜¾ç¤ºæ¬¢è¿æ¶ˆæ¯
    welcome_msg = """å¤šåŠ¨ç—‡å„¿ç«¥æ³¨æ„åŠ›ä¸æƒ…ç»ªæ£€æµ‹ç³»ç»Ÿ v4.0

    ç³»ç»ŸåŠŸèƒ½ï¼š
    1. å®æ—¶æ³¨æ„åŠ›åˆ†æï¼ˆä½¿ç”¨çœ¼åŠ¨è¿½è¸ªå’Œå¤´éƒ¨å§¿æ€ï¼‰
    2. æƒ…ç»ªè¯†åˆ«ï¼ˆæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼‰
    3. åŒè¾“å…¥æ¨¡å¼ï¼šå®æ—¶æ‘„åƒå¤´å’Œè§†é¢‘ä¸Šä¼ 
    4. å®æ—¶å¯è§†åŒ–æ˜¾ç¤ºå’Œè­¦æŠ¥
    5. å…¨é¢çš„æŠ¥å‘Šå’Œç»Ÿè®¡ä¿¡æ¯
    6. è¯­éŸ³åé¦ˆç³»ç»Ÿ

    ä½¿ç”¨è¯´æ˜ï¼š
    1. ç‚¹å‡»'å¯åŠ¨æ‘„åƒå¤´'è¿›è¡Œå®æ—¶åˆ†æ
    2. æˆ–ç‚¹å‡»'ä¸Šä¼ è§†é¢‘'åˆ†æå½•åˆ¶çš„è§†é¢‘
    3. åœ¨æ§åˆ¶é¢æ¿ä¸­è°ƒæ•´æ˜¾ç¤ºè®¾ç½®
    4. åœ¨å³ä¾§é¢æ¿æŸ¥çœ‹å®æ—¶ç»Ÿè®¡æ•°æ®
    5. å¯¼å‡ºå…¨é¢çš„åˆ†ææŠ¥å‘Š

    æ³¨æ„ï¼šä¸ºç¡®ä¿æœ€ä½³æ•ˆæœï¼Œè¯·ç¡®ä¿è‰¯å¥½çš„ç…§æ˜å’Œæ­£ç¡®çš„æ‘„åƒå¤´ä½ç½®ã€‚
    """

    QMessageBox.information(window, "æ¬¢è¿ä½¿ç”¨", welcome_msg)

    sys.exit(app.exec_())


if __name__ == '__main__':
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    os.makedirs("models", exist_ok=True)
    os.makedirs("reports", exist_ok=True)
    os.makedirs("recordings", exist_ok=True)

    main()